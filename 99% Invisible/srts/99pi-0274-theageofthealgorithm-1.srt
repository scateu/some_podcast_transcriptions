1
00:00:01,024 --> 00:00:03,584
99percentinvisible is brought to you by Progressive

2
00:00:03,840 --> 00:00:06,144
One of the country's leading providers of auto insurance

3
00:00:06,656 --> 00:00:08,704
With progressives Name Your Price tool

4
00:00:08,960 --> 00:00:15,104
You can say what kind of coverage were looking for and how much you want to pay and Progressive will show you options that fit within your budget

5
00:00:15,616 --> 00:00:18,688
Use the Name Your Price tool and start an online quote today

6
00:00:18,944 --> 00:00:19,968
Progressive.com

7
00:00:20,480 --> 00:00:22,784
Price in coverage match limited by state law

8
00:00:24,832 --> 00:00:25,344
Family ghosts

9
00:00:25,600 --> 00:00:31,232
Is a podcast explores all the unexpected ways our family structure and shape our view of the world

10
00:00:31,744 --> 00:00:33,024
Each episode of Family ghost

11
00:00:33,280 --> 00:00:37,120
Takes a look at the story behind a different person's family lore

12
00:00:37,376 --> 00:00:39,936
Digging into unanswered questions about their history

13
00:00:40,192 --> 00:00:42,496
Season 2 Launches on May 15th

14
00:00:42,752 --> 00:00:45,824
Between episode where they take you inside the love Israel family called

15
00:00:46,080 --> 00:00:50,432
An episode about uncovering the secrets scrolled in a newly discovered stack of Love Letters

16
00:00:50,688 --> 00:00:56,832
Asuna episode that brings you along to the hunt for a container of missing Chinese food that's haunted a family

17
00:00:57,088 --> 00:00:58,112
For 25 years

18
00:00:58,368 --> 00:00:58,880
And more

19
00:00:59,392 --> 00:01:03,232
Subscribe to family goes wherever you get your podcast and find out why they say

20
00:01:03,488 --> 00:01:04,768
Every house is haunted

21
00:01:05,792 --> 00:01:07,328
This is 99% invisible

22
00:01:07,840 --> 00:01:08,608
I'm Roman Mars

23
00:01:10,400 --> 00:01:16,544
On April 9th 2017 United Airlines flight 3411 was about to fly from Chicago to Louisville

24
00:01:16,800 --> 00:01:19,616
Flight attendants discovered the plane was overbooked

25
00:01:20,128 --> 00:01:24,992
I tried to get volunteers to give up their seats with Promises of travel vouchers and hotel accommodations

26
00:01:25,504 --> 00:01:27,552
But not enough people were willing to get off

27
00:01:33,440 --> 00:01:35,744
United ended up calling some airport security officers

28
00:01:36,256 --> 00:01:40,096
Board the plane and forcibly removed a passenger name doctor David down

29
00:01:40,608 --> 00:01:42,400
The officers ripped out of a seat

30
00:01:42,656 --> 00:01:48,800
And carry them down the aisle the airplane nose bleeding while horrified onlookers shot video with their phones

31
00:01:49,056 --> 00:01:52,128
Busted his lip

32
00:01:55,200 --> 00:01:58,016
You probably remember this incident in the outrage a generator

33
00:01:58,528 --> 00:02:04,672
The international uproar continued over the forced removal of a passenger from the United Airlines flight

34
00:02:04,928 --> 00:02:08,000
Today the airline CEO Oscar Munoz

35
00:02:08,256 --> 00:02:10,304
Issued an apology saying quote

36
00:02:10,560 --> 00:02:13,120
No one should ever be mistreated this way

37
00:02:13,376 --> 00:02:14,400
I want you to know that

38
00:02:14,656 --> 00:02:15,168
We take forward

39
00:02:16,960 --> 00:02:18,240
But why doctor

40
00:02:19,264 --> 00:02:23,360
How did he end up being the Unlucky passenger that United decided to remove

41
00:02:23,872 --> 00:02:25,408
Immediately following the incident

42
00:02:25,664 --> 00:02:28,224
Some people thought racial discrimination may have played a part

43
00:02:28,480 --> 00:02:30,528
And it's possible that this played a role in how you

44
00:02:30,784 --> 00:02:31,296
Treated

45
00:02:31,808 --> 00:02:33,600
But the answer to how he was chosen

46
00:02:33,856 --> 00:02:34,368
Was actually

47
00:02:34,880 --> 00:02:35,648
An algorithm

48
00:02:36,160 --> 00:02:36,928
Computer program

49
00:02:37,696 --> 00:02:38,976
It runs through a bunch of data

50
00:02:39,488 --> 00:02:44,864
Looking at stuff like how much each passenger I pays their ticket what time they checked into off Nicky one United

51
00:02:45,120 --> 00:02:47,424
And whether they were part of a rewards program

52
00:02:47,936 --> 00:02:52,800
The algorithm likely determined that dr.dao was one of the least valuable customers on the Fly

53
00:02:53,056 --> 00:02:53,568
I got the time

54
00:02:57,152 --> 00:03:01,248
Algorithms shape our world and profound and mostly invisible ways

55
00:03:01,504 --> 00:03:03,552
They predict it will be valuable customers

56
00:03:03,808 --> 00:03:05,856
Or whether were likely to repay a loan

57
00:03:06,112 --> 00:03:08,160
They filter what we see on social media

58
00:03:08,416 --> 00:03:09,184
Sort to resume

59
00:03:09,440 --> 00:03:10,464
And evaluate job performance

60
00:03:10,976 --> 00:03:13,280
Inform prison sentences in Montour or hell

61
00:03:14,048 --> 00:03:16,608
Most of these algorithms have been created with good intentions

62
00:03:17,120 --> 00:03:20,704
The goal is to replace objective judgments with objective measurements

63
00:03:21,216 --> 00:03:22,752
But it doesn't always work out like that

64
00:03:23,520 --> 00:03:24,032
The subject

65
00:03:24,544 --> 00:03:26,592
Huge I think algorithm design Maybe

66
00:03:26,848 --> 00:03:29,408
The big design problem of the 21st Century

67
00:03:29,664 --> 00:03:31,968
And that's why I wanted to interview Cathy O'Neil

68
00:03:32,736 --> 00:03:37,856
Okay thank you so much so cuz when we start can you give me one of them sort of npr-style

69
00:03:38,112 --> 00:03:40,672
Introductions in to say your name and what you do

70
00:03:40,928 --> 00:03:41,440
Sure

71
00:03:41,952 --> 00:03:44,768
I'm Cathy O'Neil I'm a mathematician data scientist

72
00:03:45,024 --> 00:03:51,168
Activist an author I wrote the book weapons of math destruction how big did it increases in equality and threatens to

73
00:03:51,424 --> 00:03:56,288
Call Krissy O'Neal studied number Theory and then left Academia to build predictive algorithms for a hedgehog

74
00:03:57,056 --> 00:04:01,408
But she got really disillusioned by the use of mathematical models in the financial industry

75
00:04:01,664 --> 00:04:03,968
I wanted to like have more impact in the world

76
00:04:04,224 --> 00:04:08,064
But I didn't really know that that impact could be really terrible I was very naive

77
00:04:08,576 --> 00:04:10,880
Every. O'Neill work as a data scientist.

78
00:04:11,136 --> 00:04:11,648
Couple of startups

79
00:04:12,160 --> 00:04:13,440
And through these experiences

80
00:04:13,952 --> 00:04:16,000
She started to get worried about the influence of

81
00:04:16,256 --> 00:04:17,536
Poorly designed algorithm

82
00:04:18,303 --> 00:04:20,095
So we'll start with the most obvious question

83
00:04:20,351 --> 00:04:21,119
What is an algorithm

84
00:04:22,399 --> 00:04:23,167
As most basic

85
00:04:23,423 --> 00:04:26,495
An algorithm is a step-by-step guide to solving a problem

86
00:04:27,007 --> 00:04:28,543
It's a set of instructions like a recipe

87
00:04:29,055 --> 00:04:32,127
Cooking dinner for my family

88
00:04:32,895 --> 00:04:36,223
Do in this case the problem is how to make a successful dinner

89
00:04:36,735 --> 00:04:38,527
O'Neill starts with a set of ingredients

90
00:04:38,783 --> 00:04:40,319
She's creating the meal

91
00:04:40,575 --> 00:04:45,951
You constantly making choices about what ingredients are healthy enough to include in her dinner algorithm

92
00:04:46,207 --> 00:04:52,351
Accurate that data because those ramen noodle packages

93
00:04:52,607 --> 00:04:54,911
Exclude

94
00:04:55,167 --> 00:04:59,007
And therefore imposing my agenda on this algorithm

95
00:04:59,263 --> 00:05:01,311
In addition to curating the ingredients

96
00:05:01,567 --> 00:05:05,919
O'Neal as the cook also defines what a successful outcome looks like

97
00:05:06,175 --> 00:05:10,527
I'm also defining success right I'm in charge of success success

98
00:05:11,039 --> 00:05:13,343
If my kids eat vegetables at that meal

99
00:05:13,599 --> 00:05:16,159
We know different cock my define success differently

100
00:05:16,415 --> 00:05:19,743
Define success to be like

101
00:05:22,047 --> 00:05:24,607
Impose

102
00:05:24,863 --> 00:05:26,399
Origin.

103
00:05:28,447 --> 00:05:31,263
O'Neil's main point here is that our them aren't we object

104
00:05:31,519 --> 00:05:33,311
Even when they're carried out by computers

105
00:05:33,823 --> 00:05:37,663
This is relevant because the companies that build them like to Market them as a jacket

106
00:05:38,175 --> 00:05:42,015
Claiming they remove human error infallibility from complex decision-making

107
00:05:42,271 --> 00:05:45,087
But every algorithm reflects the priorities and judgments

108
00:05:45,343 --> 00:05:46,367
David human designer

109
00:05:47,135 --> 00:05:50,207
Of course that doesn't necessarily make algorithms bad

110
00:05:50,463 --> 00:05:53,023
Right so I mean it's very important to me that I don't

111
00:05:53,279 --> 00:05:55,071
Get the reputation of hating allowed

112
00:05:55,583 --> 00:05:58,655
I actually like algorithms I think algorithms could really help

113
00:05:58,911 --> 00:06:02,495
Brunilda single out a particular kind of algorithm for scrutiny

114
00:06:03,007 --> 00:06:04,799
These are the ones we should worry about

115
00:06:06,079 --> 00:06:07,615
Are characterized by three properties

116
00:06:07,871 --> 00:06:12,479
That's very widespread and important so it's like they make important decisions about a lot of

117
00:06:12,735 --> 00:06:16,831
Number to that their secret that the people don't understand how they're being scored

118
00:06:17,343 --> 00:06:20,415
I never treated their destructive like one bad mistake

119
00:06:20,927 --> 00:06:23,743
In the in the design if you

120
00:06:23,999 --> 00:06:26,815
Will actually not only make it unfair for individuals butts

121
00:06:27,071 --> 00:06:30,655
Categorically unfair enormous populations As It Gets

122
00:06:30,911 --> 00:06:31,423
Scaled-up

123
00:06:32,447 --> 00:06:34,239
O'Neill has a shorthand for these algorithms

124
00:06:34,751 --> 00:06:37,311
The widespread mysterious and destructive

125
00:06:38,079 --> 00:06:38,847
She calls them

126
00:06:39,103 --> 00:06:40,639
Weapons of math destruction

127
00:06:42,943 --> 00:06:45,503
To show how one of these destructive algorithms work

128
00:06:46,015 --> 00:06:47,807
O'Neal points to criminal justice

129
00:06:48,575 --> 00:06:51,391
For hundreds of years key decisions in the legal process

130
00:06:51,903 --> 00:06:52,927
Like a mountain belt

131
00:06:53,183 --> 00:06:55,487
Sentence and likelihood of parole

132
00:06:55,743 --> 00:06:59,071
I've been in the hands of fallible human beings Guided by their instinct

133
00:06:59,583 --> 00:07:00,095
And sometime

134
00:07:00,863 --> 00:07:01,375
Personal bias

135
00:07:01,887 --> 00:07:04,703
The judges are super famously race at some of the more than others

136
00:07:04,959 --> 00:07:08,287
And that racism can produce very different outcomes for defendants

137
00:07:08,543 --> 00:07:13,407
For example the ACLU has found that sentences imposed on black men in the federal system

138
00:07:13,663 --> 00:07:17,759
Are nearly 20% longer than those four white men convicted of similar crimes

139
00:07:18,271 --> 00:07:23,647
And Studies have shown that prosecutors are more likely to seek the death penalty for African Americans that for whites convicted

140
00:07:23,903 --> 00:07:24,671
The same charges

141
00:07:24,927 --> 00:07:29,791
So you might think that computerized models fed by data would contribute to more even-handed treatment

142
00:07:30,303 --> 00:07:32,351
Criminal Justice System think so too

143
00:07:32,607 --> 00:07:35,167
It is increasingly try to minimize human bias

144
00:07:35,423 --> 00:07:36,447
By turning to risk

145
00:07:36,703 --> 00:07:37,727
Assessment algorithm

146
00:07:37,983 --> 00:07:41,823
Like crime risk what is a chance of someone coming back to prison after leaving it

147
00:07:42,079 --> 00:07:45,919
Many of these risk algorithms look at a person's record of arrest and conviction

148
00:07:46,431 --> 00:07:47,199
The problem is

149
00:07:47,455 --> 00:07:50,271
That data is already skewed by some social realities

150
00:07:50,783 --> 00:07:55,391
Take for example the fact that white people and black people use marijuana at roughly equal rates

151
00:07:55,647 --> 00:07:57,951
And yet there's five times as many

152
00:07:58,207 --> 00:08:00,767
Blacks getting arrested for smoking pot as whites

153
00:08:01,535 --> 00:08:02,559
5 times as many

154
00:08:03,071 --> 00:08:06,399
This may be because black neighborhoods tend to be more heavily policed than white nearby

155
00:08:06,911 --> 00:08:09,727
Which means black people get arrested for certain crimes more often than white

156
00:08:10,495 --> 00:08:13,823
Risk algorithms detect these patterns and apply them to the Future

157
00:08:14,335 --> 00:08:16,895
So the past is shaped in part by racism

158
00:08:17,151 --> 00:08:18,431
The future will be to

159
00:08:18,687 --> 00:08:21,247
The larger point is we have terrible date

160
00:08:22,527 --> 00:08:23,807
For the statisticians

161
00:08:24,063 --> 00:08:29,439
Involve the data scientist are like bliley going forward and pretending that our data is good

162
00:08:29,951 --> 00:08:33,023
And they were using it to actually make important decisions

163
00:08:33,279 --> 00:08:39,167
Risk assessment algorithm is also look at defendants answers to a questionnaire that supposed to tease out certain risk

164
00:08:39,423 --> 00:08:42,239
Understand questions like you know did you grow up in high-crime neighborhood

165
00:08:43,007 --> 00:08:46,591
Are you on welfare do you have a mental health problem do you have addiction problems

166
00:08:46,847 --> 00:08:48,127
Did your father go to prison

167
00:08:48,383 --> 00:08:50,431
Eproxies for racing class

168
00:08:51,199 --> 00:08:53,759
What is embedded in the scoring system and the judge is

169
00:08:54,015 --> 00:08:56,831
Given the score and it's called objective what is

170
00:08:57,343 --> 00:08:57,855
What is a

171
00:08:58,111 --> 00:09:03,487
Judge take away from it or you know how is it used if you have a high risk to send you to prison for longer

172
00:09:03,743 --> 00:09:04,511
Incessant

173
00:09:04,767 --> 00:09:09,631
Is also it's also used in bail hearings and parole hearings you have a high recidivism score

174
00:09:09,887 --> 00:09:10,911
You don't get girl

175
00:09:12,447 --> 00:09:14,495
And presumably you can take all that bias

176
00:09:14,751 --> 00:09:15,519
Import data

177
00:09:16,287 --> 00:09:16,799
And say

178
00:09:17,311 --> 00:09:18,847
This High chance recidivism

179
00:09:19,359 --> 00:09:20,383
Means that we should

180
00:09:20,639 --> 00:09:21,407
Rehabilitate

181
00:09:21,663 --> 00:09:27,039
I mean that you can take that pill that same stuff and choose to do a completely different thing with the result of the algorithm

182
00:09:27,295 --> 00:09:29,599
That's exactly my point exactly my point

183
00:09:29,855 --> 00:09:35,487
Say oh I wonder why people who have this characteristic have so much worse recidivism well

184
00:09:35,743 --> 00:09:38,303
Let's try to help them find a job that'll help

185
00:09:38,815 --> 00:09:41,631
We could use those algorithms those risk scores

186
00:09:42,143 --> 00:09:45,471
To try to account for our society

187
00:09:45,983 --> 00:09:52,127
Instead O'Neill says in many cases were effectively penalizing people for societal and structural issues that they have live

188
00:09:52,383 --> 00:09:53,151
Little control over

189
00:09:53,407 --> 00:09:55,199
And we're doing it at a massive scale

190
00:09:55,455 --> 00:09:57,503
Using these new technological tools

191
00:09:57,759 --> 00:10:03,903
We're Shifting the blame if you will from the society which is the one that should only has problems to the individual and

192
00:10:05,439 --> 00:10:10,559
It should be said that and some cases algorithms are helping to change elements of the criminal justice system for the better

193
00:10:11,071 --> 00:10:13,887
For example New Jersey recently did away with their cash bail system

194
00:10:14,399 --> 00:10:16,447
Which disadvantage low-income defendant

195
00:10:16,959 --> 00:10:19,263
They now rely on predictive algorithms instead

196
00:10:19,775 --> 00:10:21,055
Data shows that the states

197
00:10:21,311 --> 00:10:25,151
Pretrial County Jail populations are down by about 20%

198
00:10:27,711 --> 00:10:31,295
But still algorithms like that one remain unaudited and unregulated

199
00:10:31,807 --> 00:10:34,623
And it's a problem when algorithms are basically black boxes

200
00:10:35,135 --> 00:10:39,231
In many cases they're designed by private companies who sell them to other companies

201
00:10:39,487 --> 00:10:40,511
Any exact details

202
00:10:40,767 --> 00:10:41,279
And how they work

203
00:10:41,791 --> 00:10:42,303
Secret

204
00:10:42,559 --> 00:10:46,143
Not only the public in the dark even the companies using these things

205
00:10:46,399 --> 00:10:49,215
Might not understand exactly how the data is being processed

206
00:10:49,727 --> 00:10:53,055
This is true of many of the problematic algorithms that O'Neill has looked at

207
00:10:53,567 --> 00:10:55,615
Whether they use for sorting loan applications

208
00:10:55,871 --> 00:10:56,895
We're assessing teacher before

209
00:10:57,407 --> 00:11:00,479
Is there some kind of weird thing that happens to people

210
00:11:00,735 --> 00:11:02,527
In mathematical scores are trotted out

211
00:11:03,295 --> 00:11:04,319
They just start

212
00:11:04,831 --> 00:11:07,647
Closing their eyes and believing it because it's math

213
00:11:08,415 --> 00:11:11,231
And they do I feel like I know I'm not an expert of math so

214
00:11:11,743 --> 00:11:12,767
I can't push back

215
00:11:13,535 --> 00:11:19,167
And that's not something you just see time and time again you're like why didn't you question that this doesn't make sense

216
00:11:19,423 --> 00:11:20,959
Oh well it's math and I don't understand it

217
00:11:21,727 --> 00:11:22,751
Right now it seems like

218
00:11:23,263 --> 00:11:25,311
Because of algorithms in math

219
00:11:25,823 --> 00:11:27,359
It's just a new place

220
00:11:27,615 --> 00:11:28,127
2

221
00:11:28,383 --> 00:11:29,407
Place blame

222
00:11:30,175 --> 00:11:31,967
So that you do not have to think about

223
00:11:32,223 --> 00:11:34,015
Your decisions as natural company

224
00:11:35,295 --> 00:11:39,391
Because he's things are just so powerful and so you know mesmerizing to us

225
00:11:39,903 --> 00:11:40,927
Especially right now

226
00:11:41,439 --> 00:11:42,207
Making music

227
00:11:42,463 --> 00:11:42,975
All kinds of

228
00:11:43,487 --> 00:11:45,535
Nefarious way almost magical

229
00:11:46,559 --> 00:11:47,071
Yeah

230
00:11:48,095 --> 00:11:48,863
Hatsune

231
00:11:49,119 --> 00:11:50,143
Scary

232
00:11:50,399 --> 00:11:55,007
It's scary and I think I think I'm going to like I would go one step further than that I feel like

233
00:11:55,519 --> 00:11:57,823
Just an observation that these algorithms

234
00:11:58,079 --> 00:12:03,711
They don't show up randomly they show up when there's a really difficult conversation that people want to avoid

235
00:12:07,551 --> 00:12:08,575
They're like oh

236
00:12:09,087 --> 00:12:14,207
We don't know what it's what makes a good teacher and different people have different opinions about that

237
00:12:14,975 --> 00:12:15,487
Siletz

238
00:12:15,999 --> 00:12:20,351
Just bypasses conversation by having an algorithm score teachers or

239
00:12:20,607 --> 00:12:23,423
We don't know what prison is really for

240
00:12:23,679 --> 00:12:27,775
Let's have a way of deciding how long to sentence up

241
00:12:28,287 --> 00:12:33,151
We introduced these Silver Bullet mathematical algorithms because we don't want to have a cover

242
00:12:33,407 --> 00:12:33,919
Station

243
00:12:36,991 --> 00:12:39,807
In a news book she writes about this young guy named Kyle B

244
00:12:40,319 --> 00:12:43,391
Who takes some time off college to get treated for bipolar disorder

245
00:12:43,903 --> 00:12:48,255
When's he's better and ready to go back to school supplies for a part-time job at Kroger

246
00:12:48,511 --> 00:12:49,535
The big grocery store chain

247
00:12:50,047 --> 00:12:52,095
His friend works there who offers to vouch for

248
00:12:52,607 --> 00:12:56,447
Kyle was such a good student that he figured the application would be just a formality

249
00:12:56,703 --> 00:12:58,239
But he didn't get called back for an interview

250
00:12:58,751 --> 00:13:00,287
Is application was Redline

251
00:13:00,543 --> 00:13:03,871
By the personality test be taken when he applied for the job

252
00:13:04,383 --> 00:13:06,687
The test was part of an employee selection algorithm

253
00:13:07,199 --> 00:13:10,015
Developed by private Workforce company called Kronos

254
00:13:10,783 --> 00:13:16,671
70% of job applicants in this country take personality test before they get an interview so this is a very common practice

255
00:13:17,439 --> 00:13:21,535
Kyle had that screening and he found out because his friend worked at Kroger's

256
00:13:21,791 --> 00:13:24,863
But he has failed the test so most people never find that other just don't hear back

257
00:13:25,631 --> 00:13:29,215
And the other thing that was unusual about Kyle is that his dad is a lawyer

258
00:13:29,471 --> 00:13:31,007
So his dad was like what

259
00:13:31,263 --> 00:13:33,055
What were the questions like on this test

260
00:13:33,567 --> 00:13:38,175
And he said well there's some of them are a lot like the questions I got at the hospital the mental health assessment

261
00:13:38,687 --> 00:13:41,759
The Tesco got at the hospital was called the five Factor model

262
00:13:42,527 --> 00:13:43,551
Integrates people on

263
00:13:43,807 --> 00:13:46,367
Extraversion agreeableness conscientiousness

264
00:13:46,623 --> 00:13:48,927
Neuroticism and openness to new ideas

265
00:13:49,695 --> 00:13:50,975
It's used in mental health of Ally

266
00:13:51,999 --> 00:13:54,047
Potential employees answers to the test

267
00:13:54,303 --> 00:13:56,607
Are then plugged into an algorithm that decides whether to

268
00:13:56,863 --> 00:13:57,887
The person should be hired

269
00:13:58,143 --> 00:14:01,471
Something's father was like well that's illegal under the Americans with Disability Act

270
00:14:02,239 --> 00:14:04,543
Does father and he's figured out together

271
00:14:04,799 --> 00:14:09,919
But something very fishy had been going on in his father's actually filed class action lawsuit against Krogers for the

272
00:14:10,431 --> 00:14:15,807
The suit is still pending but arguments are likely to focus on whether the personality test can be considered a medical exam

273
00:14:16,319 --> 00:14:16,831
If it is

274
00:14:17,343 --> 00:14:19,135
In the illegal under the Ada

275
00:14:19,647 --> 00:14:22,975
Anil gets a different jobs require people with different personalities

276
00:14:23,487 --> 00:14:29,375
But she says a hiring algorithm is a blunt and unregulated tool that ends up disqualifying big categories of people

277
00:14:29,887 --> 00:14:30,911
Which makes it a classic

278
00:14:31,423 --> 00:14:32,703
Weapon of math destruction

279
00:14:33,215 --> 00:14:39,359
In certain jobs you wouldn't want neurotic people or introverted people like if you're at a call center

280
00:14:39,615 --> 00:14:40,127
We're a lot of

281
00:14:40,383 --> 00:14:42,431
Really irate customers call you up

282
00:14:42,687 --> 00:14:43,711
That might be a problem

283
00:14:44,223 --> 00:14:45,759
In which case it is

284
00:14:46,015 --> 00:14:47,807
Actually legal if you get

285
00:14:48,063 --> 00:14:50,367
An exception for your company

286
00:14:50,623 --> 00:14:56,767
The problem is that these personality tests are not carefully designed for each business

287
00:14:57,023 --> 00:15:00,607
But rather what happens if these companies just sell the same personality test

288
00:15:00,863 --> 00:15:02,655
To all the businesses that will buy them

289
00:15:05,727 --> 00:15:09,055
A lot of the algorithms that O'Neill explores in her book are largely hidden

290
00:15:09,311 --> 00:15:10,591
They don't get a lot of attention

291
00:15:11,103 --> 00:15:14,175
We as consumers and job applicants and employees

292
00:15:14,431 --> 00:15:15,711
May not even be aware that

293
00:15:16,223 --> 00:15:17,759
Coming along in the background of Our Lives

294
00:15:18,271 --> 00:15:20,063
Piles in categories

295
00:15:20,575 --> 00:15:24,671
But there is one kind of algorithm has gotten a lot of attention in the news lately

296
00:15:24,927 --> 00:15:29,023
Social media has been able to infiltrate

297
00:15:29,279 --> 00:15:30,303
Politics

298
00:15:32,095 --> 00:15:38,239
Social media is a technology

299
00:15:38,495 --> 00:15:42,591
Title is all depends on our conversation started talking about

300
00:15:42,847 --> 00:15:47,967
The recent election and the complex ways that social media algorithms shape the news that we receive

301
00:15:48,735 --> 00:15:54,623
Facebook shows the stories and ads based on what they think we want in a horse what they think we want is based on algorithms

302
00:15:55,135 --> 00:15:59,487
These algorithms look at what we clicked on before and then he has more content we like

303
00:15:59,999 --> 00:16:00,767
The result is that

304
00:16:01,023 --> 00:16:03,071
We've Ended up in these information silos

305
00:16:03,327 --> 00:16:08,191
Increasingly polarized and oblivious to what people of different political Persuasions might be seen

306
00:16:08,703 --> 00:16:14,847
I do think this is a major problem you know it's the start of the sky's the limit we have built the internet in the internet

307
00:16:15,103 --> 00:16:16,383
Is a propaganda machine

308
00:16:16,895 --> 00:16:17,919
It's propaganda

309
00:16:18,175 --> 00:16:20,223
Delivery device

310
00:16:20,479 --> 00:16:20,991
If you out

311
00:16:21,247 --> 00:16:22,271
And that's not

312
00:16:22,527 --> 00:16:24,319
I don't see how that's going to stop

313
00:16:24,831 --> 00:16:25,343
Yeah

314
00:16:25,599 --> 00:16:27,135
Especially if every moment

315
00:16:27,391 --> 00:16:28,159
Optimized

316
00:16:28,671 --> 00:16:30,975
Find algorithm it's meant to manipulate your emotions

317
00:16:32,255 --> 00:16:36,863
Right let's go to Facebook's Optimizer algorithm that's not optimizing

318
00:16:37,119 --> 00:16:37,631
Chartreuse

319
00:16:37,887 --> 00:16:39,423
Optimizing for profit

320
00:16:40,447 --> 00:16:42,239
And then they claim to be neutral

321
00:16:43,007 --> 00:16:44,287
But of course nothing's neutral

322
00:16:45,311 --> 00:16:47,359
And we have seen the result

323
00:16:47,615 --> 00:16:49,151
We've seen what it's actually optimized

324
00:16:50,943 --> 00:16:57,087
This kind of data-driven political micro-targeting means conspiracies and misinformation can gain surprising traction on

325
00:16:57,343 --> 00:16:57,855
Online

326
00:16:58,111 --> 00:17:00,927
Stories claiming that Pope Francis endorse Donald Trump

327
00:17:01,183 --> 00:17:03,487
And that Hillary Clinton sold weapons to Isis

328
00:17:03,999 --> 00:17:05,791
Game millions of viewers on Facebook

329
00:17:06,303 --> 00:17:07,071
Neither of those two

330
00:17:07,583 --> 00:17:08,095
Betrayal

331
00:17:12,447 --> 00:17:18,591
Fixing the problem of these destructive algorithms is not going to be easy especially when they're insinuating themselves and then more

332
00:17:18,847 --> 00:17:19,871
And more parts of our lives

333
00:17:20,127 --> 00:17:24,479
Romeo thinks that measurement and transparency is one place to start

334
00:17:24,991 --> 00:17:28,575
Like what that Facebook algorithm and the political ads that it serves to its users

335
00:17:29,087 --> 00:17:30,879
If you were to talk to Facebook about

336
00:17:31,903 --> 00:17:32,415
How to

337
00:17:32,671 --> 00:17:35,231
Inject some ethics into their optimization

338
00:17:36,511 --> 00:17:37,535
What would you

339
00:17:37,791 --> 00:17:43,935
Do would you make a case for the bottom line of Truth being a like a longer tail way to make more

340
00:17:44,191 --> 00:17:45,471
Money or would you just say

341
00:17:45,983 --> 00:17:48,543
This is about ethics and you should be thinking about ethics

342
00:17:49,311 --> 00:17:52,383
To be honest if I really had their attention I would ask them to

343
00:17:52,895 --> 00:17:53,919
Voluntarily

344
00:17:54,431 --> 00:17:58,783
Find a space on the web to just put every political

345
00:17:59,039 --> 00:17:59,551
Add

346
00:17:59,807 --> 00:18:02,111
And actually every ad just have a way for

347
00:18:02,367 --> 00:18:03,135
For journalists

348
00:18:03,647 --> 00:18:06,463
And people interested in the concept of the informed citizenry

349
00:18:06,719 --> 00:18:10,047
To go through all the ads that they have on Facebook at a given time

350
00:18:10,303 --> 00:18:15,167
Because even if that article about Hillary Clinton and Isis with shared thousands of times

351
00:18:15,423 --> 00:18:17,471
Lots of people never saw it at all

352
00:18:17,983 --> 00:18:20,031
Just show us what you're showing other people

353
00:18:20,287 --> 00:18:22,591
Because I think one of the most pernicious issues is

354
00:18:22,847 --> 00:18:24,895
The fact that we don't know what other people are saying

355
00:18:25,407 --> 00:18:26,943
I'm not waiting for Facebook to like

356
00:18:27,711 --> 00:18:28,991
Actually go against their interest

357
00:18:29,503 --> 00:18:34,623
I'd like changed their profit goal but I do think this kind of transparency can be given

358
00:18:35,647 --> 00:18:39,231
O'Neill also says it's important to measure the broad effects of these algorithms

359
00:18:39,487 --> 00:18:41,535
Can't understand who they most impact

360
00:18:42,047 --> 00:18:43,583
Everyone should start measuring it

361
00:18:43,839 --> 00:18:45,119
And what I mean by that

362
00:18:45,375 --> 00:18:47,679
It is relatively simple and it's

363
00:18:47,935 --> 00:18:49,471
This might not be a complete

364
00:18:49,727 --> 00:18:51,263
Start but it's a pretty good for

365
00:18:51,519 --> 00:18:52,287
Stop witches

366
00:18:52,799 --> 00:18:53,567
Measure

367
00:18:53,823 --> 00:18:55,103
Perfume this fails

368
00:18:55,615 --> 00:18:57,151
Meaning which population

369
00:18:57,407 --> 00:19:00,735
Are most negatively impacted by the results of these acronyms

370
00:19:00,991 --> 00:19:03,039
What is the harm that befalls those

371
00:19:04,831 --> 00:19:06,623
And how are the failures distribute

372
00:19:06,879 --> 00:19:07,391
Across the pond

373
00:19:08,415 --> 00:19:10,207
So if you see a hiring algorithm

374
00:19:10,463 --> 00:19:10,975
Feel

375
00:19:11,231 --> 00:19:12,511
Much more often for women

376
00:19:13,791 --> 00:19:14,559
That's a problem

377
00:19:15,071 --> 00:19:17,119
Especially if the failure is

378
00:19:17,375 --> 00:19:18,911
They don't get hired when they should get

379
00:19:19,423 --> 00:19:21,215
I really do think that a study

380
00:19:21,471 --> 00:19:24,031
A close examination of the distribution of failures

381
00:19:25,055 --> 00:19:26,079
And the harms of those failure

382
00:19:27,103 --> 00:19:28,639
Would really really be a good start

383
00:19:49,375 --> 00:19:52,191
You're not mad enough about how algorithms influence your life

384
00:19:52,703 --> 00:19:53,215
I've got a doozy

385
00:19:53,983 --> 00:19:54,495
These messages

386
00:19:56,799 --> 00:19:57,567
Between the beats

387
00:19:57,823 --> 00:19:59,871
Bongs and chatter of our devices

388
00:20:00,127 --> 00:20:03,455
We're immersed in a cluttered world of man made sound

389
00:20:03,967 --> 00:20:04,479
If

390
00:20:04,735 --> 00:20:10,879
I'm in room listen to an Alexa interacting with Alexa that Alexis talking to me and my wife isn't

391
00:20:11,135 --> 00:20:14,975
All hearing that voice

392
00:20:15,231 --> 00:20:15,743
Technology

393
00:20:15,999 --> 00:20:16,511
He talked to us

394
00:20:17,279 --> 00:20:18,815
It is a creation of Sonic trash

395
00:20:19,327 --> 00:20:21,887
That's Joel Beckerman is a sound designer and composer

396
00:20:22,143 --> 00:20:25,471
Who studies how we can improve our interactions with technology

397
00:20:25,727 --> 00:20:27,263
To make them more human

398
00:20:30,079 --> 00:20:31,359
Turn the living room lights on

399
00:20:32,639 --> 00:20:33,151
Okay

400
00:20:33,663 --> 00:20:35,711
Dim the living room lights to 50%

401
00:20:37,247 --> 00:20:37,759
Okay

402
00:20:38,527 --> 00:20:43,903
It's not exactly a thrilling conversation but taking away the fake human and adding and some thoughtful sound

403
00:20:44,159 --> 00:20:44,671
Mix

404
00:20:44,927 --> 00:20:46,207
For a much more natural

405
00:20:46,463 --> 00:20:46,975
Experience

406
00:20:47,487 --> 00:20:48,767
Turn the living room lights on

407
00:20:51,071 --> 00:20:52,095
Dim the living room lights

408
00:20:52,351 --> 00:20:53,119
50%

409
00:20:55,423 --> 00:20:56,447
We're presenting to

410
00:20:56,703 --> 00:21:00,543
Special episodes from 99% invisible and the Robert Wood Johnson Foundation

411
00:21:00,799 --> 00:21:03,359
About how are soundscape is impacting our health

412
00:21:03,871 --> 00:21:05,919
And how we can humanize the sounds around us

413
00:21:06,175 --> 00:21:08,735
Look for them Fridays May 17th and May 24th

414
00:21:08,991 --> 00:21:09,503
Right here

415
00:21:13,599 --> 00:21:15,391
We need to talk about something

416
00:21:15,647 --> 00:21:21,792
Constipation abdominal pain and bloating yourself it's not that bad you take laxatives modify your diet

417
00:21:22,048 --> 00:21:22,560
Size routine

418
00:21:22,816 --> 00:21:24,608
But thinking about it all the time is frustrating

419
00:21:24,864 --> 00:21:31,008
It doesn't have to Define you if your constipation and abdominal symptoms come back again and again and you don't know why

420
00:21:31,264 --> 00:21:37,408
Then it may be time to seek help go to oh my God. Info podcast we can learn more

421
00:21:37,664 --> 00:21:41,760
More about your symptoms that's oh my God. Info / podcast

422
00:21:42,016 --> 00:21:42,528
Oh my God

423
00:21:43,552 --> 00:21:44,320
Flash podcast

424
00:22:00,704 --> 00:22:01,984
We are currently experience

425
00:22:02,240 --> 00:22:03,264
Higher call volume

426
00:22:03,520 --> 00:22:04,032
The normal

427
00:22:04,800 --> 00:22:05,824
Please stay on the line

428
00:22:06,080 --> 00:22:06,592
And then

429
00:22:06,848 --> 00:22:07,616
I'll be with you shortly

430
00:22:08,128 --> 00:22:09,920
Just one that I think is kind of

431
00:22:10,176 --> 00:22:12,480
Fun because it's annoying and Secret

432
00:22:13,504 --> 00:22:14,784
But you would never know it

433
00:22:15,040 --> 00:22:17,344
So if you call but customer service line

434
00:22:17,600 --> 00:22:20,416
I'm not saying this will always happen but it will sometimes happen

435
00:22:20,672 --> 00:22:21,952
That your phone number

436
00:22:22,464 --> 00:22:23,744
Will be used to back

437
00:22:24,256 --> 00:22:25,792
Track like who you are

438
00:22:26,048 --> 00:22:26,816
Ubisoft

439
00:22:27,328 --> 00:22:29,632
As like are you high value customer or low value

440
00:22:30,144 --> 00:22:31,680
If you're high value customer

441
00:22:31,936 --> 00:22:33,984
You'll talk to a customer service representative

442
00:22:34,240 --> 00:22:35,776
Much sooner than if your love

443
00:22:37,056 --> 00:22:37,568
Put on hold

444
00:22:38,336 --> 00:22:40,384
That's how businesses make decisions now

445
00:22:41,920 --> 00:22:43,200
You are caller number

446
00:22:43,712 --> 00:22:44,224
90

447
00:22:44,736 --> 00:22:45,248
9

448
00:22:46,016 --> 00:22:47,040
Your call is important

449
00:22:47,808 --> 00:22:48,832
Please stay on the line

450
00:22:50,368 --> 00:22:52,928
99percentinvisible with bruises week by Delaney Hall

451
00:22:53,184 --> 00:22:54,464
Tecademics production by Emmett

452
00:22:55,232 --> 00:22:58,048
Katie mingle is the senior producer Kolstad is the digital

453
00:22:58,816 --> 00:23:00,352
Some real composed all the music

454
00:23:00,608 --> 00:23:02,656
Staff includes a breach of immense refused

455
00:23:02,912 --> 00:23:04,448
Turn master and me Roman Mars

456
00:23:05,216 --> 00:23:06,752
Special thanks to Ryan keasler and

457
00:23:07,008 --> 00:23:07,520
Courtney riddle

458
00:23:08,032 --> 00:23:11,104
We are brother to 91.7 kalw in San Francisco

459
00:23:11,360 --> 00:23:11,872
Produced

460
00:23:12,128 --> 00:23:12,896
On radio row

461
00:23:13,408 --> 00:23:13,920
And beautiful

462
00:23:14,432 --> 00:23:15,200
Downtown

463
00:23:15,712 --> 00:23:16,736
Oakland California

464
00:23:18,016 --> 00:23:20,576
99% invisible part of radiotopia from pra

465
00:23:20,832 --> 00:23:23,392
The best most Innovative shows in all app

466
00:23:24,160 --> 00:23:25,696
We are supported by the Knight Foundation

467
00:23:25,952 --> 00:23:26,976
Coin carrying list

468
00:23:27,488 --> 00:23:28,000
Just like you

469
00:23:29,024 --> 00:23:35,168
You can find 99percentinvisible and join discussions about the show on Facebook you can't we do meet at Roman Mars in the show at 99p

470
00:23:35,424 --> 00:23:35,936
Gyorgy

471
00:23:36,192 --> 00:23:37,984
Run Instagram Tumblr and Reddit to

472
00:23:38,240 --> 00:23:42,592
The lovely home on the internet with more design story than we can ever tell you here on the radio

473
00:23:44,384 --> 00:23:44,896
The podcast

474
00:23:46,176 --> 00:23:46,944
Is there a website

475
00:23:47,200 --> 00:23:48,224
And 99 pi

476
00:23:48,480 --> 00:23:48,992
. org

477
00:23:51,552 --> 00:23:54,112
Radio Tempe

478
00:24:00,768 --> 00:24:03,328
Before we go I want to tell you about the body genius

479
00:24:03,584 --> 00:24:06,912
Series from Bellow radio TV show the truth

480
00:24:07,424 --> 00:24:08,704
The body genius is a dark

481
00:24:08,960 --> 00:24:09,728
Comic mystery

482
00:24:09,984 --> 00:24:11,008
That whines through a

483
00:24:11,264 --> 00:24:13,056
Treacherous fictionalized Hollywood

484
00:24:13,568 --> 00:24:14,592
We're looking the part

485
00:24:14,848 --> 00:24:16,128
Might cost you your life

486
00:24:16,640 --> 00:24:21,504
You know how sometimes a famous actor has to get really jacked play superhero in a movie

487
00:24:21,760 --> 00:24:25,856
What is a ton of weight to play a prisoner or a psychopath

488
00:24:26,112 --> 00:24:29,184
Or put on 50lb all of a sudden to play like

489
00:24:29,440 --> 00:24:30,208
A dad

490
00:24:31,744 --> 00:24:34,560
I'm a Hollywood trainer and I specialize in that stuff

491
00:24:34,816 --> 00:24:36,864
Actually if I'm being honest

492
00:24:37,120 --> 00:24:38,912
I'm one of the main guys who know how to do that guy

493
00:24:39,168 --> 00:24:39,936
Nothing

494
00:24:40,192 --> 00:24:46,336
A few days ago Wesley Stern and actor that I was helping to get huge for an action movie called Immortal cop

495
00:24:46,848 --> 00:24:49,920
Was found dead in my private gym

496
00:24:50,176 --> 00:24:52,992
Crushed inside a high-tech weight machine

497
00:24:53,248 --> 00:24:54,272
Even worse

498
00:24:54,528 --> 00:24:57,344
The cops think that I had something to do with it

499
00:24:57,600 --> 00:24:58,624
Now

500
00:24:58,880 --> 00:25:00,672
I need to clear my name

501
00:25:00,928 --> 00:25:07,072
Somebody out there killed Wesley and I'm going to do whatever it takes to find them

502
00:25:09,888 --> 00:25:13,984
Final episode of the body genius drops on May 30th Bend all 5 parts

503
00:25:14,240 --> 00:25:15,264
Saint the truth podcast

504
00:25:15,520 --> 00:25:16,032
Dot-com
