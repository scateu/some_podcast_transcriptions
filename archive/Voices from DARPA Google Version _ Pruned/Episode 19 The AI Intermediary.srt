1
00:00:11,264 --> 00:00:17,408
Dell Technologies the internet CPS in the palm of your hand

2
00:00:19,456 --> 00:00:21,760
Technology is a driver of our times

3
00:00:22,016 --> 00:00:23,552
Since its founding in 19

4
00:00:23,808 --> 00:00:24,320
58th in the Mist

5
00:00:24,576 --> 00:00:25,344
The Cold War

6
00:00:25,600 --> 00:00:28,160
DARPA the defense Advanced research projects agency

7
00:00:28,672 --> 00:00:29,184
Has Bennett Drive

8
00:00:30,976 --> 00:00:31,488
Welcome to

9
00:00:31,744 --> 00:00:32,768
Voices from DARPA

10
00:00:33,024 --> 00:00:34,048
A window Into Darkness

11
00:00:34,304 --> 00:00:35,584
Tour of program managers

12
00:00:36,096 --> 00:00:36,864
Their job

13
00:00:37,120 --> 00:00:38,144
To redefine

14
00:00:39,936 --> 00:00:41,216
My name is Ivan Amato

15
00:00:41,728 --> 00:00:42,496
DARPA hosts

16
00:00:42,752 --> 00:00:44,800
And today I'm pleased to have with me in the studio

17
00:00:45,056 --> 00:00:45,824
Mr. David Gunner

18
00:00:46,336 --> 00:00:49,408
Since late 2015 has been a missed his third

19
00:00:49,664 --> 00:00:50,176
Tour of Duty

20
00:00:50,432 --> 00:00:50,944
The other program man

21
00:00:51,712 --> 00:00:52,992
In this case with the information

22
00:00:53,248 --> 00:00:53,760
Innovation

23
00:00:54,272 --> 00:00:57,344
During his previous tour in the early years of the new millennium

24
00:00:57,600 --> 00:00:58,368
David manager

25
00:00:58,880 --> 00:01:00,672
The personalized assistant that learns

26
00:01:01,440 --> 00:01:01,952
How

27
00:01:02,464 --> 00:01:04,768
That led to the now-familiar voice of Siri

28
00:01:05,536 --> 00:01:08,864
Through his first tour in the 1990s he managed a variety of AI

29
00:01:09,376 --> 00:01:10,400
Projects artificial intelligence

30
00:01:11,168 --> 00:01:12,704
Including the command post of the

31
00:01:12,960 --> 00:01:13,472
The future

32
00:01:13,728 --> 00:01:15,776
Which was later adopted by the US Army

33
00:01:16,032 --> 00:01:17,568
As their command-and-control system

34
00:01:17,824 --> 00:01:18,336
M4u cyttorak

35
00:01:19,360 --> 00:01:20,384
Between his stints at the

36
00:01:20,896 --> 00:01:23,200
Mr. going to play the variety professional roles

37
00:01:23,456 --> 00:01:23,968
Among them

38
00:01:24,224 --> 00:01:27,040
Program director for data analytics and contextual in

39
00:01:27,296 --> 00:01:27,808
Intelligence

40
00:01:28,064 --> 00:01:29,856
The Palo Alto Research Center Parc

41
00:01:30,368 --> 00:01:32,416
A number of founding and managerial roles for

42
00:01:32,672 --> 00:01:33,184
Fortec companies

43
00:01:33,696 --> 00:01:36,256
And as a senior scientist in the Air Force research

44
00:01:36,768 --> 00:01:37,280
Wright-Patterson

45
00:01:38,560 --> 00:01:40,608
The thematic thread throughout his career

46
00:01:41,120 --> 00:01:43,168
Have been artificial intelligence in human

47
00:01:43,424 --> 00:01:44,192
Computer symbiosis

48
00:01:44,704 --> 00:01:46,752
It's a thread that goes back to his education

49
00:01:47,008 --> 00:01:47,520
David Holt

50
00:01:47,776 --> 00:01:48,288
A pair of Master

51
00:01:49,312 --> 00:01:51,104
1 in computer science from Stanford

52
00:01:51,872 --> 00:01:53,920
And another in experimental psychology

53
00:01:54,432 --> 00:01:54,944
From the University of

54
00:01:56,224 --> 00:01:58,016
David gunning thanks for joining me in the studio today

55
00:01:58,272 --> 00:01:59,040
Yes glad to do it

56
00:01:59,296 --> 00:02:00,576
So like many here at DARPA

57
00:02:00,832 --> 00:02:01,344
You work

58
00:02:01,600 --> 00:02:04,416
At the intersection of different mindsets skill sets

59
00:02:04,672 --> 00:02:05,184
Tool sets

60
00:02:05,440 --> 00:02:07,488
In your case the intersection seem to focus on

61
00:02:07,744 --> 00:02:10,560
Humankind computer kind in the ways these two creatures

62
00:02:11,584 --> 00:02:12,352
I can work together

63
00:02:12,864 --> 00:02:14,144
Where does Interest come from

64
00:02:14,400 --> 00:02:15,680
Go back as far as you can

65
00:02:16,448 --> 00:02:18,496
Well I can go back to the

66
00:02:18,752 --> 00:02:20,800
Originating event that started dark

67
00:02:22,080 --> 00:02:23,616
Sputnik in 1958

68
00:02:24,640 --> 00:02:25,920
I was 8 years old

69
00:02:26,944 --> 00:02:29,504
Living at my grandfather's house out in the country

70
00:02:30,528 --> 00:02:33,600
And I can remember him taking me out in the yard at night

71
00:02:34,368 --> 00:02:37,952
To see the light of Sputnik as it's flying overhead

72
00:02:38,208 --> 00:02:40,512
You're kind of conveying what a shock this week

73
00:02:40,768 --> 00:02:41,536
Where were you

74
00:02:42,048 --> 00:02:42,560
Complete the setting

75
00:02:42,816 --> 00:02:44,096
And Covington Kentucky outside

76
00:02:44,352 --> 00:02:44,864
Covington Kentucky

77
00:02:45,632 --> 00:02:47,424
And so that at the time

78
00:02:47,936 --> 00:02:51,776
Also not only spark DARPA but sparked a huge interest in technology

79
00:02:52,288 --> 00:02:54,592
My there's a huge interest in encouraging

80
00:02:54,848 --> 00:02:57,664
Young minds to be interested in engineering and science

81
00:02:58,432 --> 00:02:59,712
Remember that is kind of a

82
00:02:59,968 --> 00:03:00,480
Ark

83
00:03:00,736 --> 00:03:01,248
Started it

84
00:03:01,504 --> 00:03:05,344
Later on I was in Otterbein College in Ohio in the

85
00:03:05,600 --> 00:03:07,392
Late 60s early 70s

86
00:03:08,160 --> 00:03:12,768
You know during this kind of Rapid change and a lot of cultural chain going on

87
00:03:13,280 --> 00:03:15,072
I became very interested in psychology

88
00:03:15,328 --> 00:03:16,352
So that was my first

89
00:03:16,608 --> 00:03:18,656
Technical interest just how did people

90
00:03:19,168 --> 00:03:23,520
Right people are clearly the issue and most of the problems we have today and

91
00:03:23,776 --> 00:03:26,080
How does that mind work and what could we do about

92
00:03:26,336 --> 00:03:26,848
Eliquis

93
00:03:27,104 --> 00:03:27,872
Starting pay

94
00:03:28,128 --> 00:03:29,152
And then I

95
00:03:29,408 --> 00:03:33,248
Did that went through got a master's degree in Psychology started work

96
00:03:33,504 --> 00:03:39,136
At Wright-Patterson Air Force Base as a human factors engineer which was one of the practical uses of that

97
00:03:39,392 --> 00:03:40,928
Psychology degree I had

98
00:03:41,952 --> 00:03:45,792
And then very quickly the first jobs I had were too

99
00:03:46,560 --> 00:03:52,192
Look at the human factors of how they were introducing electronic displays into the cockpit

100
00:03:52,448 --> 00:03:54,752
Does a 19 late 1970

101
00:03:55,520 --> 00:03:56,544
Kind of one of the first

102
00:03:57,568 --> 00:03:59,872
Are workplaces to be automated if you would

103
00:04:00,128 --> 00:04:02,944
Pilots were getting the most advanced technology

104
00:04:03,200 --> 00:04:06,016
They're putting electronic displays in the place of

105
00:04:06,272 --> 00:04:06,784
Instruments

106
00:04:07,040 --> 00:04:11,648
Pilots for making sub microsecond decision life-or-death decisions

107
00:04:11,904 --> 00:04:15,744
You want to make sure they get just the right information in just the right way

108
00:04:16,256 --> 00:04:17,536
You know to make those to say

109
00:04:18,047 --> 00:04:21,375
And so that triggered my interest in computer science

110
00:04:21,631 --> 00:04:27,007
Because quickly became apparent that all the important aspects of this job were in computer soft

111
00:04:27,519 --> 00:04:28,287
And so that

112
00:04:28,543 --> 00:04:34,687
Prompted my interest in computer science then went back to school to Stanford to get my master's degree in computer

113
00:04:34,943 --> 00:04:36,735
Are science and especially in AI

114
00:04:37,247 --> 00:04:40,319
That was one of ai's Summers if you will

115
00:04:40,831 --> 00:04:43,135
There's a hole in the 1980s

116
00:04:43,647 --> 00:04:47,999
A lot of interest in expert systems in one of the first waves of AI

117
00:04:48,511 --> 00:04:53,887
And so that captured my interest and certainly was aligned with my background in cognitive psychology

118
00:04:54,655 --> 00:04:57,471
David as I noted in my introduction you're now in your

119
00:04:57,727 --> 00:04:58,751
Third tour

120
00:04:59,263 --> 00:04:59,775
The program manager

121
00:05:00,031 --> 00:05:00,543
DARPA

122
00:05:00,799 --> 00:05:03,103
The death of the later we'll talk about the programs you

123
00:05:03,359 --> 00:05:04,639
Who inherited in this latest

124
00:05:05,151 --> 00:05:06,175
Communicating with computers

125
00:05:06,687 --> 00:05:11,039
And the program that you initiated the explaining a guy but before we get to

126
00:05:11,551 --> 00:05:15,391
I'd like for you to take us on a brief tour of your prior to

127
00:05:15,647 --> 00:05:16,415
What time does the program

128
00:05:16,927 --> 00:05:17,951
So let's start

129
00:05:18,207 --> 00:05:22,303
With how you and DARPA met for the very first time

130
00:05:22,559 --> 00:05:24,607
As a p.m. I guess this was a 19-9

131
00:05:26,143 --> 00:05:26,655
Mission systems

132
00:05:27,423 --> 00:05:32,031
What is the said as they started off as a psychologist then switch the computer science

133
00:05:32,799 --> 00:05:37,663
In both those roles I was a researcher at the Air Force research Labs at Wright-Patterson

134
00:05:38,687 --> 00:05:39,967
So doing a lot of work at

135
00:05:40,223 --> 00:05:42,783
The time on the F-22 that was the latest

136
00:05:43,039 --> 00:05:44,831
Fighter aircraft of the day

137
00:05:45,343 --> 00:05:47,647
Now those computer displays that started

138
00:05:48,159 --> 00:05:54,303
You know in the 1970s were very Advanced and in particular then I was working on a integrated

139
00:05:54,559 --> 00:05:55,839
Information maintenance system

140
00:05:56,607 --> 00:06:01,215
So now it was possible for the maintenance technician to walk up to a computer

141
00:06:01,727 --> 00:06:04,543
Walk up to the aircraft with a portable computer

142
00:06:04,799 --> 00:06:05,823
Plug it in

143
00:06:06,079 --> 00:06:09,151
Download information about what failures had occurred

144
00:06:09,663 --> 00:06:12,991
Get all of his tech manual instructions presented on the computer

145
00:06:13,503 --> 00:06:15,039
Create kind of an integrated

146
00:06:15,295 --> 00:06:20,415
You know workplace for the maintenance technician to help him do his job the question about that because

147
00:06:21,183 --> 00:06:21,695
You know when you go in

148
00:06:22,719 --> 00:06:24,767
Any garage tent have your own car looked

149
00:06:25,023 --> 00:06:25,535
Dad

150
00:06:25,791 --> 00:06:27,327
There is a system very much like that

151
00:06:27,583 --> 00:06:33,727
Yes it's about this procedure this procedure that it was actually the very first prototype of electronic

152
00:06:33,983 --> 00:06:35,263
Maintenance instructions anywhere

153
00:06:35,775 --> 00:06:36,543
The services

154
00:06:37,055 --> 00:06:37,823
I did that

155
00:06:38,079 --> 00:06:39,615
Did that time on my program

156
00:06:39,871 --> 00:06:46,015
And so I was doing that work that got the attention of the general who is running the F-22 program

157
00:06:46,527 --> 00:06:52,671
He pulled me into the F-22 the kind of man age in coordinate of the development of all of his digital

158
00:06:52,927 --> 00:06:53,439
Ecology

159
00:06:53,695 --> 00:06:58,047
And while I was there some people from DARPA came to visit the try to convince the skeptic

160
00:06:58,303 --> 00:06:58,815
General

161
00:06:59,327 --> 00:07:03,167
Oh that they should listen to DARPA and adopt some of their new technology

162
00:07:04,191 --> 00:07:06,751
So he expressed how skeptical he wasn't

163
00:07:07,007 --> 00:07:08,799
Pretty graphic way and said

164
00:07:09,567 --> 00:07:12,383
If you're going to do something on the F-22 you need to convince Dave

165
00:07:12,895 --> 00:07:13,663
He's my guide

166
00:07:14,175 --> 00:07:18,783
Computers that was with the Steve cross who is deputy director of one of the offices

167
00:07:19,295 --> 00:07:20,575
Gio wiederhold

168
00:07:20,831 --> 00:07:26,975
Who is a program manager here on loan from Stanford University who is doing a project in IntelliJ

169
00:07:27,231 --> 00:07:28,255
Integration of data set

170
00:07:29,023 --> 00:07:29,535
Using a

171
00:07:30,047 --> 00:07:33,887
And so I began to interact with those guys and when go left

172
00:07:34,399 --> 00:07:36,191
DARPA after his tour was done

173
00:07:36,959 --> 00:07:40,799
Then they invited me to come to DARPA to be a p.m. in 1994

174
00:07:41,311 --> 00:07:43,103
I just I find an interaction you mentioned

175
00:07:43,359 --> 00:07:46,431
The general quite interesting because it reflects a conservatism

176
00:07:46,687 --> 00:07:47,455
That

177
00:07:47,711 --> 00:07:48,223
The services

178
00:07:48,479 --> 00:07:51,039
Is have because they want to work with tried-and-true systems and it's a

179
00:07:51,295 --> 00:07:51,807
A bit scary

180
00:07:52,063 --> 00:07:52,831
Bring a new techno

181
00:07:53,087 --> 00:07:54,111
Knology which does not have as much

182
00:07:54,367 --> 00:07:55,903
Track record but then you were there

183
00:07:56,159 --> 00:07:58,207
As someone really a champion of the new

184
00:07:58,463 --> 00:08:02,559
Yes a champion of the new in a very conservative environment it's not that there

185
00:08:03,071 --> 00:08:07,935
Technology adverse these people are very serious Engineers are building this aircraft

186
00:08:08,191 --> 00:08:09,983
With enormous life and death

187
00:08:10,239 --> 00:08:12,287
You know consequences if it's done incorrectly

188
00:08:13,055 --> 00:08:15,359
So they don't want to put anything in that airplane

189
00:08:15,871 --> 00:08:17,663
Right unless they really know

190
00:08:17,919 --> 00:08:18,431
But they can depend on

191
00:08:18,943 --> 00:08:25,087
Give me a really solid understanding in background of where those people are coming from when you're trying to introduce new

192
00:08:25,343 --> 00:08:25,855
Technology

193
00:08:26,367 --> 00:08:27,903
Doing what you need to do to convince them

194
00:08:28,159 --> 00:08:30,207
One thing that I noticed regarding your this first

195
00:08:30,463 --> 00:08:32,255
Tour of Duty is that not only did you come here

196
00:08:32,511 --> 00:08:34,303
You went full Mind and Body in

197
00:08:34,559 --> 00:08:35,071
If I'm reading your

198
00:08:35,327 --> 00:08:36,351
Background correctly

199
00:08:36,607 --> 00:08:37,119
You were

200
00:08:37,631 --> 00:08:38,911
Over c9pro

201
00:08:39,679 --> 00:08:41,727
Released in your period of 6 years in that

202
00:08:42,751 --> 00:08:43,519
Torrent they were 9

203
00:08:43,775 --> 00:08:44,287
Find programs on your

204
00:08:44,799 --> 00:08:46,335
Yeah that's right I was starting

205
00:08:46,591 --> 00:08:49,663
One or two new programs a year inherited a few

206
00:08:49,919 --> 00:08:52,735
You know from other PMS but I was all-in

207
00:08:53,247 --> 00:08:56,063
I was a lot younger then had a lot more energy and

208
00:08:56,575 --> 00:08:57,599
But just love the job

209
00:08:57,855 --> 00:08:59,391
She mention I've been through three times

210
00:08:59,903 --> 00:09:02,719
Cuz to me it's one of the most fascinating jobs you can have

211
00:09:03,999 --> 00:09:06,303
Pinnacle of new technology development

212
00:09:06,815 --> 00:09:09,375
You have such a broad view of what's going on

213
00:09:09,631 --> 00:09:15,007
And DARPA gives you this incredible freedom to pursue what you see is you know what division ennui

214
00:09:15,263 --> 00:09:15,775
Where things should go

215
00:09:16,031 --> 00:09:17,311
That one of the standouts

216
00:09:17,567 --> 00:09:18,079
From the

217
00:09:18,591 --> 00:09:20,639
First work is known as command post of the

218
00:09:20,895 --> 00:09:21,407
The future

219
00:09:21,663 --> 00:09:23,455
And that one actually did get out into

220
00:09:23,711 --> 00:09:26,271
To the field so just briefly can you tell us a little bit about what the

221
00:09:26,527 --> 00:09:28,831
Goal of that program wasn't and infect how

222
00:09:29,087 --> 00:09:29,855
Howitzer

223
00:09:30,111 --> 00:09:30,879
It unfolded

224
00:09:31,135 --> 00:09:37,279
And of course my interest has been an AI all along and most of those nine programs I ran my first tour had

225
00:09:37,535 --> 00:09:39,839
Had to do with knowledge and reasoning

226
00:09:40,095 --> 00:09:40,863
Planning

227
00:09:41,119 --> 00:09:43,679
Trying to provide intelligent decision AIDS

228
00:09:43,935 --> 00:09:46,751
Two people so we created command post of the future

229
00:09:47,519 --> 00:09:49,823
You know as a way to see if we could

230
00:09:50,079 --> 00:09:51,103
Explorer

231
00:09:51,359 --> 00:09:53,151
All sorts of AI technology

232
00:09:53,919 --> 00:09:54,943
See what worked

233
00:09:55,455 --> 00:09:58,783
Package that up into a system that Army Commanders

234
00:09:59,039 --> 00:10:04,415
Could use out in the field so we started that with a wide variety of Technologies of varying

235
00:10:04,671 --> 00:10:09,023
Degrees of maturity and risk and by time we got to the end of the program

236
00:10:09,535 --> 00:10:12,095
And I have left are but by then and there was

237
00:10:12,863 --> 00:10:14,655
Other people have taken over

238
00:10:15,167 --> 00:10:18,239
But the technology that survived was something that was

239
00:10:18,495 --> 00:10:21,311
Might call adaptive or intelligent visualization

240
00:10:21,823 --> 00:10:25,919
A lot of the more higher-risk AI techniques that actually kind of falling out this

241
00:10:26,175 --> 00:10:26,687
Too immature

242
00:10:27,199 --> 00:10:27,711
Bright

243
00:10:27,967 --> 00:10:32,831
But they had through a lot of iteration with some Visionary people from the

244
00:10:33,087 --> 00:10:34,623
Army Marine Corps Air Force

245
00:10:34,879 --> 00:10:37,951
And really kind of picked out the Nuggets of new technology

246
00:10:38,207 --> 00:10:39,231
They were incredibly value

247
00:10:40,255 --> 00:10:45,375
Basically was that if a commander and his staff could each have their own view of the battlefield

248
00:10:45,887 --> 00:10:47,935
In their command post the future screen

249
00:10:48,703 --> 00:10:50,751
So the operator saw the plans for the attack

250
00:10:51,519 --> 00:10:54,591
The logistics guy then so what did he have to do to support the

251
00:10:55,615 --> 00:10:57,663
Write the Intel guy just got to see

252
00:10:58,175 --> 00:11:00,735
What information was coming in on what the enemy would do neck

253
00:11:00,991 --> 00:11:05,343
In one of the commanders Army General Corelli who is the one who

254
00:11:05,599 --> 00:11:07,391
Champion didn't took it to Baghdad

255
00:11:08,415 --> 00:11:11,487
Ashley said that by watching his staff's

256
00:11:12,255 --> 00:11:14,303
Command post of the future screens

257
00:11:15,071 --> 00:11:17,631
He had a better idea of what they were thinking

258
00:11:18,143 --> 00:11:20,959
Then he did if they were all in the same room talking to

259
00:11:22,239 --> 00:11:25,567
So would enable them to one be distributed which is very important

260
00:11:26,079 --> 00:11:26,591
Four the battle for

261
00:11:27,103 --> 00:11:31,199
The two gave him better insight into what his staff was thinking

262
00:11:31,711 --> 00:11:33,247
And where the bad how the battle

263
00:11:33,503 --> 00:11:34,015
Was emerging

264
00:11:34,783 --> 00:11:35,551
Fasting

265
00:11:36,063 --> 00:11:37,855
This event is a case where the

266
00:11:38,111 --> 00:11:40,159
Human machine interaction

267
00:11:40,927 --> 00:11:43,231
Something into something more than the

268
00:11:43,999 --> 00:11:47,327
Yes and parley critical part of that programs the way we set it up

269
00:11:47,583 --> 00:11:50,143
Was the have is constant iteration

270
00:11:50,655 --> 00:11:51,935
Between the technologist

271
00:11:52,703 --> 00:11:54,239
And the Visionary users

272
00:11:55,007 --> 00:11:57,055
Right which is your Now call Dad Jewel

273
00:11:58,591 --> 00:11:59,871
Development there's a lot of

274
00:12:00,127 --> 00:12:03,711
Names that has pretty much been adopted by Silicon Valley as a religion

275
00:12:04,223 --> 00:12:07,039
But that strategy was pretty early in those days

276
00:12:07,807 --> 00:12:10,111
And they had a lot of exercises where they

277
00:12:10,367 --> 00:12:10,879
Play take the

278
00:12:11,135 --> 00:12:12,927
Researchers from the University

279
00:12:13,183 --> 00:12:18,815
Send them down to Fort Polk in the Army and actually put on the army gear and run them to exercise

280
00:12:20,095 --> 00:12:21,119
Realsense for what

281
00:12:21,375 --> 00:12:22,143
The problem was

282
00:12:22,655 --> 00:12:28,799
But they also had exercises where the Visionary military guys would work closely with the technologist

283
00:12:29,055 --> 00:12:32,895
To try to sort out what was the most valuable thing the technology could do

284
00:12:33,151 --> 00:12:34,175
For user in the field

285
00:12:34,431 --> 00:12:36,479
And not just let the technologist kind of

286
00:12:36,735 --> 00:12:38,783
Hypothesize what that might be

287
00:12:39,039 --> 00:12:40,575
With a somewhat naive idea

288
00:12:41,087 --> 00:12:41,599
But what's needed

289
00:12:42,111 --> 00:12:44,671
Some 2,000 you leave. But you go and do other things

290
00:12:45,183 --> 00:12:46,719
And in 2003 though

291
00:12:46,975 --> 00:12:47,743
You come back

292
00:12:47,999 --> 00:12:49,279
And this time you're in the information

293
00:12:49,535 --> 00:12:50,047
Processing

294
00:12:51,327 --> 00:12:51,839
Office

295
00:12:52,351 --> 00:12:53,631
Tell us a little bit about

296
00:12:54,143 --> 00:12:55,423
The programs that you

297
00:12:55,935 --> 00:12:58,239
Ram there including one that I think many

298
00:12:58,495 --> 00:12:59,007
Listeners

299
00:13:00,287 --> 00:13:01,823
We came back as they say

300
00:13:02,079 --> 00:13:04,383
Tony tether just come in as the new director

301
00:13:05,151 --> 00:13:09,503
Ron Brockman as the office Chiefs of this newly-formed office if

302
00:13:10,783 --> 00:13:16,927
And It Go was also the name of one of the original dark offices led by lickliter

303
00:13:17,439 --> 00:13:21,791
Is really where the personal computer was created where a whole idea of

304
00:13:22,047 --> 00:13:26,399
If those days are computer metal large Ivy and run frame just for listeners

305
00:13:26,655 --> 00:13:27,935
Lickliter is a

306
00:13:28,191 --> 00:13:31,263
Giant in the history of Dart but he's known as

307
00:13:31,519 --> 00:13:34,335
Usually is jcr licklider actually stands

308
00:13:36,127 --> 00:13:37,919
Joseph Carl robnett

309
00:13:38,175 --> 00:13:38,687
Lickliter

310
00:13:38,943 --> 00:13:41,247
And he really brought in an ethic of

311
00:13:41,503 --> 00:13:44,831
This idea that the human machine interaction in a symbiosis

312
00:13:45,087 --> 00:13:47,391
This was going to be a long long theme of

313
00:13:48,159 --> 00:13:49,439
LG from then in the sixties when he

314
00:13:50,207 --> 00:13:50,719
Until now

315
00:13:51,231 --> 00:13:56,095
So David your View continuing the age of exactly I did before he came in

316
00:13:56,607 --> 00:14:01,215
Did what a computer meant was an IBM Mainframe that took up the size of a small room

317
00:14:01,983 --> 00:14:04,031
And people would interact with it by

318
00:14:04,287 --> 00:14:06,847
Sending Punch Cards into a punch card reader

319
00:14:07,103 --> 00:14:09,151
And getting an answer back on the other side

320
00:14:09,663 --> 00:14:15,807
There was no concept of this being a personal computer you'd sit down with and interact with on a daily basis

321
00:14:16,575 --> 00:14:19,391
It was actually under his program is Alan Kay

322
00:14:19,903 --> 00:14:22,719
Who is that that Stanford at the time in later Park

323
00:14:23,231 --> 00:14:24,511
Sketched out the first

324
00:14:25,023 --> 00:14:27,839
Diagrams of what we recognize today is the person

325
00:14:28,095 --> 00:14:30,399
Brighton park being Palo Alto Research Center

326
00:14:30,655 --> 00:14:31,167
Exactly

327
00:14:32,191 --> 00:14:34,495
And Tony Rich so that was zip to ISO

328
00:14:34,751 --> 00:14:37,823
Tony Taylor and Ron Brockman they really wanted to bring that back

329
00:14:38,079 --> 00:14:38,847
They wanted

330
00:14:39,103 --> 00:14:43,455
To re-energize DARPA to go after the Next Generation

331
00:14:43,711 --> 00:14:45,247
Human-computer symbiosis

332
00:14:46,015 --> 00:14:48,319
And the phrase then was cognitive system

333
00:14:48,575 --> 00:14:49,599
Cognitive technology

334
00:14:49,855 --> 00:14:53,183
And so Tony believed investing big in this

335
00:14:53,951 --> 00:14:57,535
And so we came up with the idea for this program called pal

336
00:14:58,047 --> 00:15:00,095
Personalized assistant that learns

337
00:15:00,607 --> 00:15:06,751
Which even by DARPA standards was a fairly large program 250 million dollars over 5 years

338
00:15:07,007 --> 00:15:08,543
And it had multiple levels

339
00:15:08,799 --> 00:15:10,335
One level that funded

340
00:15:11,103 --> 00:15:15,199
20 some research universities and agencies around the country

341
00:15:15,711 --> 00:15:19,551
To explore a lot of cutting-edge technology in AI

342
00:15:20,063 --> 00:15:21,599
In particular machine learning

343
00:15:21,855 --> 00:15:24,159
Machine learning is now commonplace today

344
00:15:24,671 --> 00:15:27,487
Was just kind of emerging technology in those days

345
00:15:27,999 --> 00:15:32,095
So it funded a huge amount of basic research on machine learning

346
00:15:32,351 --> 00:15:33,887
Let me just stop you for a second

347
00:15:34,143 --> 00:15:34,655
Listening to me

348
00:15:34,911 --> 00:15:36,703
Not really know what that means

349
00:15:36,959 --> 00:15:38,239
We can't go into a full

350
00:15:40,543 --> 00:15:41,311
How would you define that

351
00:15:41,567 --> 00:15:46,431
Willne earlier days of AI we tried to develop intelligent systems by having people

352
00:15:47,199 --> 00:15:48,479
Program rules

353
00:15:48,735 --> 00:15:50,783
Soar statements into a system

354
00:15:51,295 --> 00:15:54,111
That would tell it what the knowledge was unleaded then reason why

355
00:15:54,623 --> 00:15:58,719
An expert system with electric systems knowledge bases

356
00:15:59,999 --> 00:16:00,767
TurboTax

357
00:16:01,023 --> 00:16:05,119
Text today is kind of a current Incarnation shuttling program

358
00:16:05,375 --> 00:16:05,887
Exactly

359
00:16:06,143 --> 00:16:12,287
Those ended up being very difficult very difficult to program very difficult to maintain it's very difficult for people to

360
00:16:12,543 --> 00:16:13,823
But actually specify

361
00:16:14,335 --> 00:16:16,383
Exactly the logic rules

362
00:16:16,895 --> 00:16:17,919
They're behind intelligence

363
00:16:18,175 --> 00:16:24,319
But over those decades and especially during this pal program began to realize that if you fed a computer Lotto

364
00:16:24,575 --> 00:16:25,087
Data

365
00:16:25,599 --> 00:16:28,927
In turned it Loose with the right machine learning algorithms which were

366
00:16:29,183 --> 00:16:29,951
We're kind of learning

367
00:16:30,463 --> 00:16:32,255
The statistics and probabilistic

368
00:16:32,511 --> 00:16:33,791
Nature of the data

369
00:16:34,303 --> 00:16:37,375
And learning different recognition patterns

370
00:16:37,887 --> 00:16:39,935
The computer could learn its own patterns

371
00:16:40,447 --> 00:16:42,239
They're not specified by a person

372
00:16:42,751 --> 00:16:46,079
But those patterns were much more effective at doing a lot of jobs

373
00:16:46,335 --> 00:16:49,663
Stand like understanding speech or recognizing images are

374
00:16:50,431 --> 00:16:50,943
You know

375
00:16:51,199 --> 00:16:52,479
Finding credit card fraud

376
00:16:52,991 --> 00:16:53,503
Xcetera

377
00:16:54,015 --> 00:16:57,599
Answer machine learning was much more effective than the older style

378
00:16:58,111 --> 00:16:58,623
Explicit

379
00:16:59,135 --> 00:17:00,159
Logic rules

380
00:17:00,671 --> 00:17:03,487
So in a sense with the computer was doing is taking those

381
00:17:03,999 --> 00:17:04,511
Assertive

382
00:17:04,767 --> 00:17:06,047
Analog signal

383
00:17:06,303 --> 00:17:07,327
That we humans are coming

384
00:17:08,607 --> 00:17:10,399
Looking them now in there

385
00:17:10,911 --> 00:17:11,679
Beta version

386
00:17:12,191 --> 00:17:13,471
Right and that's what they're comfort

387
00:17:13,983 --> 00:17:14,495
Yes

388
00:17:15,007 --> 00:17:15,519
Exactly

389
00:17:16,031 --> 00:17:17,823
And so that was a big component of the program

390
00:17:18,335 --> 00:17:21,407
We also did a lot of work on man-machine dialogue

391
00:17:21,663 --> 00:17:22,175
Gone

392
00:17:22,431 --> 00:17:28,575
You know how do you carry on an intelligent dialogue with the computer the speech understanding to some extent was already being

393
00:17:28,831 --> 00:17:32,159
How would you have a computer that could be an intelligent assistant

394
00:17:32,671 --> 00:17:34,463
It would watch what the user is doing

395
00:17:34,975 --> 00:17:36,255
Understand his job

396
00:17:36,767 --> 00:17:39,583
Then be able to give him assistants at the right time

397
00:17:39,839 --> 00:17:41,119
And one of the

398
00:17:41,375 --> 00:17:47,519
Metaphors we used as you want to do create an assistant that was like Sergeant O'Reilly on the old TV show

399
00:17:47,775 --> 00:17:48,287
Show MASH

400
00:17:48,799 --> 00:17:50,847
For those of you old enough to remember

401
00:17:51,103 --> 00:17:54,431
O'Reilly always knew what the kernel needed before the kernel

402
00:17:54,687 --> 00:17:55,711
Need new himself

403
00:17:55,967 --> 00:17:56,735
Right so we just

404
00:17:57,503 --> 00:18:01,599
Followed the context of what the user was doing could anticipate

405
00:18:01,855 --> 00:18:02,623
And provide

406
00:18:02,879 --> 00:18:03,647
Intelligent advice

407
00:18:04,159 --> 00:18:09,023
So that was the Grand Vision and we funded a lot of it very Advanced research and all of those areas

408
00:18:10,047 --> 00:18:12,607
And in one of our performers was the

409
00:18:12,863 --> 00:18:16,703
SRA International out in Menlo Park California

410
00:18:17,215 --> 00:18:19,263
They were one of the integrators of this they

411
00:18:19,775 --> 00:18:22,335
They are looked at all the research was being developed

412
00:18:22,847 --> 00:18:24,383
Tried to create a prototype

413
00:18:24,639 --> 00:18:30,015
Of this pal system that would could actually be used as an intelligent assistant on your laptop

414
00:18:30,271 --> 00:18:31,551
So that they were looking at

415
00:18:32,063 --> 00:18:32,831
Many of the

416
00:18:33,087 --> 00:18:36,159
Component technologies that the other performers were were developing in C

417
00:18:36,415 --> 00:18:36,927
See how can we put this

418
00:18:37,439 --> 00:18:43,583
Exactly and I was really their job was how do we integrate the best of all these techniques and put

419
00:18:43,839 --> 00:18:44,863
Bad together in night

420
00:18:45,119 --> 00:18:47,423
Real prototype of a condom

421
00:18:47,935 --> 00:18:52,287
And so they as long as they were doing that began to develop a

422
00:18:53,055 --> 00:18:56,127
Kind of streamline more efficient version of that Arkansas

423
00:18:57,407 --> 00:19:01,759
And when the power program the research version the power program ended around

424
00:19:02,015 --> 00:19:03,039
2008

425
00:19:03,807 --> 00:19:07,391
They had several people who'd worked on Powell and some

426
00:19:07,647 --> 00:19:10,975
New performers they brought informed Adventure called Siri

427
00:19:11,487 --> 00:19:14,047
And they decided they were going to take this technology

428
00:19:14,559 --> 00:19:15,071
Adapted

429
00:19:15,327 --> 00:19:17,887
For the iPhone which was a brand new product

430
00:19:19,167 --> 00:19:20,959
And people didn't know quite what to do with it

431
00:19:21,727 --> 00:19:24,287
Azarai was very wisely said

432
00:19:24,543 --> 00:19:26,591
This is the place for a cognitivist

433
00:19:27,359 --> 00:19:29,151
People going to have this phone they won't have a

434
00:19:29,407 --> 00:19:29,919
Keyboard

435
00:19:30,175 --> 00:19:32,991
They're really going to need an assistant that can help them out

436
00:19:33,759 --> 00:19:37,599
So they had this Venture and for two years it was just funded as a new startup

437
00:19:38,111 --> 00:19:39,647
DARPA then was out of the picture

438
00:19:40,159 --> 00:19:43,743
They got Venture money from Silicon Valley to develop this

439
00:19:44,255 --> 00:19:48,863
And in two years later they put their first Siri application on the App Store

440
00:19:49,887 --> 00:19:50,911
Napster was brand new

441
00:19:51,423 --> 00:19:55,775
And I think it was three days later they got a call from Steve Jobs himself

442
00:19:57,055 --> 00:19:59,103
Who said it first they're like washer this is.

443
00:19:59,871 --> 00:20:02,431
Try to just cause I didn't want random phones

444
00:20:02,687 --> 00:20:04,223
You know in the office after they

445
00:20:04,991 --> 00:20:07,039
Began to believe it really was Steve Jobs

446
00:20:07,807 --> 00:20:12,415
He asked the three founders he said why don't you come over to my house tonight for dinner I've got a

447
00:20:12,671 --> 00:20:13,951
Vision for what I want to do with

448
00:20:15,231 --> 00:20:17,791
And then there was some negotiation / selling

449
00:20:18,047 --> 00:20:18,559
The company

450
00:20:19,071 --> 00:20:19,839
Apple bought it

451
00:20:20,351 --> 00:20:23,423
It's been several years developing it into the Sirius system

452
00:20:23,679 --> 00:20:24,447
Aquino today

453
00:20:25,215 --> 00:20:30,079
And for the first year of that Steve Jobs was personally involved in directing

454
00:20:30,335 --> 00:20:31,359
He had really seen that

455
00:20:31,871 --> 00:20:34,943
As a vision for he wanted to go but then got too well

456
00:20:36,223 --> 00:20:38,271
Okay so before we move on now to your

457
00:20:38,527 --> 00:20:39,039
Current

458
00:20:40,063 --> 00:20:44,415
What you described she was sort of the archetypal technology development cycle

459
00:20:44,671 --> 00:20:47,231
At DARPA when it's about a sex successful at the

460
00:20:47,743 --> 00:20:48,767
Yet which is to say

461
00:20:49,279 --> 00:20:50,815
Yeah you and others here in your

462
00:20:51,071 --> 00:20:52,095
Your performers those who work

463
00:20:52,351 --> 00:20:53,887
Under contract on your program

464
00:20:54,143 --> 00:20:56,703
They brought a technology to a point of proteus

465
00:20:57,471 --> 00:20:57,983
And

466
00:20:58,239 --> 00:20:59,519
Then taking the risk

467
00:21:00,031 --> 00:21:02,591
Down enough that say other corporate entities or

468
00:21:02,847 --> 00:21:03,871
With the military services

469
00:21:04,127 --> 00:21:04,895
Can take a handoff

470
00:21:05,407 --> 00:21:07,199
And do the final development until we

471
00:21:08,479 --> 00:21:09,247
Product eggnog

472
00:21:09,759 --> 00:21:10,527
Turn off the dark

473
00:21:10,783 --> 00:21:11,807
What goes up to that point of

474
00:21:12,063 --> 00:21:12,575
The product

475
00:21:14,111 --> 00:21:14,623
Prototype

476
00:21:15,135 --> 00:21:15,647
Out with proof

477
00:21:15,903 --> 00:21:17,440
The principal hears a case where

478
00:21:17,696 --> 00:21:18,464
All of this works

479
00:21:18,720 --> 00:21:21,536
But you did led to something that now is part of

480
00:21:21,792 --> 00:21:22,816
The Techno scape

481
00:21:23,328 --> 00:21:23,840
Of the world

482
00:21:24,608 --> 00:21:27,168
So the question I have for you is just what does that

483
00:21:27,424 --> 00:21:32,800
Feel like I'm here you are you you here Siri all over the place and you know that you had

484
00:21:33,056 --> 00:21:34,592
A role in the sequence

485
00:21:34,848 --> 00:21:35,360
That led to it

486
00:21:35,616 --> 00:21:36,128
So

487
00:21:36,896 --> 00:21:37,920
What feels great

488
00:21:38,176 --> 00:21:41,248
It was quite a surprise in a way right as you save this

489
00:21:41,504 --> 00:21:44,832
That's darpa's job we're always doing this advanced technology

490
00:21:45,344 --> 00:21:46,880
We're always trying to reduce the risk

491
00:21:47,648 --> 00:21:49,952
You know and we knew a personal assistant

492
00:21:50,720 --> 00:21:51,232
Headband

493
00:21:51,488 --> 00:21:54,304
That's been a part of the AI science fiction

494
00:21:54,560 --> 00:21:55,072
Sense

495
00:21:55,328 --> 00:21:57,888
2001 and Star Trek in

496
00:21:58,144 --> 00:22:02,240
That's always at like why certainly will be able to interact talk to her computer someday

497
00:22:02,752 --> 00:22:08,896
But it never quite worked but I don't never know how l9ng vanilla

498
00:22:09,152 --> 00:22:10,688
We want the pod bay doors

499
00:22:12,224 --> 00:22:18,112
But the idea of a computer that was intelligent enough to carry on a conversation being assistant had been around for

500
00:22:19,136 --> 00:22:20,160
But never implemented

501
00:22:20,928 --> 00:22:23,232
When people tried it didn't work well enough

502
00:22:23,744 --> 00:22:25,536
There's always a little bit too kludgy

503
00:22:26,048 --> 00:22:32,192
And I think that that pal program really brought it to the point that Steve Jobs was willing to take a risk and say yeah I can make

504
00:22:32,448 --> 00:22:32,960
Make a real Prada

505
00:22:33,984 --> 00:22:36,288
And I can say I left Tarpon 2008

506
00:22:36,800 --> 00:22:38,848
It's two years later that this comes out

507
00:22:39,360 --> 00:22:42,944
And renewed Apple had bought it but had no idea what they were going to do

508
00:22:43,456 --> 00:22:46,784
And then suddenly it's Apple's marketing department is advertising

509
00:22:47,040 --> 00:22:47,552
The product of my

510
00:22:47,808 --> 00:22:48,320
I program

511
00:22:48,832 --> 00:22:49,600
And that was very nice

512
00:22:50,368 --> 00:22:51,136
Like my children even

513
00:22:53,184 --> 00:22:53,952
Momentarily

514
00:22:54,464 --> 00:22:56,256
The greatest achievement of all right

515
00:22:56,768 --> 00:22:59,584
So let's then move now to your current

516
00:23:00,864 --> 00:23:03,936
Here in 2015 that you left in 2018 you want to wait either.

517
00:23:04,192 --> 00:23:04,704
Other things in

518
00:23:04,960 --> 00:23:07,776
Industry and you came back in 2015 so again

519
00:23:08,032 --> 00:23:10,848
Tell me that that approach how it is that you were convinced to come back

520
00:23:11,104 --> 00:23:12,640
We'll talk about your

521
00:23:12,896 --> 00:23:13,408
To program

522
00:23:13,664 --> 00:23:19,040
Well one is I said I didn't just being a darpa's the best job I've ever had

523
00:23:19,296 --> 00:23:23,136
Wright's interesting to go out and Industry its industry to be in other places

524
00:23:23,648 --> 00:23:28,256
But if you enjoy technology and you really want to make an impact there's nothing like

525
00:23:28,512 --> 00:23:29,024
Like being a dark

526
00:23:30,560 --> 00:23:35,424
I was going to head been out for a while in a variety of jobs and Industry I was

527
00:23:35,936 --> 00:23:37,984
Kind of ready to looking for something to do

528
00:23:38,240 --> 00:23:42,592
The other Factor though is my son who lives in Roslyn nearby

529
00:23:42,848 --> 00:23:44,128
Just had her first grandchild

530
00:23:44,640 --> 00:23:49,248
And so that convince my wife it was worth moving from Seattle where are bases

531
00:23:50,272 --> 00:23:51,552
Back to DC

532
00:23:51,808 --> 00:23:53,088
So I got a great job

533
00:23:53,344 --> 00:23:55,136
My wife got to play with our grandson

534
00:23:56,160 --> 00:23:56,672
So that was

535
00:23:57,952 --> 00:24:00,256
Pure justification right there no need for anything else

536
00:24:01,024 --> 00:24:07,168
It's so let's talk about the to program 1-1 you inherited that's communicating with computers WC right

537
00:24:08,192 --> 00:24:09,472
Tell me a little bit about the gold

538
00:24:09,984 --> 00:24:16,128
Well in a sense it's kind of do the next generation of Siri and Alexa in those

539
00:24:16,384 --> 00:24:16,896
Sorts of things

540
00:24:17,152 --> 00:24:19,200
We're now that technology is there

541
00:24:19,456 --> 00:24:25,600
You know what can understand what you say for the most part not always but for the most part and can provide you know minor

542
00:24:26,112 --> 00:24:28,928
Assistance can set an alarm can play a song

543
00:24:29,440 --> 00:24:30,976
Right help you find directions

544
00:24:31,744 --> 00:24:35,584
Ncwc we really want to raise the level of intelligence

545
00:24:35,840 --> 00:24:37,632
In Partnership of the computer

546
00:24:38,144 --> 00:24:40,448
So it is really intelligent partner

547
00:24:40,960 --> 00:24:45,824
That's you know what joint partner with the human it's really human-computer symbiosis

548
00:24:46,080 --> 00:24:47,872
Not human is the manager

549
00:24:48,128 --> 00:24:49,152
Iran computer

550
00:24:49,408 --> 00:24:50,176
As a low level of

551
00:24:50,944 --> 00:24:51,712
So that's the goal

552
00:24:51,968 --> 00:24:54,528
Let's stay here for just one more minute what's a kind of

553
00:24:54,784 --> 00:24:56,576
Question that I might be able to ask

554
00:24:56,832 --> 00:24:59,904
After a successful CWC communicating with

555
00:25:00,160 --> 00:25:00,672
Computers Pro

556
00:25:01,184 --> 00:25:01,696
That I can tell

557
00:25:02,464 --> 00:25:02,976
Say Siri now

558
00:25:03,232 --> 00:25:05,792
Let me not you Siri but you was one of the examples from the program

559
00:25:06,304 --> 00:25:07,840
Is the Harvard Medical School

560
00:25:08,608 --> 00:25:12,192
They have done some work on a previous star program called Big mechanism

561
00:25:12,960 --> 00:25:16,032
Where they are trying to create a system

562
00:25:16,800 --> 00:25:19,872
That could read masses of research in biology

563
00:25:20,896 --> 00:25:24,480
And then automatically construct models of cancer pathways

564
00:25:24,736 --> 00:25:28,064
Instead of had the human biologist having to read thousands of

565
00:25:28,576 --> 00:25:29,088
Research art

566
00:25:29,600 --> 00:25:31,392
The computer would do it themselves

567
00:25:31,904 --> 00:25:37,280
Ingrate suggestions of new Pathways offer how cancer behaves in a cell

568
00:25:37,536 --> 00:25:39,840
To the biologist would have that as a suggestion

569
00:25:40,352 --> 00:25:43,936
So now the CWC part of it is can we put a collaborative

570
00:25:44,192 --> 00:25:45,472
User interface on top

571
00:25:46,496 --> 00:25:48,800
So now the biologists can collaborate with

572
00:25:49,056 --> 00:25:49,568
What's a computer

573
00:25:49,824 --> 00:25:51,872
To hypothesize how cancer works

574
00:25:52,128 --> 00:25:53,920
Christmas elf to design experiment

575
00:25:54,432 --> 00:25:54,944
2

576
00:25:55,200 --> 00:25:55,968
You know Soda

577
00:25:56,736 --> 00:25:59,040
The computer is working as a virtual

578
00:25:59,296 --> 00:26:01,600
Scientist the lab assistant if you will or

579
00:26:01,856 --> 00:26:03,648
Grad student in a biology lab

580
00:26:04,416 --> 00:26:06,720
Who's really helping to digest the literature

581
00:26:06,976 --> 00:26:09,024
Pull out the important nuggets

582
00:26:09,792 --> 00:26:10,304
And

583
00:26:10,560 --> 00:26:11,072
Suggest

584
00:26:11,328 --> 00:26:11,840
Experiments

585
00:26:12,096 --> 00:26:13,120
I'm getting this.

586
00:26:13,376 --> 00:26:14,912
I think right that the value-added is

587
00:26:15,168 --> 00:26:16,448
If I'm a scientist in neuroscience

588
00:26:17,472 --> 00:26:20,288
And we each spent a month immersed in the literature

589
00:26:20,544 --> 00:26:22,848
You've read a hundred papers and I brought hundred paper

590
00:26:23,104 --> 00:26:23,872
But if I'm working with a

591
00:26:24,128 --> 00:26:26,176
Computer the computer might have read 10000

592
00:26:26,432 --> 00:26:27,456
Papers were exactly

593
00:26:27,712 --> 00:26:28,992
And now it has enough

594
00:26:29,248 --> 00:26:31,808
Intelligence and has the right kind of

595
00:26:32,064 --> 00:26:36,928
Dialog Machinery so it can carry on a much more intelligent conversation with you

596
00:26:37,696 --> 00:26:42,048
Today if it was Siri or you going to confirm technology and might find me articles on

597
00:26:42,816 --> 00:26:43,840
You know certain pathway

598
00:26:44,096 --> 00:26:46,144
Ryder find me articles about a certain protein

599
00:26:46,912 --> 00:26:48,192
Computer could do that

600
00:26:48,448 --> 00:26:51,264
But it's really not going to absorb that information integrated

601
00:26:51,776 --> 00:26:54,848
And in carry-on kind of more give-and-take conversation

602
00:26:55,872 --> 00:26:58,432
What's a now let's move on to the program that I suspect is taking

603
00:26:58,944 --> 00:26:59,968
Most of your time because

604
00:27:00,992 --> 00:27:01,504
Play my mind is

605
00:27:02,528 --> 00:27:03,040
Seven performers

606
00:27:04,064 --> 00:27:05,088
And this is called

607
00:27:05,344 --> 00:27:06,368
Explainable

608
00:27:06,624 --> 00:27:07,136
Artificial

609
00:27:08,672 --> 00:27:09,184
AI

610
00:27:09,440 --> 00:27:12,256
So talk about the impetus for the program I guess

611
00:27:12,512 --> 00:27:13,024
First

612
00:27:13,536 --> 00:27:14,304
And then

613
00:27:14,816 --> 00:27:15,584
Talk to me about

614
00:27:15,840 --> 00:27:16,608
This status

615
00:27:17,632 --> 00:27:21,728
Well first I said I came back in 2015 you know

616
00:27:21,984 --> 00:27:25,568
I talked to the in-office director Dan Kaufman about

617
00:27:26,080 --> 00:27:29,408
Coming back to DARPA and and so is his

618
00:27:29,664 --> 00:27:31,456
Give you listeners probably no

619
00:27:31,712 --> 00:27:36,320
My program manager comes to Darfur the question is what's your vision what project you want to do

620
00:27:36,832 --> 00:27:39,392
You know that that's kind of a job interview better do it fast

621
00:27:39,648 --> 00:27:43,232
Because it said you already have your sell-by date on your badge right you're here for a few years

622
00:27:43,744 --> 00:27:45,792
And so Dan head

623
00:27:46,048 --> 00:27:48,608
Actually suggested this idea of explainable

624
00:27:48,864 --> 00:27:49,376
AI

625
00:27:49,632 --> 00:27:54,496
I had just been to a workshop with a group of Intel analyst from.

626
00:27:54,752 --> 00:27:57,056
And it was a group of machine learning and

627
00:27:57,568 --> 00:27:59,616
HCI visualization people

628
00:28:00,128 --> 00:28:03,712
We're trying to convince this group ball the great new technology we could

629
00:28:03,968 --> 00:28:04,480
Provide

630
00:28:04,736 --> 00:28:06,016
To help Intel analysis

631
00:28:07,040 --> 00:28:09,600
And one of the ladies there who was an analyst at

632
00:28:10,624 --> 00:28:11,904
One of the dod agency

633
00:28:12,416 --> 00:28:13,696
Said that's not a problem

634
00:28:14,208 --> 00:28:17,280
Her probably she already has big data analytics systems

635
00:28:17,536 --> 00:28:18,816
They're recommending

636
00:28:19,328 --> 00:28:21,632
Suggesting patterns are Target's

637
00:28:22,400 --> 00:28:23,680
You know suspects to her

638
00:28:24,192 --> 00:28:28,032
But she has to put her name on the recommendation that goes up the chain

639
00:28:28,800 --> 00:28:29,824
And she gets graded

640
00:28:30,080 --> 00:28:31,104
If not punished

641
00:28:31,616 --> 00:28:33,152
If that recommendation is wrong

642
00:28:33,920 --> 00:28:35,456
And she wanted these systems to it

643
00:28:35,712 --> 00:28:37,504
Reminder the rationale

644
00:28:38,016 --> 00:28:39,296
For making the decisions that

645
00:28:39,808 --> 00:28:41,600
This was for her how she was going

646
00:28:41,856 --> 00:28:42,368
Trust them

647
00:28:42,624 --> 00:28:43,392
The machine

648
00:28:43,904 --> 00:28:45,440
Trusted in you in a window

649
00:28:45,696 --> 00:28:48,512
When is the machine when is this a false alarm when is it

650
00:28:48,768 --> 00:28:49,792
Positive example like

651
00:28:50,048 --> 00:28:50,560
I should trust

652
00:28:51,072 --> 00:28:51,840
Into her

653
00:28:52,352 --> 00:28:53,888
Having the explanation

654
00:28:54,144 --> 00:28:55,168
Would what's really needed

655
00:28:56,960 --> 00:29:03,104
And so that resonated and it's about that time is when the new wave of AI which is deep learning

656
00:29:03,360 --> 00:29:04,128
Just taking off

657
00:29:04,640 --> 00:29:05,152
Bright

658
00:29:05,664 --> 00:29:11,552
And you hear about this cold kind of new Revolution and AI That's all about these deep learning techniques

659
00:29:12,064 --> 00:29:16,672
What are kind of the most advanced of this machine learning technology we talked about earlier

660
00:29:17,440 --> 00:29:19,744
We're takes in huge volumes of data

661
00:29:20,512 --> 00:29:23,840
These deep learning systems that are very large neural Nets

662
00:29:24,096 --> 00:29:25,632
With millions of neurons if

663
00:29:26,656 --> 00:29:28,448
They're each learning weights

664
00:29:28,960 --> 00:29:32,544
It helped it build a model of what's the correct decision in any situation

665
00:29:33,056 --> 00:29:34,080
Electronic versions

666
00:29:34,592 --> 00:29:37,152
Yeah exactly what they're incredibly OPEC

667
00:29:37,408 --> 00:29:40,480
And difficult for people to understand even their developers or not

668
00:29:40,736 --> 00:29:41,248
Quite sure

669
00:29:42,016 --> 00:29:44,064
How do you say systems are making their decisions

670
00:29:44,832 --> 00:29:47,648
They can be incredibly accurate they're fed enough data

671
00:29:48,160 --> 00:29:51,488
And the data fits the world in the right way

672
00:29:51,744 --> 00:29:53,024
They can be very effective

673
00:29:53,280 --> 00:29:55,072
And now we're incredibly effect

674
00:29:55,328 --> 00:29:56,096
Tantalizing

675
00:29:56,352 --> 00:29:56,864
Faces

676
00:29:57,376 --> 00:29:58,144
Objects in image

677
00:29:59,168 --> 00:29:59,680
Voice record

678
00:29:59,936 --> 00:30:00,448
Cognition

679
00:30:01,216 --> 00:30:02,752
You know and computer games

680
00:30:03,008 --> 00:30:04,288
But they're not very explainable

681
00:30:04,544 --> 00:30:07,104
It's not very easy for user to understand

682
00:30:07,360 --> 00:30:08,640
What these systems are doing

683
00:30:08,896 --> 00:30:10,944
So for a lot of applications that's okay

684
00:30:11,200 --> 00:30:13,504
If you're finding cat videos on Facebook

685
00:30:13,760 --> 00:30:14,784
An explanation is not

686
00:30:15,040 --> 00:30:15,552
A critical

687
00:30:16,064 --> 00:30:18,880
But if you are an analyst for DOD that's picking out

688
00:30:19,392 --> 00:30:20,416
Target's to investigate

689
00:30:21,440 --> 00:30:23,488
Are you running a autonomous

690
00:30:24,000 --> 00:30:30,144
Unmanned air vehicle that's going to go off onto a mission on its own you really want to get a good explanation

691
00:30:30,400 --> 00:30:31,424
What this thing is thing

692
00:30:32,192 --> 00:30:33,728
So it's a very hard problem

693
00:30:33,984 --> 00:30:37,312
To some extent this machine learning technology work so well

694
00:30:37,824 --> 00:30:39,616
Because it learns models

695
00:30:40,128 --> 00:30:43,456
That are inherently more difficult more complex in what person

696
00:30:43,712 --> 00:30:44,224
Quit special

697
00:30:44,736 --> 00:30:47,040
But yet you need to get some explanation out of it

698
00:30:47,552 --> 00:30:49,344
So that user can get an intuition

699
00:30:49,600 --> 00:30:50,112
What the cyst

700
00:30:51,136 --> 00:30:51,904
So that's the problem

701
00:30:52,160 --> 00:30:58,304
So we laid that out as is typical then the DARPA you talk to a lot of researchers a lot of people in

702
00:30:58,560 --> 00:31:00,352
DOD about the problem

703
00:31:00,608 --> 00:31:01,888
You craft your

704
00:31:02,400 --> 00:31:08,544
Solicitation you get in your proposals and you pick your performers and as you mentioned I've picked 11 performers

705
00:31:08,800 --> 00:31:12,128
Or each developing what I'm calling an explainable Learning System

706
00:31:12,896 --> 00:31:14,688
So we still wanted to be a machine-learning sis

707
00:31:14,944 --> 00:31:19,552
We don't want to go back to the old version of Technology where you're trying to specify the rules

708
00:31:20,064 --> 00:31:22,624
All the windows older versions they were more explainable

709
00:31:23,136 --> 00:31:25,184
Because a human head describe the logic

710
00:31:25,440 --> 00:31:25,952
It's using

711
00:31:26,208 --> 00:31:28,768
It was much easier to regurgitate that logic

712
00:31:29,024 --> 00:31:29,792
Take to another human

713
00:31:30,048 --> 00:31:31,584
And people could understand what is

714
00:31:32,608 --> 00:31:34,144
But in this case that's not possible

715
00:31:34,400 --> 00:31:38,240
So each one of these 11 teams is developing two major components

716
00:31:38,752 --> 00:31:40,288
They're going to plug together into Asus

717
00:31:41,312 --> 00:31:44,384
One component is a modified machine learning process

718
00:31:45,408 --> 00:31:47,456
So they're changing the Deep learning

719
00:31:47,712 --> 00:31:48,480
A process

720
00:31:48,736 --> 00:31:50,528
Were there adding a new learning Tech

721
00:31:51,040 --> 00:31:54,112
Color changing something about the way the system learns

722
00:31:54,368 --> 00:31:54,880
It's model

723
00:31:55,904 --> 00:31:56,672
So there's more

724
00:31:56,928 --> 00:31:57,440
Interpretable

725
00:31:57,696 --> 00:31:58,464
More explainable

726
00:31:58,720 --> 00:31:59,744
In the second piece

727
00:32:00,000 --> 00:32:01,024
Going back to my

728
00:32:01,280 --> 00:32:04,096
No background in HCI in psychology

729
00:32:04,608 --> 00:32:06,912
Is the have an explanation interface

730
00:32:07,680 --> 00:32:12,800
Which is the best combination of visualization natural language generation

731
00:32:13,312 --> 00:32:19,456
You know creates just the right explanation that's appropriate for an end-user so he understands what the sea

732
00:32:19,712 --> 00:32:20,480
System is doing

733
00:32:20,736 --> 00:32:23,552
Even though that end user is not going to be a machine-learning

734
00:32:24,064 --> 00:32:27,392
So he has to take the complexity of this machine Learning System

735
00:32:27,648 --> 00:32:29,696
Create the right kind of explanation

736
00:32:29,952 --> 00:32:31,744
So your average user can understand what is

737
00:32:32,512 --> 00:32:33,024
The question about

738
00:32:34,048 --> 00:32:34,560
This is really

739
00:32:34,816 --> 00:32:35,328
The Human Side

740
00:32:36,096 --> 00:32:36,864
I might do something

741
00:32:37,120 --> 00:32:37,888
Behave in a certain way

742
00:32:38,656 --> 00:32:39,424
And somebody might say

743
00:32:39,936 --> 00:32:41,728
Why did you do that what's the explanation for your behavior

744
00:32:42,240 --> 00:32:48,384
Yes and I can come up with one after the fact that sounds reasonable to me yes but I can't read

745
00:32:48,640 --> 00:32:49,920
Can we be sure that I'm not

746
00:32:50,176 --> 00:32:50,688
Fooling myself

747
00:32:50,944 --> 00:32:52,992
Listen to just have just made up an explanation

748
00:32:53,504 --> 00:32:54,016
So

749
00:32:54,272 --> 00:32:55,552
Can't that be happening here

750
00:32:55,808 --> 00:33:00,928
It's it can't it's called rationalizations actualization one of the techniques were using we we could

751
00:33:01,184 --> 00:33:05,792
Some of the teams are taking approach where they're going to treat the machine Learning System is a black box

752
00:33:06,304 --> 00:33:07,840
They don't know what's going on

753
00:33:08,096 --> 00:33:12,960
But they can experiment with it they can run a million simulation trials with different inputs

754
00:33:13,472 --> 00:33:14,240
And see what are the out

755
00:33:15,008 --> 00:33:17,824
And see if they can infer model that explains what is do

756
00:33:18,336 --> 00:33:21,152
So the test though we do to avoid the problem yours

757
00:33:21,408 --> 00:33:22,944
Describing is the test is

758
00:33:23,200 --> 00:33:25,248
Does your explanation predict

759
00:33:25,504 --> 00:33:27,040
What the machine will do in a new city

760
00:33:27,552 --> 00:33:28,832
So if you're fooling yourself

761
00:33:29,088 --> 00:33:30,368
The prediction will be wrong

762
00:33:31,136 --> 00:33:33,440
And especially to help user

763
00:33:33,696 --> 00:33:34,464
Gametrust

764
00:33:34,976 --> 00:33:37,280
You know so the user needs to have a model

765
00:33:37,536 --> 00:33:38,816
From the explanation

766
00:33:39,072 --> 00:33:40,608
So the user knows when

767
00:33:40,864 --> 00:33:43,424
In which situation should they trust it in when shouldn't they

768
00:33:43,936 --> 00:33:45,216
So they need to have

769
00:33:45,728 --> 00:33:46,752
Some model that

770
00:33:47,264 --> 00:33:47,776
So you know

771
00:33:48,288 --> 00:33:50,592
Predictive of what the system is going to do

772
00:33:51,104 --> 00:33:51,616
So they can

773
00:33:51,872 --> 00:33:53,664
Create this understanding in their own head

774
00:33:53,920 --> 00:33:56,224
Even though that understanding may be very different

775
00:33:56,992 --> 00:33:59,296
Then what's actually going on inside the machine

776
00:33:59,808 --> 00:34:00,832
It's close enough

777
00:34:01,088 --> 00:34:02,880
It's capturing the main features

778
00:34:03,392 --> 00:34:04,672
What the system is doing

779
00:34:05,184 --> 00:34:06,976
So that they can understand

780
00:34:07,232 --> 00:34:08,512
And have some faith

781
00:34:08,768 --> 00:34:10,048
And understanding what the system

782
00:34:11,072 --> 00:34:12,096
Another fascinating

783
00:34:12,352 --> 00:34:12,864
Think of Me about

784
00:34:13,120 --> 00:34:14,144
Explainable AI

785
00:34:14,400 --> 00:34:15,168
And certainly hasn't learned

786
00:34:17,472 --> 00:34:21,824
Is it also seems to be rubbing shoulders with some of the great questions of philosophy

787
00:34:23,104 --> 00:34:23,872
Dysentery

788
00:34:24,128 --> 00:34:27,968
Epistemology write a big word which means had had how do we come to know that

789
00:34:28,224 --> 00:34:28,736
Things as human

790
00:34:29,504 --> 00:34:34,880
What is an explanation anyway what's the nature of explanation it makes me think that philosophers would be quite interested

791
00:34:35,904 --> 00:34:37,184
Unfortunately they are

792
00:34:37,440 --> 00:34:40,256
And I'll explain that

793
00:34:40,768 --> 00:34:41,280
And we have

794
00:34:41,536 --> 00:34:43,840
11 teams are actually building the system

795
00:34:44,096 --> 00:34:45,376
We have one team

796
00:34:45,632 --> 00:34:48,192
It's just a group of cognitive psychologists

797
00:34:48,448 --> 00:34:51,520
Who are digging into what's known about explanation

798
00:34:52,032 --> 00:34:54,592
Not so much from philosophy but from psychology

799
00:34:55,104 --> 00:35:00,224
Psychologist for years have studied what explanation is more effective for person

800
00:35:00,480 --> 00:35:01,760
What explanation leads

801
00:35:02,016 --> 00:35:02,528
Learning

802
00:35:03,040 --> 00:35:03,808
Better than another

803
00:35:04,576 --> 00:35:05,344
What explanation

804
00:35:05,856 --> 00:35:06,880
Improve decision-making

805
00:35:07,392 --> 00:35:09,952
So their job is to just a dig through all that literature

806
00:35:10,208 --> 00:35:11,232
Here and help us out

807
00:35:11,488 --> 00:35:12,000
With Watson

808
00:35:12,768 --> 00:35:16,608
And I mentioned about the business about philosophers Interest being unfortunate

809
00:35:16,864 --> 00:35:22,752
Because it's so easy to get lost in this very deep conversation about what is causality

810
00:35:23,264 --> 00:35:23,776
What's the next

811
00:35:24,032 --> 00:35:24,544
Coronation

812
00:35:25,056 --> 00:35:27,616
And we've had a fair amount of that discussion in the

813
00:35:28,128 --> 00:35:31,712
I'm trying to get around that by using a very practical definition

814
00:35:32,224 --> 00:35:33,504
The test will be

815
00:35:34,016 --> 00:35:38,880
If you create one of these systems a human user has to use that system to make decision

816
00:35:39,136 --> 00:35:39,904
Used to manage

817
00:35:40,416 --> 00:35:41,440
An unmanned vehicle

818
00:35:41,696 --> 00:35:42,208
Or two

819
00:35:42,464 --> 00:35:43,488
Do until analysis

820
00:35:44,512 --> 00:35:48,608
Does the explanation enable him to appropriately trust the system

821
00:35:48,864 --> 00:35:53,216
Does the explanation enable him to predict when the system will be right in winter

822
00:35:53,472 --> 00:35:55,008
Will be wrong and that's our measure

823
00:35:55,520 --> 00:35:57,312
So I want to try to avoid

824
00:35:57,824 --> 00:35:59,104
This philosophical

825
00:36:00,640 --> 00:36:01,152
Swamped

826
00:36:01,664 --> 00:36:03,968
And just get to a practical solution

827
00:36:04,224 --> 00:36:07,552
When does an explanation help a user do his job

828
00:36:07,808 --> 00:36:08,320
And one dozen

829
00:36:08,832 --> 00:36:09,856
So did you mean you want

830
00:36:10,112 --> 00:36:12,160
Tools in hand in a relatively short time

831
00:36:12,416 --> 00:36:13,184
As opposed to

832
00:36:13,440 --> 00:36:13,952
A Thousand Years

833
00:36:14,208 --> 00:36:15,744
Exactly right

834
00:36:16,000 --> 00:36:18,048
1000 years of discussion with no tools

835
00:36:18,304 --> 00:36:18,816
Sentient

836
00:36:20,352 --> 00:36:21,888
So we're getting toward the end we don't have that much

837
00:36:22,400 --> 00:36:22,912
But a few other things

838
00:36:24,192 --> 00:36:27,008
Discuss with you one of them is you're looking at

839
00:36:27,264 --> 00:36:28,288
The way artist

840
00:36:28,544 --> 00:36:30,080
Official intelligence is

841
00:36:30,848 --> 00:36:31,616
You received by the

842
00:36:32,128 --> 00:36:33,152
Discussions

843
00:36:33,408 --> 00:36:33,920
Abacus having

844
00:36:34,176 --> 00:36:38,272
There is certainly some excitement I think the public is leaning toward a positive you

845
00:36:38,528 --> 00:36:39,040
Beyonce

846
00:36:39,296 --> 00:36:40,832
Fleets of driverless cars attack

847
00:36:41,088 --> 00:36:42,112
Actually I'm working out there there's

848
00:36:42,624 --> 00:36:45,696
Sorry about that but they're also result that dystopian side

849
00:36:46,208 --> 00:36:48,000
Artificial intelligence is it

850
00:36:48,256 --> 00:36:49,280
It's another kind of intelligence

851
00:36:49,536 --> 00:36:50,304
Scary

852
00:36:51,072 --> 00:36:51,584
Powerful

853
00:36:51,840 --> 00:36:53,888
And that this is something to really

854
00:36:54,144 --> 00:36:55,168
Fear and worry about

855
00:36:55,424 --> 00:36:59,008
So you're in the midst of you are one of the

856
00:36:59,264 --> 00:37:00,800
The tool builders in the AI space

857
00:37:01,312 --> 00:37:03,616
How do you think about that

858
00:37:03,872 --> 00:37:05,152
You're the promise and the potential

859
00:37:05,920 --> 00:37:06,688
What's a great question

860
00:37:07,200 --> 00:37:07,712
And one bed

861
00:37:07,968 --> 00:37:09,504
We talked about a lot

862
00:37:10,528 --> 00:37:14,624
I'm so first day I like any technology can be used for good or evil

863
00:37:14,880 --> 00:37:16,672
Right so there is both sides to a

864
00:37:17,696 --> 00:37:18,720
And we have to be

865
00:37:18,976 --> 00:37:19,488
Be careful

866
00:37:20,000 --> 00:37:20,512
Like to manage that

867
00:37:20,768 --> 00:37:23,072
Hive overall think it's very positive

868
00:37:23,328 --> 00:37:24,096
Right I think it's

869
00:37:24,352 --> 00:37:26,144
Be much easier for us to avoid

870
00:37:26,400 --> 00:37:27,680
Literally negative aspects

871
00:37:28,704 --> 00:37:29,472
This this is where

872
00:37:29,728 --> 00:37:31,264
Some of my press you and say well

873
00:37:31,520 --> 00:37:32,032
But you're teknaf

874
00:37:32,800 --> 00:37:33,568
Write an essay

875
00:37:33,824 --> 00:37:34,336
And so

876
00:37:34,848 --> 00:37:36,128
And you said that in a fairly Cavill

877
00:37:36,896 --> 00:37:39,200
Forgive me one or two bases for why you have it up

878
00:37:39,712 --> 00:37:40,736
What is

879
00:37:40,992 --> 00:37:42,784
The level of intelligence

880
00:37:43,040 --> 00:37:44,064
We're really Adonai

881
00:37:44,576 --> 00:37:47,648
So what we have even though there's been huge advances

882
00:37:48,160 --> 00:37:49,440
Especially in this deep learning Tech

883
00:37:50,208 --> 00:37:50,720
In the last couple

884
00:37:51,488 --> 00:37:54,304
We're so far away from Human intelligence

885
00:37:54,816 --> 00:37:56,096
Anything that would be

886
00:37:56,864 --> 00:37:57,632
An independent

887
00:37:58,144 --> 00:37:58,656
Superman

888
00:37:59,424 --> 00:37:59,936
Stats

889
00:38:00,192 --> 00:38:00,704
Sad signs

890
00:38:01,472 --> 00:38:05,056
To me it's like worrying about a meteor that's going to hit Earth

891
00:38:05,568 --> 00:38:07,616
Sometime in the next million years

892
00:38:07,872 --> 00:38:09,152
And in Life as We Know It

893
00:38:09,664 --> 00:38:10,944
Right is not imminent

894
00:38:11,200 --> 00:38:12,992
Price is not high probability

895
00:38:13,248 --> 00:38:15,808
In which NASA should make sure they've got telescopes

896
00:38:16,320 --> 00:38:19,648
You know around the planet so we can see that when it's happening

897
00:38:20,416 --> 00:38:22,720
So we need a small number of people worrying about

898
00:38:23,232 --> 00:38:25,536
Let's make sure we don't create a superintelligence

899
00:38:27,328 --> 00:38:27,840
But I think the

900
00:38:28,096 --> 00:38:30,656
The current technology today is so far from that

901
00:38:31,424 --> 00:38:32,192
That's not a big canoe

902
00:38:32,960 --> 00:38:35,008
Dystopian and point of something as you said

903
00:38:35,264 --> 00:38:35,776
But it's not

904
00:38:36,032 --> 00:38:37,312
Worth worrying about too much

905
00:38:38,592 --> 00:38:42,176
And it's on the other side then to move forward suggest to me that you think that

906
00:38:42,688 --> 00:38:43,456
Positive Ana

907
00:38:43,712 --> 00:38:44,480
Points are

908
00:38:44,736 --> 00:38:46,016
Really worth pursuing osher

909
00:38:46,272 --> 00:38:47,808
What is just likes a driverless cars

910
00:38:48,064 --> 00:38:48,576
Write that

911
00:38:49,088 --> 00:38:55,232
There's a lot of issues to be worked out there but I've no doubt when that comes they'll be fewer deaths on a highway

912
00:38:55,744 --> 00:38:57,280
Then it will be with human driver

913
00:38:57,792 --> 00:39:01,632
Medicine you know just huge advancements in applying this to Medicine

914
00:39:02,400 --> 00:39:03,680
I think one of the latest results

915
00:39:03,936 --> 00:39:07,264
It was one of these deep Learning Systems can learn to recognize

916
00:39:08,032 --> 00:39:09,312
Skin cancer

917
00:39:09,824 --> 00:39:12,128
Events better than human.

918
00:39:13,920 --> 00:39:14,432
You know so

919
00:39:14,688 --> 00:39:17,760
Just analyzing the vast amounts of data that we have

920
00:39:18,016 --> 00:39:18,528
Hand

921
00:39:18,784 --> 00:39:23,904
You know being able to understand it or just creating Siri in personal assistance it can do a better job

922
00:39:24,160 --> 00:39:26,208
There's just so many things were AI is

923
00:39:26,464 --> 00:39:26,976
Going to be

924
00:39:28,000 --> 00:39:29,024
Right this is making me think

925
00:39:29,280 --> 00:39:30,304
Of your new grandchild

926
00:39:30,816 --> 00:39:31,584
He or she

927
00:39:31,840 --> 00:39:33,120
Chi heat so she is

928
00:39:33,376 --> 00:39:36,192
I'm going to grow up in a time when AI is

929
00:39:36,448 --> 00:39:36,960
Heart

930
00:39:37,216 --> 00:39:37,728
Of the landscape

931
00:39:37,984 --> 00:39:39,776
A part of the tecnis right but he will not

932
00:39:40,288 --> 00:39:42,080
No right time without a high

933
00:39:42,592 --> 00:39:43,104
It's kind of interesting

934
00:39:43,360 --> 00:39:46,432
Have a great little video of him at 1 year old

935
00:39:47,200 --> 00:39:48,992
Is first interaction with Siri

936
00:39:49,760 --> 00:39:52,064
And he's learned if you press this button

937
00:39:52,832 --> 00:39:54,112
The iPhone talks to him

938
00:39:54,368 --> 00:39:57,696
And he doesn't even have language that we understand but he would press this

939
00:39:57,952 --> 00:39:58,464
Button

940
00:39:58,976 --> 00:40:01,024
And have a conversation in his own

941
00:40:01,536 --> 00:40:02,304
Garbled language

942
00:40:02,560 --> 00:40:03,328
You're just fascinating

943
00:40:03,584 --> 00:40:06,144
So I think will be interesting to see how this generation

944
00:40:06,656 --> 00:40:07,168
Pro Shop

945
00:40:07,680 --> 00:40:12,288
I want to see that video at some point of time and I just want to thank you

946
00:40:12,544 --> 00:40:13,056
At 4

947
00:40:13,312 --> 00:40:14,592
Engaging me and really would have

948
00:40:15,104 --> 00:40:17,152
Absolutely fascinating started my morning here

949
00:40:17,664 --> 00:40:19,200
Lots of things for me to think about for the rest

950
00:40:19,456 --> 00:40:22,528
Rest of the day so thanks a lot for 12 thank you certainly enjoyed it

951
00:40:22,784 --> 00:40:24,832
And thanks listeners for sharing this time with us

952
00:40:25,344 --> 00:40:27,136
I hope you join us again for the next

953
00:40:27,392 --> 00:40:28,160
Voices from DARPA

954
00:40:31,488 --> 00:40:33,280
For more information about David gunning

955
00:40:33,792 --> 00:40:34,560
The program.

956
00:40:35,072 --> 00:40:36,608
Any other breakthrough technology

957
00:40:36,864 --> 00:40:37,888
DARPA is working on

958
00:40:38,144 --> 00:40:38,912
Visit DARPA.

959
00:40:43,520 --> 00:40:45,568
And four links that enable you to download this

960
00:40:46,592 --> 00:40:48,128
Go to the voices from DARPA

961
00:40:48,896 --> 00:40:49,408
Grand Rapids weather
