1
00:00:11,570 --> 00:00:21,790
Stuff technology , the internet , GPS in the column of your hand are tonight with technology is a driver of our times . 

2
00:00:22,150 --> 00:00:30,350
Since its founding a nineteen fifty eight , in the midst of the cold war , doubt the defense advanced research projects has been a driver of technology . 

3
00:00:30,800 --> 00:00:39,220
Welcome to Voices from DARPA , a window onto dark as core of program managers , their job to redefine what is possible . 

4
00:00:39,470 --> 00:00:45,413
My name is iphone a motto , and on your dark bowl host until I am pleased to have with me in the studio . 

5
00:00:45,413 --> 00:01:20,589
Miss your david gunning , who has been admitted his third tour of duty as a program manager , durban , in this case , with the information innovation office during his previous tour in the early years of the new millennium , david managed the personalized system that learns program paul that lent to the now familiar voice of syria during his first tour and nineteen nineties , he managed to variety of AIprojects artificial intelligence , including the command post of the future , which was later adopted by the USarmy as the command control system for using a regular afghanistan between his institute . 

6
00:01:20,589 --> 00:01:28,230
But mister gonna play the variety professional roles , among them , program director for data and lyrics and contextual intelligence at the pale . 

7
00:01:28,230 --> 00:01:44,540
Although research center park , a number of funding and managerial roles for test , and as a senior scientist in the air force research lab at white petterson air force base , the synthetic threat threatens career , have been artificial intelligence and human computers in bias . 

8
00:01:44,720 --> 00:01:47,164
It's a threat that goes back to his education . 

9
00:01:47,164 --> 00:01:56,690
They would hold a pair of master's degrees , one in computer size from stanford university and another and experimental psychology from the university of david gonna . 

10
00:01:56,690 --> 00:01:58,160
Thanks for joining me in the studio today . 

11
00:01:58,230 --> 00:01:59,580
Yes , glad to do so . 

12
00:02:00,000 --> 00:02:01,031
You're a double . 

13
00:02:01,031 --> 00:02:05,157
You work at the intersection of different mine sets , skill sets and tall sets . 

14
00:02:05,157 --> 00:02:12,530
In your case , intersection seems to focus on humankind , computer kind in the ways these two creatures can work together . 

15
00:02:12,840 --> 00:02:16,310
Where do those interests come from and go back as foreign , we need to learn. 

16
00:02:16,540 --> 00:02:23,740
Well , I can go back to the original event that started up up , spotted in nineteen fifty eight . 

17
00:02:24,380 --> 00:02:29,900
I was eight years old living at my grandfather's house out in the country . 

18
00:02:30,480 --> 00:02:40,880
And I can remember him taking me out on the yard night to see the light of the spotlight gets its flying overhead in a kind of conveying what I shocked this world . 

19
00:02:40,880 --> 00:02:58,060
But where were you to complete the setting and coming down and talk outside governing kentucky , and so that at the time , also , not only sparked dark , but sparked a huge interest in technology , she might as a huge interest in encouraging young minds to be interested in engineering and science . 

20
00:02:58,060 --> 00:03:00,300
So I can remember that is kind of the sport dark . 

21
00:03:00,690 --> 00:03:02,310
It started later on . 

22
00:03:02,310 --> 00:03:12,820
I was an album college in ohio and late sixties , early seventies , you know , during this kind of rapid change , and a lot of cultural change going on . 

23
00:03:12,990 --> 00:03:15,389
I became very interested in psychology . 

24
00:03:15,389 --> 00:03:18,007
So there was my first technical entries . 

25
00:03:18,007 --> 00:03:23,680
Just , how do people work , right , people , clearly , the issue in most of the problems we have the day . 

26
00:03:23,680 --> 00:03:24,680
How does that mind work ? 

27
00:03:24,680 --> 00:03:26,281
And what could we do about ? 

28
00:03:26,281 --> 00:03:40,580
It was was the starting point , and that I did that would get master's degree in psychology , started to work at the right patterns and air force base as a human factors engineer , which was one of the practical uses of that psychology degree . 

29
00:03:40,580 --> 00:03:52,855
I am , and then very quickly , the first jobs I had worked to look at the human factors of how they were introducing electronic displays into the cockpit . 

30
00:03:52,855 --> 00:04:07,249
This , in nineteen ninety seven days , can one of the first workplaces to be automated if you want pilots were good in the most advanced technology , they are putting electronic displays in the place of instruments . 

31
00:04:07,249 --> 00:04:27,550
Pilots were making submitted microsecond decision , life or decisions you want to make sure they get just the right information and just the right way , you know , to make those decisions , and so that trigger might interest in computer science because quickly became parents that all the important aspects job were in computer software . 

32
00:04:27,550 --> 00:04:50,110
And so that prompted my interest in computer science , then went back to school to stand for to get my masters degree in computer science , and especially in AI, that was one of the aisles , summers , if you will , there is a hole in the nineteen eighties , a lot of interest in expert systems in one of the first waves of a and so that captured my interests . 

33
00:04:50,110 --> 00:04:54,150
And certainly was a lined with my background in cognitive psychology. 

34
00:04:54,650 --> 00:05:10,229
David , as I noted in my introduction , you know , in your third tour as a program management dirt butt , and just the latter will talk about the programs you inherited in this latest tour , communicating with computers and the program that you initiated explaining AI. 

35
00:05:10,229 --> 00:05:17,334
But before we get to that , i'd like for you to take us on a brief of your prior two times as a program management . 

36
00:05:17,334 --> 00:05:22,558
So let's start with how you and double met for the very first time as APM. 

37
00:05:22,558 --> 00:05:27,545
Iguess this was the ninety ninety four in the information systems of us . 

38
00:05:27,545 --> 00:05:28,020
What is? 

39
00:05:28,050 --> 00:05:41,930
As they started off as a psychologist and switched the computer science in both those roles , I was a research idea for his research lives , right , patterson , so doing a lot of work at the time on the afternoon too . 

40
00:05:41,930 --> 00:05:44,800
That was the latest fighter aircraft of the day . 

41
00:05:45,260 --> 00:05:56,448
Now , those computer displays that , you know , and nineteen seventies were very advanced , and particularly then I was working on a integrated information mainland system . 

42
00:05:56,448 --> 00:06:05,715
So now it was possible for the maintenance technician woke up to a computer , woke up to the aircraft with a portable computer plugged in . 

43
00:06:05,715 --> 00:06:09,373
Download information about what files had occurred . 

44
00:06:09,373 --> 00:06:17,770
Did all of his tech manual instructions present on the computer , great kind of integrated in a workplace for the maintenance technician. 

45
00:06:17,800 --> 00:06:27,530
Do you help him do it , just a question about that , because , you know , you know , when you go into a any garage tent of your own car looked at regard , there is a system very much like that . 

46
00:06:27,530 --> 00:06:28,630
Yeah , it is . 

47
00:06:28,630 --> 00:06:29,760
But this proceeded. 

48
00:06:29,840 --> 00:06:39,800
Proceeded that it was actually the very first prototype of electronic maintenance instructions anywhere the service did that the that time on my program . 

49
00:06:39,980 --> 00:06:46,170
And so I was doing at work that gets the attention of the general who was running the afternoon two program . 

50
00:06:46,460 --> 00:06:48,610
He pulled me into the afternoon , too . 

51
00:06:48,610 --> 00:06:53,343
The kind of managing coordinate model model of his digital technology . 

52
00:06:53,343 --> 00:06:56,882
And what I was there , some people from doctor came to visit . 

53
00:06:56,882 --> 00:07:03,410
They tried to convince the skeptical of the day he should listen to double and adopt some of their new technology . 

54
00:07:04,050 --> 00:07:07,980
So he expressed how skeptical he wasn't photographic way . 

55
00:07:07,980 --> 00:07:29,682
Instead , if you're not doing something , going to have twenty two , you need to convince dave going on his , my computers that was with the steve cross , who was deputed record of one of the officers geo video hold , who was a program manager here on long from stanford university , who is doing a project and intelligent integration of the world heat systems using AI. 

56
00:07:29,682 --> 00:07:42,030
And so I began to interact with those guys in when jail left , died after his tour was done , then invited me to come to the door , but to be APMand nineteen ninety four , I just , I finally had. 

57
00:07:42,030 --> 00:07:44,187
这个是我们这边的一个产品。

58
00:07:44,187 --> 00:07:54,790
然后第二部分就是说呃有些客户可能需要去做什么东西，或者其他方面还有没有什么问题的话，可以bringing a new technology ，which does not have as much of a track record . 

59
00:07:54,790 --> 00:07:58,200
But then you were there , as someone really a champion of the. 

60
00:07:58,440 --> 00:07:59,930
Yes , can I be in the new year ? 

61
00:08:00,000 --> 00:08:04,087
Very conservative environment is not that their technology adverse . 

62
00:08:04,087 --> 00:08:05,807
These people are very serious . 

63
00:08:05,807 --> 00:08:12,834
Engineers are building the aircraft with the normal life and deaths , and , you know , consequences that we started incorrectly . 

64
00:08:12,834 --> 00:08:23,536
So they don't want to put anything in that airplane right unless they really know that they can depend on it , but they may really solid understanding in background where those people are coming from . 

65
00:08:23,536 --> 00:08:28,070
When you're try to introduce new technology , you know what you need to do to convince them. 

66
00:08:28,120 --> 00:08:33,429
One thing that I noticed regarding your this first tour , beauty is , and naturally , did you come here ? 

67
00:08:33,429 --> 00:08:34,859
But you went full mind and body . 

68
00:08:34,859 --> 00:08:37,310
And if i'm reading your background correctly . 

69
00:08:37,510 --> 00:08:43,090
Overseas , nine programs released in your period of six years , and at first tour. 

70
00:08:43,170 --> 00:08:45,400
There were nine programs , and you win , yeah , that's right . 

71
00:08:45,400 --> 00:08:52,740
I was starting , wanted to new programs a year and heard at a few , you know , for mother PMS, but I was all in . 

72
00:08:53,090 --> 00:08:58,240
I was like younger than had a lot more energy , and but just love the job you mention . 

73
00:08:58,240 --> 00:09:02,930
I've been here three times , just to me , is one of the most fascinating job you can have . 

74
00:09:02,930 --> 00:09:06,340
You are the panic goal of new technology development . 

75
00:09:06,750 --> 00:09:16,030
You have such a broad view was going on , and dark gives you this incredible freedom to pursue what you see is advantage on where they should go. 

76
00:09:16,120 --> 00:09:24,040
Now , one of the standards from that first tour is not a command post of the future , and that one actually did get out into the field . 

77
00:09:24,040 --> 00:09:31,450
So just briefly , could tell us a little bit about what gold of that program wasn't , in fact , how I was so it unfolded in the field. 

78
00:09:31,500 --> 00:09:36,384
Of course , my interests spent AIall along in most of those nine programs . 

79
00:09:36,384 --> 00:09:44,410
I ran my first tour had to do with knowledge in reasoning , planning , trying to provide intelligent decision ages to people . 

80
00:09:44,520 --> 00:10:00,340
So we created command posts of the future , you know , as a way to save weekend , explore all sorts of vi technologies , see what work packaged that up into a system that army commanders could use out in the future film . 

81
00:10:00,340 --> 00:10:06,500
So we started that with a wide variety of technologies of varying degrees of maturity and risk . 

82
00:10:06,500 --> 00:10:08,980
And by time we get to the end of the program . 

83
00:10:09,540 --> 00:10:20,655
And I had left up by then it was other people were taken it over , but the technology that survive was something that was my call adapted for intelligent visualization . 

84
00:10:20,655 --> 00:10:37,895
A lot of the , the more higher risk I techniques that actually kind of following out this two imagery , right , but they had through a lot of virtue with some visionary people from the army , marine , corn , air force and really kind of picked out the knowledge of the new technology . 

85
00:10:37,895 --> 00:10:48,100
And they were incredibly valuable , and basically , was that if a commander in his staff could each have their own view , the battlefield in their command post the future screen . 

86
00:10:48,560 --> 00:10:55,910
So the operators saw the plans for the attack , the logistics guide , and saw , what did he have to do to support that plan , right ? 

87
00:10:55,910 --> 00:11:01,090
The entire guy just got to see what the information was coming in on what the I mean , we do next . 

88
00:11:01,090 --> 00:11:07,760
And one of the commanders are army , general korea , who is the one who championed it and took it to back then . 

89
00:11:08,410 --> 00:11:17,890
Ashley said that by watching his staff campaign pushed the future screens , he had a better idea of what they were thinking . 

90
00:11:18,190 --> 00:11:27,000
Then he did , if they were all in the same room , so it enabled them to one be distributed , which is very important for the battlefield . 

91
00:11:27,000 --> 00:11:33,290
The two gave him better insight into what his staff was and wear the battle , the battle. 

92
00:11:33,390 --> 00:11:43,690
Was emerging so fascinating this , that is the case where the human machine interaction into something more than the components. 

93
00:11:43,890 --> 00:11:54,770
Yes , and partly critical part of that programs where we set it up was the health is consistent interaction between the technologies and the visionary use . 

94
00:11:55,050 --> 00:12:03,720
Right , which is it now called edgy development as a lot of names , is pretty much been adopted by silicon valley as religion . 

95
00:12:04,050 --> 00:12:12,880
But that strategy was pretty early in those , and they had a lot of exercises where they take researchers from the university . 

96
00:12:13,280 --> 00:12:34,470
Send them down the ford pork in the army , initially put on the army gear and run up director sizes , so they get a real sense of what the problem was , but they also had exercises where the visionary military guys would work closely with the technologies to try to sort out what was the most valuable thing the technology could do for user in the field . 

97
00:12:34,470 --> 00:12:40,840
And not just let the technologies kind of hypothesize what that might be with somewhat idea of. 

98
00:12:40,970 --> 00:12:41,770
But watching it in . 

99
00:12:42,120 --> 00:12:51,180
So in two thousand , you leave the article and do other things , and in two thousand reasons , you come back , and this time , you need information processing technology . 

100
00:12:51,180 --> 00:12:59,930
Office tells a little bit about the programmes that you , well , they are , including one that I think many listeners will have purple. 

101
00:13:00,060 --> 00:13:21,970
Working back , as they say , a tony teller , just comment is the new director around black men as the office cheese of this newly formed officer , though , and ipad was also the name of one of the original dark officers are LED by a legislator is really where the personal computer was created with whole idea . 

102
00:13:22,060 --> 00:13:23,930
Yeah , those days , a computer met a large . 

103
00:13:23,930 --> 00:13:25,200
I be in range and just. 

104
00:13:25,280 --> 00:13:37,608
For listeners who don't know so lick lighter is a giant in the history of dark , but he is now usually as JCRAlegislator actually stands for those in joseph , carl robinet lick later . 

105
00:13:37,608 --> 00:13:50,432
And he really brought in an ethic of of this idea that the human machine interaction in the symbolism was going to be a long , long theme of technology from the end of sixties , when he was until now . 

106
00:13:50,432 --> 00:13:53,620
And so david , you'll continue in this tradition. 

107
00:13:53,670 --> 00:14:12,790
Exactly , and I did before he came in , did what a computer man was an , I be a main frame to took up eyes of a small room , and people would interact with it by sending punch cards and do a punch card reader and getting an answer back on the other side , there was no concept of this being a personal computer . 

108
00:14:12,790 --> 00:14:33,160
You sit down with , interact with on a daily basis , and was actually under his program as alan kay , who was that the stanford at the time in later , it park sketched out the first diagrams of what we recognized the day is the personal computer in parking pilot to research in the exactly right and the journey . 

109
00:14:33,160 --> 00:14:34,060
So that was it . 

110
00:14:34,060 --> 00:14:38,020
So tony teller and run block men , I really wanted to bring that back . 

111
00:14:38,020 --> 00:14:45,550
They wanted to reenergize the top to go after the next generation of human computer service . 

112
00:14:45,790 --> 00:14:50,490
And the phrase then was cognitive systems , cognitive technology . 

113
00:14:50,490 --> 00:14:53,532
And so tony believed in western biggest . 

114
00:14:53,532 --> 00:15:09,630
And so we came up with the idea for this program called pile , personalized the system to learn which , even by daughter standards , was a fairly large program , two hundred and fifty million dollars over five years , and it had multiple levels at one level . 

115
00:15:09,630 --> 00:15:21,610
It funded twenty some research , universities and agencies around the country to explore a lot of cutting edge technology in AI, in particular , machine learning . 

116
00:15:21,770 --> 00:15:28,003
Machine learning is now common place today , with this kind of emerging technology and those days . 

117
00:15:28,003 --> 00:15:32,390
So we'd funded a huge amount of basic research on machine learning. 

118
00:15:32,460 --> 00:15:34,520
And let me just save you for a second for listening . 

119
00:15:34,520 --> 00:15:36,630
You might not really know what that means . 

120
00:15:36,800 --> 00:15:39,810
We can go into a full seminar about that . 

121
00:15:39,810 --> 00:15:41,330
But what would you find that? 

122
00:15:41,400 --> 00:15:46,937
Well , in the earlier days of AI, we try to develop intelligent systems by having program . 

123
00:15:46,937 --> 00:15:54,464
Rules are statements into a system that we tell it what the knowledge wasn't later than reason with that . 

124
00:15:54,464 --> 00:15:59,033
And the expert systems of expert systems and knowledge bases . 

125
00:15:59,033 --> 00:16:04,990
And the terrible tax today is kind of a current incarnation in that time , just playing programs . 

126
00:16:04,990 --> 00:16:17,456
Exactly , those ended up being very difficult , very difficult to program , very difficult to maintain its very difficult for people that actually , exactly the logic rules that are behind intelligence . 

127
00:16:17,456 --> 00:16:40,090
But over those decades , and especially during this power program , began to realize that if you fed a computer , a lot of data entertained loose with the right machine learning algorithms , which were kind of learning the statistics and probably stick nature of the data and learning a different recognition patterns , the computer could learn its own patterns . 

128
00:16:40,390 --> 00:16:49,580
They were not specified by a person , but those patterns were much more effective than doing a lot of jobs than like understanding , speech , recognizing images . 

129
00:16:50,280 --> 00:16:53,480
You know , a funny credit card fraud . 

130
00:16:54,020 --> 00:16:59,650
And so machine learning was much more effective than the older style , explicit logic. 

131
00:17:00,000 --> 00:17:09,002
Basketball , so , in a sense of the computer was doing is taking those , those sort of analyzed signals that that we humans are comfortable with . 

132
00:17:09,002 --> 00:17:13,980
But looking them now in their data versions , right , and that's where they are comfortable. 

133
00:17:14,070 --> 00:17:25,646
Yes , exactly , until there was a big component of the program , but we also did not work on machine dialogue on , you know how you carry on an intelligent dialogue with a computer . 

134
00:17:25,646 --> 00:17:28,816
The speech , understanding to some extent was already being developed . 

135
00:17:28,816 --> 00:17:32,320
How would you have a computer that could be an intelligent assistance ? 

136
00:17:32,630 --> 00:17:39,770
They would watch what the user is doing , understand his job , then be able to give him assistance at the right time . 

137
00:17:39,870 --> 00:17:56,320
And one of the metaphors we used as you wanted to create an assistant that was like a surgery right only on the old TVshow match for those viewed enough to remember already always knew what the kernel needed before the kernel need new himself , right . 

138
00:17:56,320 --> 00:18:04,140
So we just followed the context of what the user could anticipate and provide intelligent advice . 

139
00:18:04,140 --> 00:18:09,230
So there was the grand vision , and we found it a lot of it very advanced research , and all of those areas . 

140
00:18:10,030 --> 00:18:19,380
And in one of our performers was the israel international and how the middle park , california , they were one of the integrators of the state . 

141
00:18:19,680 --> 00:18:30,290
They looked at all the research was being tried to create a prototype of this power system that would conductor be used as an intelligent assistance on your laptop. 

142
00:18:30,290 --> 00:18:36,580
So that they were looking at many of the component technologies that the other performers were were developing and see. 

143
00:18:36,610 --> 00:18:39,870
We put it together exactly , and I was really their job . 

144
00:18:39,870 --> 00:18:47,630
Was , how do we interpret the best of all these techniques and put that together , and a real prototype of a cut into a system . 

145
00:18:47,900 --> 00:18:56,670
And so they , as long as they were doing that , began to develop a kind of streamlined , more efficient version of the architecture . 

146
00:18:57,380 --> 00:19:03,060
And when the pile program , the research version , the book , our program ended around two thousand and eight . 

147
00:19:04,360 --> 00:19:27,020
They had several people who worked on power , and some new performers they brought in from the venture called , and they decided they were going to take this technology adapted for the iphone , which was a brilliant new product at the time , and people didn't know quite what to do as a ride was very wisely said , this is the place for , according to the system . 

148
00:19:27,300 --> 00:19:28,690
People gonna help this phone . 

149
00:19:28,690 --> 00:19:30,030
They won't have a keyboard . 

150
00:19:30,270 --> 00:19:32,990
They really going to need an assistance that can help them out . 

151
00:19:33,720 --> 00:19:35,850
So they had the adventurer in for two years . 

152
00:19:35,850 --> 00:19:39,760
It was just funded as a new start up , darker than without the picture . 

153
00:19:40,070 --> 00:19:43,730
They get venture money from silicon valley to develop . 

154
00:19:44,060 --> 00:19:49,709
And in two years later , they put their first syrian application on the APP store . 

155
00:19:49,709 --> 00:19:51,592
The APP store was brand new . 

156
00:19:51,592 --> 00:19:59,010
And I think , was three days later , they got a call from steve jobs himself , who said , in first , they are like what sure this is . 

157
00:19:59,010 --> 00:19:59,890
Steve jobs . 

158
00:20:00,030 --> 00:20:07,200
He just caused by the trend of phones , you know , in the office after they began to believe it really was steve jobs . 

159
00:20:07,810 --> 00:20:11,860
He asked the three founders , he said , wanted to come over to my house tonight for dinner . 

160
00:20:11,860 --> 00:20:18,630
I've got a vision for what I wanted to do , and it was some negotiation over selling the company . 

161
00:20:18,930 --> 00:20:24,720
Apple boarded , spends several years developing it under the syrian assistant we know today . 

162
00:20:25,230 --> 00:20:30,270
And for the first year of that , steve jobs was personally involved in directing it . 

163
00:20:30,270 --> 00:20:34,640
He had really seen that as a vision for he wanted to go , but then get. 

164
00:20:35,390 --> 00:20:48,255
To continue , okay , so before we move on now to your current in your third tour , which is described your result of the architectural technology development cycle at dark , wanted about successful . 

165
00:20:48,255 --> 00:20:51,941
They can get in the u and others here in your performance . 

166
00:20:51,941 --> 00:20:55,140
Those works under contract on your programs . 

167
00:20:55,140 --> 00:21:06,346
They brought the technology to a point of proving it and then taking the risk down enough that say , other corporate entities , or where the military services can take a hand off and do the final . 

168
00:21:06,346 --> 00:21:09,423
Developmentally , we actually have the product technology . 

169
00:21:09,423 --> 00:21:16,440
Some of the development goes to that point of a product technology , but it's more to that prototype with a proof of principle . 

170
00:21:16,440 --> 00:21:24,040
Here is the case where all of this worth that you did , let do something that now is part of the technology of the world . 

171
00:21:24,510 --> 00:21:27,750
So the question I have for you is just , what does that feel like ? 

172
00:21:27,750 --> 00:21:35,649
I mean , here you are , you hear right here you all over the place , and you know that you had it a role in the sequences that LED to it . 

173
00:21:35,649 --> 00:21:36,840
So I had that film. 

174
00:21:36,840 --> 00:21:44,093
We've feels great and was quite a surprise in a way , right , as you say , that starbucks job were always doing . 

175
00:21:44,093 --> 00:21:50,927
His advanced technology were always trying to , you know , and we knew a personal assistant had been . 

176
00:21:50,927 --> 00:22:03,308
That's been apart of the AI science fiction since two thousand and one and star track , and that's always like what certainty , be able to interact october computer someday . 

177
00:22:03,308 --> 00:22:07,890
But we've never quite work , but we've never heard about that. 

178
00:22:07,930 --> 00:22:09,730
But now we want , we want the party . 

179
00:22:09,730 --> 00:22:10,720
It was all right. 

180
00:22:11,210 --> 00:22:23,419
But the idea of a computer that was intelligent enough to carry on a conversation being assistant had been around for a long time , but never employment it when people tried it , it didn't work well enough . 

181
00:22:23,419 --> 00:22:25,754
It was always a little bit too close to . 

182
00:22:25,754 --> 00:22:34,121
And I think that that power program really brought it to the point the cheap jobs is willing to take a risk and say , yeah , I can make a real product out of this . 

183
00:22:34,121 --> 00:22:39,852
And like you say , left up in two thousand and eight , it's two years later that this comes out and renewed . 

184
00:22:39,852 --> 00:22:50,020
Apple had bought it , but had no idea what they're going to do , and it suddenly its apple's marketing department is advertising the product , my program , and that was very nice . 

185
00:22:50,020 --> 00:22:52,520
My , my children even thought I was a cool guy . 

186
00:22:52,520 --> 00:22:52,810
I. 

187
00:22:53,130 --> 00:22:56,680
Momentarily , the greatest achievement of all right . 

188
00:22:56,680 --> 00:22:59,910
So let's then move now to your current . 

189
00:23:00,000 --> 00:23:03,075
You came here in twenty fifteenth , you left until thursday . 

190
00:23:03,075 --> 00:23:05,959
You wanna wait , you do other things in an industry . 

191
00:23:05,959 --> 00:23:07,689
And you came back in twenty fifteen . 

192
00:23:07,689 --> 00:23:13,590
So again , tell me that that approach holders that you were convinced the third time , and it was talking about your two programs. 

193
00:23:13,690 --> 00:23:18,646
Well , one is , I said , and it just being a dark is the best job by weather hand . 

194
00:23:18,646 --> 00:23:21,714
It is interesting to go out in industry . 

195
00:23:21,714 --> 00:23:23,250
It's to be in other places . 

196
00:23:23,750 --> 00:23:29,280
But if you enjoy technology and you really want to make an impact , there's nothing like being a darker . 

197
00:23:29,950 --> 00:23:53,260
So I was in a headband out for a while in a friday jobs in industry , i'm ready to looking for something to do the other factor of those myself who lives in ruslan nearby just had a first grandchild , and so that convinced my wife it was worth moving from seattle , where our baseball , oh , back to DC, Igot a great job . 

198
00:23:53,260 --> 00:23:55,990
My wife got to play with her grandson all day . 

199
00:23:55,990 --> 00:23:56,770
So that was A. 

200
00:23:56,820 --> 00:23:58,716
That would be a pure justification , right ? 

201
00:23:58,716 --> 00:24:00,617
There are no need for anything thing else . 

202
00:24:00,617 --> 00:24:04,116
So let's talk about the two programs , one , one you can hurt it . 

203
00:24:04,116 --> 00:24:06,175
That's communicating with computers . 

204
00:24:06,175 --> 00:24:09,880
See that you see , right , just as tell me a little bit about to go. 

205
00:24:09,910 --> 00:24:26,960
Well , innocence is going to do the next generation of earlier alexander and no source of things , or now that technology is there , you know , can understand what you say for the most part , not always , but for the most part , and can provide , you know , assistance . 

206
00:24:26,960 --> 00:24:29,630
Consultant alarm can play a song , right ? 

207
00:24:29,630 --> 00:24:32,690
Keep you find directions instead of yourself . 

208
00:24:32,690 --> 00:24:37,750
We really want to raise the level of intelligence and partnership of the computer . 

209
00:24:38,190 --> 00:24:44,550
So it is really an intelligent partner that , you know , I join partner with the human is really human . 

210
00:24:44,550 --> 00:24:51,840
Computer sympathize , not human as the manager and computer as a low level of system to that score. 

211
00:24:51,890 --> 00:24:53,863
Let's stay here for just one more minute . 

212
00:24:53,863 --> 00:25:03,130
What's the kind of question that I might be able to ask after a successful seat of UCcommunicating with computer's program that I can't ask , say , siri. 

213
00:25:03,280 --> 00:25:12,280
Let me not use serious , but use one of the examples from the program is the harvard medical school , where they had done some work on a previous dark a program called big mechanism . 

214
00:25:12,480 --> 00:25:40,150
Were they trying to add credit system that could read masses of research and biology and an automatically construct models of cancer pathways instead of at the human biology , was having to read thousands of research articles , the computer would do it themselves in great suggestions of new pathways of how cancer behaves in a sale , so the biologists would have that as a suggestion . 

215
00:25:40,320 --> 00:25:46,070
So now we see that we see part of it is , can we put a collaborative user interface on top of that ? 

216
00:25:46,480 --> 00:26:00,480
So now the biologists can collaborate with a computer to hypothesize how answer works in the sale , the design experiments too , you know , so that the computer is working as a virtual site layout . 

217
00:26:00,480 --> 00:26:12,180
The system , if you wear a greatest tune in a biology layout , who's really helping to digest the literature , pull out the important and suggest experiments that should be . 

218
00:26:12,180 --> 00:26:13,250
I'm getting this knife. 

219
00:26:13,330 --> 00:26:24,704
Think like that , the value added is a farmers scientist in euros , scientists , and we each spend a month in most in , you've read one hundred papers , and I have read one hundred papers , but are found working with the computer . 

220
00:26:24,704 --> 00:26:27,350
The computer might have read ten thousand papers were really exact. 

221
00:26:27,380 --> 00:26:38,705
You know , what has enough intelligence and has the right kind of dialogue machinery so it can carry on a much more intelligent conversation with you about them today . 

222
00:26:38,705 --> 00:26:49,530
It was seriously in a concerned technology might find me , you know , a certain pathway right to find me articles about a certain protein , and a computer could do that , but it is really not gone up . 

223
00:26:49,530 --> 00:26:54,140
Sort of bad information integrated , and I am carry on kind of more given. 

224
00:26:54,220 --> 00:27:01,810
Conversation with the scientists or so now move on to the program that I suspect is taking up most of your time because of the number of mine might discreet you . 

225
00:27:01,810 --> 00:27:09,030
At least eleven performers on this , and this is called explainable artificial intelligence XAI. 

226
00:27:09,310 --> 00:27:11,970
So talk about in place for the program . 

227
00:27:11,970 --> 00:27:17,270
I guess first and then talk to me about the status and health. 

228
00:27:17,730 --> 00:27:31,000
Well , first I said , I came back in two thousand and fifteen yuan , a talk to the an office director , then caught men about the coming back to darker and insurers lessons . 

229
00:27:31,000 --> 00:27:36,320
Now my program manager comes to dark with the question is , what revision , what project do you want to do ? 

230
00:27:36,720 --> 00:27:38,120
You know that that's kind of job? 

231
00:27:38,195 --> 00:27:42,305
And you better do it fast , because you usually have yourself by date on your badge . 

232
00:27:42,305 --> 00:27:43,275
You hear for a few years. 

233
00:27:43,440 --> 00:27:59,700
And so then he actually suggested this idea of explainable AIIhad just been to a workshop on with a group of intelligence from the OD , and it was a group of machine learning and HIvisualization . 

234
00:28:00,080 --> 00:28:13,710
We're trying to convince this group of the great new technology we could provide the help in television analysis and wonderful it is there who was an analysis that one of the deal said , that's not a problem . 

235
00:28:13,900 --> 00:28:39,600
Her problem , she already has big data analytics systems that recommending , suggesting patterns or targets are in a suspect to her , but she has to put her name on the recommendation that goes up the chain , and she gets greeted , if not punished , if that recommendation is wrong , and she wanted these systems to explain the horizontal for making the decisions they make. 

236
00:28:39,840 --> 00:28:42,860
This was for her , how she was going to trust the machine. 

237
00:28:42,930 --> 00:28:45,443
Right , trust it and trust in a win . 

238
00:28:45,443 --> 00:28:46,586
When is the machine ? 

239
00:28:46,586 --> 00:28:48,414
Winners is a false alarm . 

240
00:28:48,414 --> 00:28:50,471
Winners is a positive example . 

241
00:28:50,471 --> 00:28:56,010
I and to her , having the explanation was what really needed to make that happen . 

242
00:28:56,860 --> 00:29:04,714
And so that resonated is about that time is when the new wave of AI, which is deep learning , is taking off right . 

243
00:29:04,714 --> 00:29:11,910
And you hear about this call and a new revolution in AI, that's all about the egg learning techniques . 

244
00:29:11,910 --> 00:29:25,920
Where are kind of the most advanced of this machine learning technology we talked about earlier , or takes an huge violence of data , these deep learning systems that are very large , narrow match with millions of neurons . 

245
00:29:25,920 --> 00:29:39,930
Well , there are each learning wage that helped build a model of which the correct decision in any situation , electronic versions , of course , yeah , exactly , but they're incredibly opaque and difficult for people to understand . 

246
00:29:39,960 --> 00:29:44,210
Collaborators are not quite sure how these systems were making their decisions . 

247
00:29:44,750 --> 00:29:46,390
They can be incredibly accurate . 

248
00:29:46,390 --> 00:29:51,490
Their federal data and the data fits the world right away . 

249
00:29:51,800 --> 00:30:04,347
They can be very effective in our incredible effective at analysis , faces and objects and images and voice recognition , and , you know , and computer games , but they're not very explainable . 

250
00:30:04,347 --> 00:30:08,695
It's not very easy for user to understand what these systems are doing . 

251
00:30:08,695 --> 00:30:10,869
So for a lot of applications . 

252
00:30:10,869 --> 00:30:23,460
That's okay if you find in cat videos on facebook , the nation is not critical , right , but if you are an analyst for DOD, that's picking out targets to investigate , are you running in autonomous ? 

253
00:30:24,040 --> 00:30:28,070
And then there were a goal that's going to go off on to a mission on its own . 

254
00:30:28,160 --> 00:30:32,900
You really want to get a good explanation of what the standard is thinking . 

255
00:30:32,900 --> 00:30:33,750
So it's a very hard problem . 

256
00:30:34,040 --> 00:30:44,700
The sum extend this machine learning technology works so well because it learns models that are inherently more difficult , more complex than what person would specify . 

257
00:30:44,700 --> 00:30:50,640
But yet , you need to get some explanation out of it so that user can get an integration for what the system is doing . 

258
00:30:51,170 --> 00:30:52,210
So that's the problem . 

259
00:30:52,210 --> 00:30:53,520
So we lay that out . 

260
00:30:53,520 --> 00:30:54,730
There is a typical . 

261
00:30:54,730 --> 00:31:12,380
Then a doubt you talk to a lot of researchers , a lot of people in duty about the you craft , your solicitation , and you get in your proposals , and you pick your performers and have you mentioned , i've picked eleven performers or each developing what i'm calling an explainable learning system . 

262
00:31:12,740 --> 00:31:14,970
So we still wanted to be a machine learning system . 

263
00:31:14,970 --> 00:31:25,920
We don't want to go back to the old version of technology where you're trying to specify the rules , although we know the older versions , they were more more because a human had described the logic it's using . 

264
00:31:26,270 --> 00:31:32,060
It was much easier to regard to take that logic to another human , and people could understand what is doing . 

265
00:31:32,600 --> 00:31:34,420
But in this case , that's not possible . 

266
00:31:34,420 --> 00:31:41,655
So each one , these eleven teams developing two major components , the dark and applied together into a system . 

267
00:31:41,655 --> 00:31:55,698
One component is a modified machine learning process , so they're changing the deep process , or they're adding a new learning technique that changing something about the way the system learns its model . 

268
00:31:55,698 --> 00:32:23,487
So this is more interpretation , more explainable in the second , going back to my inner background , and AGI and psychology is to have an explanation interface , which is the best combination of visualization , natural language generation , you know , creates just the right explanation that appropriate for the end users , or who understands what the system is doing , even know that the user is not going to be a machine learning expert . 

269
00:32:23,487 --> 00:32:29,640
So we have to take the complexity of this machine learning system system , increase the right kind of explanation . 

270
00:32:29,850 --> 00:32:31,870
So you're average user can understand what. 

271
00:32:31,950 --> 00:32:35,570
Doing so , question about that , and this really comes from the human side . 

272
00:32:36,100 --> 00:32:42,040
So I might do something behaved in a certain way , right , and somebody might say , why did you do that with the explanation for your behavior ? 

273
00:32:42,290 --> 00:32:46,980
Yes , and I can come up with one after the fact that sounds reasonable to me . 

274
00:32:46,980 --> 00:32:51,550
Yes , but I can't really be sure that i'm not fooling myself until just one . 

275
00:32:51,550 --> 00:32:53,514
Maybe i've just made up an explanation . 

276
00:32:53,514 --> 00:32:55,243
So cannot be happening here . 

277
00:32:55,243 --> 00:32:59,350
It , it again at gold rationalization , rationalization , one of the. 

278
00:32:59,380 --> 00:33:00,980
Make sure you , we , we could . 

279
00:33:01,250 --> 00:33:06,610
Some of the teams are taking a approach , whether going to treat the machine learning systems of black box . 

280
00:33:06,610 --> 00:33:09,630
They don't know what's going on , but they can experiment with that . 

281
00:33:09,630 --> 00:33:18,656
They can run a million simulation trials with different input and see what the output and and see if they can throw a model of the explains what is doing . 

282
00:33:18,656 --> 00:33:23,031
So the test , though we do to avoid the problem you describe it is the text . 

283
00:33:23,031 --> 00:33:27,520
Is , does your explanation predict what the machine will do in a new situation . 

284
00:33:27,520 --> 00:33:38,820
So if you fool in yourself , the prediction will be wrong , and especially to help the use gaining trust , you know , so the user needs to have a model from the explanation . 

285
00:33:39,120 --> 00:34:01,002
So the user knows when in which situation , should they trust it , and when they , so they need to have some model , which , you know , predictive of what the systems gonna do , so they can create this understanding , and even though that understanding may be very different than what's actually going on inside the machine , it is close enough . 

286
00:34:01,002 --> 00:34:10,780
This capturing the main features of what the system is doing , so that they can understand and have some faith and understanding what the system will. 

287
00:34:10,780 --> 00:34:23,193
Then another fascinating thing to me about explainable AI , and certainly , as I am learning for me right now , is it also seems to be robbing shoulders with some of the great questions of philosophy . 

288
00:34:23,193 --> 00:34:28,734
Not , it gets into a piece of technology where a big world which we had had , we come to know things as human things . 

289
00:34:28,734 --> 00:34:30,350
And what is an explanation anyway ? 

290
00:34:30,350 --> 00:34:35,520
What's the nature of explanation makes me think the philosophers would be quite interested in what you do. 

291
00:34:35,720 --> 00:34:43,833
Unfortunately , they are , and our explain that , and we have the eleven teams are actually building the system . 

292
00:34:43,833 --> 00:35:07,240
We have one team that is just a group of cognitive psychologists who were digging into what's known about the explanation , not so much from philosophy , but from psychology psychologist for years , have started what explanation is more affected for personal , what explanation leads to learning better than another explanation , what explanations improve decision making . 

293
00:35:07,240 --> 00:35:12,670
So their job is suggested to rule that literature and help us out with what's an explanation . 

294
00:35:12,780 --> 00:35:15,480
And I mentioned business about philosophers . 

295
00:35:15,480 --> 00:35:23,120
Interesting , unfortunate , because it is so easy to get lost in this very deep conversation about what is called . 

296
00:35:23,120 --> 00:35:24,560
And what's an explanation ? 

297
00:35:24,730 --> 00:35:43,920
And we've had a fair amount of that discussion in the program i'm trying to get around that by using a very practical definition , the test will be , if you create one of these systems , a human user has to use that system to make decisions to manage a non main vehicle or to doing televisions . 

298
00:35:44,440 --> 00:35:56,362
Does the explanation enabled him to appropriately trust the system , does the enabled him to predict when the system will be right , and when it will be wrong , and that's our measure . 

299
00:35:56,362 --> 00:36:04,590
So I want to try to avoid this philosophical swamp and just get to a practical solution on . 

300
00:36:04,710 --> 00:36:07,530
When does an explanation help user do his job ? 

301
00:36:07,760 --> 00:36:09,770
And when doesn't it so give you me , you want? 

302
00:36:10,000 --> 00:36:14,850
Tools in hand in a relatively short time , I suppose , to a thousand years of discussion. 

303
00:36:14,890 --> 00:36:18,860
Exactly right , a thousand years of discussion with no tools at the end. 

304
00:36:20,380 --> 00:36:31,676
So we're getting together , and we don't have that much time , but there are few other things that like to adjust the discussion you want them is in looking at the way our official intelligence is is received by the public . 

305
00:36:31,676 --> 00:36:33,973
We kind of discussions the public is having . 

306
00:36:33,973 --> 00:36:35,740
There is certainly some excitement . 

307
00:36:35,740 --> 00:36:41,923
I think the public is leaning toward a positive view on , say , fleets of driverless cars that actually are working out there . 

308
00:36:41,923 --> 00:36:55,280
There are still some ways around that , but there also without that disturbing insight that the artificial intelligence is another kind of intelligence , and it's scary , and it might be more powerful , and that this is something to really fear and worry about it . 

309
00:36:55,420 --> 00:37:00,820
So you are in the midst of you are one of the , the tool builders , SAIspace . 

310
00:37:01,190 --> 00:37:05,640
And how do you think about that , the in the promise and the potential paris. 

311
00:37:05,850 --> 00:37:09,470
What's a great question , and one that we mean , we talk about a lot . 

312
00:37:10,470 --> 00:37:20,600
I'm so first day , I like any technology can be used for good or evil , right , others , both sides do it , and we have to be careful right to manage that . 

313
00:37:20,600 --> 00:37:23,650
And I have overall think it's very positive , right . 

314
00:37:23,650 --> 00:37:27,600
I think it's be much easier for us to avoid that , really make it. 

315
00:37:27,680 --> 00:37:33,408
Backs of this , this is where someone might press you and say , well , but your technologies , right , and yes . 

316
00:37:33,408 --> 00:37:39,750
And so , and you said that in a fairly carefully away , so give me one of two bases for why you , you have an appointment. 

317
00:37:39,780 --> 00:37:44,470
What is the level of intelligence were really at an aim ? 

318
00:37:44,590 --> 00:38:11,600
So what we have even known that we spent huge advancements , especially in the deep learning technology in the last couple years , were so far away from human intelligence or anything that would be an independent super intelligence that that science fiction to me is like worrying about a media that's gonna hit earth sometime in the next million years , and in life , as we know , right is not imminent , right . 

319
00:38:11,600 --> 00:38:17,780
You're not high probability , you know , which nurses should make sure they get telescopes , you know , around the planet . 

320
00:38:17,780 --> 00:38:26,730
So we can see that when it's happening , so we need a small number of people worrying about , let's make sure we don't create a super intelligence that gets out ahead . 

321
00:38:27,230 --> 00:38:30,790
But I think the current technology today is so far from that. 

322
00:38:31,380 --> 00:38:32,480
That's not a big concern . 

323
00:38:32,850 --> 00:38:46,320
So the disturbing and point is something , as you said , it's not worth worrying about too much now , but on the other side , then to move forward suggests to me that you think the positive employees are really worth resuming. 

324
00:38:46,320 --> 00:38:48,705
What are just like a driver's cars , right ? 

325
00:38:48,705 --> 00:38:51,957
There is a lot of issues to be worked out there . 

326
00:38:51,957 --> 00:38:56,510
But I have no doubt when that comes , we'll be fewer deaths on the highway . 

327
00:38:56,510 --> 00:39:12,480
Then it will be with human drivers , medicine , you know , just huge scientists and applying this medicine , think one of the latest results was one of these deep learning systems can learn to recognize skin cancer events better than human . 

328
00:39:13,780 --> 00:39:24,200
You know , so just analyzing the vast amounts of data that we have , and you know , being able to understand it or just creating theory and personal systems that can do a better job . 

329
00:39:24,760 --> 00:39:27,100
So many things where AIis going to be. 

330
00:39:27,250 --> 00:39:28,180
Useful , right ? 

331
00:39:28,180 --> 00:39:32,290
This is making me think of your new grandchild , he or she , she is . 

332
00:39:32,290 --> 00:39:43,190
So he is going to grow up any time when AIis part of of the landscape , part of the technique sleep , he will not know what I am with everything I have that kind of. 

333
00:39:43,350 --> 00:39:46,550
That tried , I should have a great little video of him at one year old . 

334
00:39:47,030 --> 00:40:02,912
His first interaction was serious , and he's learned if he presses this button , the iphone talks to him , and he doesn't even have languages we understand , but he would press this button and have a conversation in his own global language , just fascinating . 

335
00:40:02,912 --> 00:40:07,170
So I think we'll be interesting to see how this generation grows. 

336
00:40:07,200 --> 00:40:09,420
Well , we have been going to want to see that video at some point . 

337
00:40:09,820 --> 00:40:13,960
So we are at the time , and I just want to thank you for engaging with . 

338
00:40:14,010 --> 00:40:17,609
And really , what has been an absolutely fascinating start of my morning here . 

339
00:40:17,609 --> 00:40:20,522
Lots of things we need to think about for the rest of the day . 

340
00:40:20,522 --> 00:40:22,065
So thanks , a lot for priority . 

341
00:40:22,065 --> 00:40:22,750
Yeah , thank you . 

342
00:40:22,750 --> 00:40:23,778
Certainly enjoyed it . 

343
00:40:23,778 --> 00:40:25,150
And thanks listeners for sure . 

344
00:40:25,440 --> 00:40:28,230
I hope you join us again for the next Voices from dark . 

345
00:40:31,060 --> 00:40:46,457
For more information about david gunning , the programs he runs and the other breakthrough technologies dark is working on visit , dark , adopt and for lengths that enabled you to download this . 

