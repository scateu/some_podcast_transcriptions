1
00:00:10,520 --> 00:00:20,870
Stuff technology , the internet , GPS in the column of your hand are tonight with technology is a driver of our times . 

2
00:00:21,150 --> 00:00:29,190
Since its founding a nineteen fifty eight , in the midst of the cold war , darker , the defence advanced research projects has been a driver of technology . 

3
00:00:29,980 --> 00:00:38,690
Welcome to voices from darpa , a window onto dark as a core of program managers , their job to redefine what is possible . 

4
00:00:39,260 --> 00:00:41,830
My name is iphone a motto , and on your dark host . 

5
00:00:42,100 --> 00:00:44,560
And today I am pleased to have with me in the studio . 

6
00:00:44,780 --> 00:01:07,400
Paul kong , a program manager , since twenty thirteen in darkness information office , which is also known here as I to o there , he oversees projects with names like big mechanism and communicating with computers that are all a bad thing about helping our all two human brand of intelligence collaborate with emerging computer based artificial intelligence . 

7
00:01:07,400 --> 00:01:11,430
Is it seen with deep groups to dark or spending back decades ? 

8
00:01:11,580 --> 00:01:15,666
So thanks , paul , for spending some time with me today , thanks for my childhood . 

9
00:01:15,666 --> 00:01:30,190
So given the projects you're overseeing here and which we will talk about , and just a little bit , i'm in trying to buy several aspects of your background and how we can talk a little bit come to mind are that you have a degree that combines psychology in computer science . 

10
00:01:30,190 --> 00:01:32,390
It's not a degree , and computer signs . 

11
00:01:32,390 --> 00:01:35,590
That's not a degree of psychology , but is one of combined . 

12
00:01:35,590 --> 00:01:59,980
And i'm also intrigued by your father , who am sorry to acknowledge past weeks a few months ago , but he also pushed in blood , the boundaries , humans and machines , in his case in the round of art are some hoping that you can talk about those two things , other aspects of your background that would help us understand how it is that you will end up your darker and why you are pursuing the solar . 

13
00:02:00,030 --> 00:02:01,530
That's a project that you are here. 

14
00:02:02,170 --> 00:02:04,750
Yeah , well , thanks for reading my dad . 

15
00:02:05,010 --> 00:02:08,860
He was in many ways , an inspiration to me . 

16
00:02:09,090 --> 00:02:12,680
Think , think back to the nineteen seventies . 

17
00:02:13,240 --> 00:02:17,080
Um , when not everyone had a computer , they want a laptop . 

18
00:02:17,080 --> 00:02:18,360
So I wasn't an internet . 

19
00:02:18,360 --> 00:02:55,170
Some people had computers and computing was that points out of the metaphor of our times , just the steam engines and electricity and other technology is what capture the imaginations of scientists , simple , it throughout history , as the computer took on that room , and many people , certainly , my dad , later , me started to think of the computer as a way of thinking , other things , how to make art have the mind works . 

20
00:02:55,510 --> 00:03:20,020
We started to think , or maybe it's a computer , or maybe computational processes could do those , but there's always been attention in AIbetween israel refusal , intelligence between whether you are using the computer as a matter of what humans do , or whether you know , you just modelling a new kind of intelligence , it's machine intelligence . 

21
00:03:20,350 --> 00:03:28,010
And I think what's happened over the last thirty years is that all of us who started out interested in using the machine as a matter what humans do . 

22
00:03:28,170 --> 00:03:34,119
I've now sort of come around to the fact the machine intelligence is only kind of intelligence , artificial intelligence . 

23
00:03:34,119 --> 00:03:36,165
These states actually has very long time. 

24
00:03:36,325 --> 00:03:41,600
这个题我们能操作，这样可以，那就say that reminds me of that combined that degree 。

25
00:03:41,600 --> 00:03:43,470
So talk to me a little bit about about a bit. 

26
00:03:43,820 --> 00:03:49,860
Well , I mean , yes , I have a combined degree in psychological psychology and computer signs . 

27
00:03:49,940 --> 00:03:59,980
But remember , that was in the late nineteen seventies , and we didn't even know what to call that thing than it's more recently being called cognitive science . 

28
00:04:00,220 --> 00:04:04,580
I attended the first cognitive science conference uses . 

29
00:04:04,580 --> 00:04:13,040
Sandy eagles are not graduate , but , but you know , I was using the machine to study human thinking . 

30
00:04:14,150 --> 00:04:25,130
Nowadays , when you talk about combining human and machine thinking , you really talking about something else you are talking about machine intelligence . 

31
00:04:26,420 --> 00:04:29,550
It's own kind of intelligence machine . 

32
00:04:29,550 --> 00:04:32,720
Well , with human intelligence , it's own kind of intelligence . 

33
00:04:32,720 --> 00:04:37,925
You're not talking about one trying to emulate the other or model the other , or should it stand in for the other ? 

34
00:04:37,925 --> 00:04:41,070
You're talking about two pretty mature kinds of intelligent. 

35
00:04:41,070 --> 00:04:46,034
Paul , listening to it , the kinds of interests you have an attraction to taking . 

36
00:04:46,034 --> 00:04:58,856
This does remind me that that your work really connects with account of long history here at darbar and trying to really find out how a human intelligence and artificial intelligence , you know , can work best together . 

37
00:04:58,856 --> 00:04:59,980
Yes , that's exactly. 

38
00:05:00,310 --> 00:05:07,120
And I think , for a really long time , people have viewed the machine role , a sort of power tool . 

39
00:05:07,860 --> 00:05:14,700
So the machine is going to do the very heavy groundwork that we don't want to do . 

40
00:05:14,700 --> 00:05:16,310
It was a proof reading . 

41
00:05:16,310 --> 00:05:20,240
It also deserts the web , but it will always do what we talk about it. 

42
00:05:20,290 --> 00:05:24,251
To do right and what the phrase data crunching comes to mind the crunch . 

43
00:05:24,251 --> 00:05:24,450
and. 

44
00:05:24,450 --> 00:05:33,450
That's right , but you know , let's let's have no doubt who is in charge and why the machines are doing things we , so that's been , there are two towards machines . 

45
00:05:33,450 --> 00:05:38,164
Machines are a , you know , that they're like the tools in my toolbox . 

46
00:05:38,164 --> 00:05:40,450
They've act on my direction . 

47
00:05:40,450 --> 00:05:53,140
They simply amplify what I want to change that I want and my communicating with computer program has as one of its tennis that the machine must actually have something to say and something to contribute to them . 

48
00:05:53,820 --> 00:05:55,590
The machine is going to be creative . 

49
00:05:55,590 --> 00:05:59,930
The machine is going to do somewhat of its own faults for its own . 

50
00:06:00,030 --> 00:06:00,800
Some reasons . 

51
00:06:01,290 --> 00:06:17,160
What I didn't count on is the enormous cultural barriers to that view of machines , even within my own program , explained what I mean , you know , within my own , uh I, Ido what north pork and managers do . 

52
00:06:17,160 --> 00:06:24,330
I say , let's pick some use cases or applications that will not really drive this technology . 

53
00:06:24,330 --> 00:06:33,870
And so you say , to all of these superb scientists tell me about some applications and all of the applications they come back with have them seen in the role of a servant . 

54
00:06:34,480 --> 00:06:36,400
And these are the best scientists in the world . 

55
00:06:36,740 --> 00:06:37,810
So you see , no , come on . 

56
00:06:38,270 --> 00:06:41,620
Imagine the machine was actually doing something for itself . 

57
00:06:41,820 --> 00:06:48,070
And it really takes a lot of effort to to think of machines , as you know what animals intelligence is. 

58
00:06:48,240 --> 00:07:03,006
Well , and I think you've gone so far as to challenge some of your performers , those who work with you on this program to even think about the machine as a coal , improvise a , we almost like they're in a jazz group together . 

59
00:07:03,006 --> 00:07:04,940
It is a little bit about that. 

60
00:07:04,940 --> 00:07:13,132
Yeah , well , actually dresses a very interesting form of communication , very , very similar to conversation . 

61
00:07:13,132 --> 00:07:14,010
And so fast . 

62
00:07:14,040 --> 00:07:26,150
There are pretty strict rules that govern the enter change between two jazz musicians , but there's also an enormous male creativity use cases within the program . 

63
00:07:26,150 --> 00:07:33,530
One is taking terms , contributing lines to a story or to a poem , or to a recipe . 

64
00:07:33,780 --> 00:07:53,297
And what we're finding is that when you look at a short poem , that is both by human and machines , and you showed that point to another human , that person listens so good , picking up which lines were written by the human and which were written by the machine , I would consider that a metric of success . 

65
00:07:53,297 --> 00:07:55,770
I think it is a metric of success given. 

66
00:07:55,770 --> 00:08:03,626
This is a program communicating with computers is a programme with in double defensive as research projects agency . 

67
00:08:03,626 --> 00:08:10,200
How do you imagine this new way of communicating with computers , assuming the general ultimately successful ? 

68
00:08:10,320 --> 00:08:12,340
Well , how will that change ? 

69
00:08:12,340 --> 00:08:18,960
Perhaps the way of those in the the world of national security relate to computers or use them ? 

70
00:08:19,130 --> 00:08:19,440
yeah. 

71
00:08:19,530 --> 00:08:33,130
Well , I think what one one has to realize that the modes with which we communicate with computers today are extremely unnatural , very , very comparison . 

72
00:08:33,130 --> 00:08:36,280
We thought gonna use to achieve things like keyboards . 

73
00:08:36,280 --> 00:08:43,942
I mean , keyboard , actually , if you want a computer to do something , then you should have two choices . 

74
00:08:43,942 --> 00:08:46,260
You can either be a programmer . 

75
00:08:46,260 --> 00:08:53,060
And most people aren't , or you can use the news , an interface that has been designed by a programmer . 

76
00:08:53,550 --> 00:08:56,920
You can point and click and swipe and things like that . 

77
00:08:57,910 --> 00:09:03,630
And neither of them is offering natural way of interacting with machine. 

78
00:09:04,190 --> 00:09:12,390
One of the girls , just on that point of natural , for example , here we are talking on looking to you on paying attention to your facial expression . 

79
00:09:12,540 --> 00:09:14,730
I am very just draw when I talks . 

80
00:09:14,730 --> 00:09:17,120
You can see my hands going all over the place . 

81
00:09:17,120 --> 00:09:19,500
Is that what you mean by natural sort of communication? 

82
00:09:20,030 --> 00:09:29,840
I do , but the sentence I just said , I do is also added during a wedding , but it means something completely different here . 

83
00:09:30,020 --> 00:09:33,200
Then it doesn't wedding , and that's because of context . 

84
00:09:34,000 --> 00:09:43,210
And there has never been a good account of a computational theory of context that would support diminishing communication . 

85
00:09:43,210 --> 00:09:49,025
And that's really the biggest technical problem in the communicating with computers . 

86
00:09:49,025 --> 00:09:57,670
Program is the natural communication depends on context , and we've got to be able to represent context when I say , I do . 

87
00:09:57,960 --> 00:10:00,570
You don't think that I want to get married with you. 

88
00:10:01,590 --> 00:10:03,981
I'm acceptable , yes , so I, Ican . 

89
00:10:03,981 --> 00:10:21,860
So how far along are you or your performers , which is sort of dark or speak for those who went up working on particular projects in getting machines to handle contacts with the certain context in the case of where there's an idea what you just think . 

90
00:10:21,860 --> 00:10:26,780
I agree with you versus , I want to marry you so helpful one day. 

91
00:10:26,970 --> 00:10:40,390
Well , we've picked three big problems to work on in the program , and one of the things that distinguishes them as the amount of context that's and the amount that has to be inferred . 

92
00:10:40,810 --> 00:10:50,580
So the simplest problem is to build a sort of castle out of wooden blocks on a table , and the context is available . 

93
00:10:50,580 --> 00:10:58,240
There is available to any computer vision system or to any human who can see , you can actually see the blocks . 

94
00:10:58,380 --> 00:11:02,437
So when I say , add another one , you can see what we're talking about . 

95
00:11:02,437 --> 00:11:04,630
The context is rather successful . 

96
00:11:04,630 --> 00:11:16,700
Then the second big problem we are working on is having humans and machines collaborate to build a very complicated models of biological systems . 

97
00:11:17,730 --> 00:11:27,881
And there , the context is maintained by all kinds of clever machine visualizations , you know , this molecule talks to that molecule . 

98
00:11:27,881 --> 00:11:30,572
And here's a great , big network of molecules . 

99
00:11:30,572 --> 00:11:35,955
What's the third use cases of a third use cases as a simple visitation , kind of use case . 

100
00:11:35,955 --> 00:11:47,150
And there the problem is that the context is established , either by the notes that the musician plays , or by the words that the person says , and nothing else just starts in this conversation . 

101
00:11:47,380 --> 00:11:53,280
Context is established by the words , and nothing else , not a hundred percent too right . 

102
00:11:53,580 --> 00:11:57,100
We both have common experience . 

103
00:11:57,800 --> 00:12:08,375
So context is established by the words , and this enormous background of human knowledge that turns out to be extremely hard for machines . 

104
00:12:08,375 --> 00:12:22,700
I have low expectations there , A, Amachine , for example , would not know why I do in a particular context is funny , because it just doesn't know anything about how people get married . 

105
00:12:22,700 --> 00:12:24,400
Or , you know , it just doesn't. 

106
00:12:24,960 --> 00:12:36,210
So when i'm waiting for now is the greatest metric metric of success for me will be when , when paul , you , from your program , we get the first committee computer . 

107
00:12:36,330 --> 00:12:40,710
Let's sit with it into your other project at the moment , big mechanism , yup. 

108
00:12:41,910 --> 00:12:45,280
Well , why are we doing big mechanism ? 

109
00:12:45,280 --> 00:12:51,670
We are doing it because I am terrified that our rich exceeds our grasp . 

110
00:12:52,460 --> 00:13:06,860
We are is a state human history where we can interfere with systems that we don't understand . 

111
00:13:07,530 --> 00:13:22,590
And that's okay if you're making some small experiment in your lab , you have to blow your lab , but it's not okay if it's the climate or the economy , or any of the other huge systems on which we depend on . 

112
00:13:22,940 --> 00:13:31,808
So so the problem is that we have really no good way of anticipating the effects of actions taken in very , very complicated systems . 

113
00:13:31,808 --> 00:13:42,310
So big mechanism is a program to develop technology to help humans to help build models of complicated systems , models of uni imaginable . 

114
00:13:42,310 --> 00:13:45,710
Complexity thinks that no human could possibly. 

115
00:13:45,900 --> 00:13:58,690
Along okay , and I think one of the ways that you are hoping computers will be able to do this gets a little bit back to that sort of data crunching boot four sets back . 

116
00:13:58,690 --> 00:14:11,330
But this , where the computer could go into the scientific literature and read one hundred thousand or a million papers that are available to something like maybe the answer of cancer , or something like that , something that people can I do. 

117
00:14:11,460 --> 00:14:13,440
Right , it's exactly what was it . 

118
00:14:13,440 --> 00:14:15,070
So talk a little bit about that. 

119
00:14:15,300 --> 00:14:21,000
Ability to kind of work at the information , spiritual ways that just are impossible . 

120
00:14:21,000 --> 00:14:26,790
But then what will the computer be able to gather from just all of all of that information? 

121
00:14:27,300 --> 00:14:44,300
Well , so let me first describe what , what we actually built , and then got it , and then i'll come back to question what we have now is big collections of programs that read the scientific literature in cell signaling biology . 

122
00:14:44,300 --> 00:14:49,940
Cell signaling is all of his medical processes , effects and cells , including cancer . 

123
00:14:50,200 --> 00:14:58,610
The machines can read those papers the at the moment I love you , so I can return to thousands of and out of every paper . 

124
00:14:58,800 --> 00:15:03,430
The machine extracts a little piece of biological mechanism . 

125
00:15:04,020 --> 00:15:08,890
You know , this molecule facilitates that molecule law binds with this molecular economy . 

126
00:15:08,890 --> 00:15:17,570
But every paper really talks about only a couple of little things like that , it may be a temperature paper , but the main result is like tiny . 

127
00:15:17,980 --> 00:15:26,880
So after you read three hundred thousand papers , you have safe , half a million tiny little fragments of a big company. 

128
00:15:26,925 --> 00:15:31,370
Get magnetical that little pixels of what might be a picture necessary. 

129
00:15:31,510 --> 00:15:40,280
And I have to put it all together , and so that the second big step in big mechanism is to assemble all of these fragments into biological model . 

130
00:15:41,030 --> 00:15:44,610
And of the third step is the reason without biological models or problems . 

131
00:15:44,610 --> 00:15:47,040
And how would I drag this cancer that kind of? 

132
00:15:47,260 --> 00:16:01,900
So this is giving me a sense of where our two different kinds of intelligence is really come together because we like to be able to tell stories we like to have causal explanations , mechanisms for why things happen in the world . 

133
00:16:02,730 --> 00:16:14,760
And we are so too good at that , but we are not great at at seeing how a hundred thousand points and dots connect , we can see how ten , twenty and thirty might wear . 

134
00:16:14,760 --> 00:16:18,010
The computer now can bring in one hundred thousand points . 

135
00:16:18,580 --> 00:16:28,500
But if we are working with it to say , computer , look at those points and also tell a cause of story , then we're beginning to come to work together in my getting yet a little bit of what you. 

136
00:16:28,695 --> 00:16:32,255
Yeah , and I want to be clear about why I think this is revolutionary . 

137
00:16:32,255 --> 00:16:51,250
And to do that , I want to go back to the fifties when dark was started as a responsible phoenix , the united states to emphasize sciences , engineering , but the united states also started to reward specialization in the science . 

138
00:16:51,870 --> 00:17:08,190
And we got to the point now where I can't understand what the guy in the office next door , does we have become highly , highly , highly specialized , hipper , specialized in the the speed which we read has not increased . 

139
00:17:08,930 --> 00:17:10,600
So we read more narrow . 

140
00:17:11,230 --> 00:17:21,740
Now , here's the problem , the world's existential problems do not fit neatly into any one academic department . 

141
00:17:23,330 --> 00:17:25,260
They are systemic problems . 

142
00:17:26,200 --> 00:17:35,970
And I can't think of the worst way to solve the systemic problem , then to get a bunch of people who are experts in the tiniest aspects of that system . 

143
00:17:37,300 --> 00:17:51,790
What we need is a big picture , and where humans can't machines , master , and humans can't , you know , I am doing well to read twenty papers a week , but in order to understand the system , I need to read three hundred thousand paper. 

144
00:17:51,925 --> 00:17:53,155
So this is very interesting . 

145
00:17:53,155 --> 00:18:01,435
And in a way which you think , if you look even just at a structure of the university or even a company , you have to do sort of departments . 

146
00:18:01,674 --> 00:18:09,329
And we're almost institutionalize in an inability to sympathize in this very broad based way that you suggesting . 

147
00:18:09,329 --> 00:18:15,420
But the institutions continue to sort of put us into these compartments was using . 

148
00:18:15,420 --> 00:18:22,290
We now are in a place where we can combine our desire to sympathize with the parents of computers have for a new kind of. 

149
00:18:22,550 --> 00:18:25,590
Error of scientists and new kindness scholarship . 

150
00:18:25,590 --> 00:18:30,608
That's why we view the pollution is not just that the machine can read a large number of papers . 

151
00:18:30,608 --> 00:18:37,520
Is that by doing so , we have a chance , just a chance at understanding the systems on which we depend for our survival. 

152
00:18:37,610 --> 00:18:39,960
Okay , so that really is exciting to me . 

153
00:18:40,310 --> 00:18:43,590
But as you know , this is a great dialogue going on society . 

154
00:18:43,590 --> 00:18:50,630
Now about , you know what it means for computers to get smarter , to get more intelligent to get smarter like us and intelligence like us . 

155
00:18:50,630 --> 00:18:52,900
We're smarter , intelligent in their own ways . 

156
00:18:53,250 --> 00:18:54,960
And this brings up some concerns . 

157
00:18:54,960 --> 00:18:56,900
It brings up the excitement and concerns . 

158
00:18:57,390 --> 00:19:02,098
I love to hear what you are excited about as the computers evolved this way . 

159
00:19:02,098 --> 00:19:06,100
In fact , as you have a role in that , and also , what gives you some part? 

160
00:19:06,940 --> 00:19:16,690
If people understood just how little machines were capable of , I, Ithink you would be hearing. 

161
00:19:17,470 --> 00:19:29,379
So much of the snow there are others I just want to push back a little bit there , because , you know , four decades bell lab rolled out a single transistor is an ugly looking thing . 

162
00:19:29,379 --> 00:19:37,370
And now you can get a chip that has billions on something , you know , that that it's snuggling in my palm . 

163
00:19:37,980 --> 00:19:42,830
Those are worlds apart , because there was nothing much that little single switch could do . 

164
00:19:43,200 --> 00:19:46,600
Maybe do something in a telephone system initially . 

165
00:19:46,600 --> 00:19:51,210
And now we have microprocessors that do you know wonders on our , even on our home computer ? 

166
00:19:51,210 --> 00:20:01,280
So , so if we add time to to the picture here , do you still think that people do not have justification of some concerns? 

167
00:20:02,130 --> 00:20:09,180
I think that people are right to be concerned about field in new technology . 

168
00:20:09,590 --> 00:20:11,410
Are they going to work ? 

169
00:20:12,460 --> 00:20:17,370
You know , self driving cars , could , I suppose , get people into traffic accidents ? 

170
00:20:17,370 --> 00:20:17,490
He said. 

171
00:20:18,750 --> 00:20:22,370
Cross the way , or the third way you are right. 

172
00:20:23,570 --> 00:20:28,910
The reason I know of that concerned is this is not by any means . 

173
00:20:28,910 --> 00:20:34,770
The first time the society has come to grips with threats posed by new technologies . 

174
00:20:34,770 --> 00:20:47,370
And we always figured out how to deal with those threats in one way , another society finds ways to offset the costs associated with new technologies . 

175
00:20:47,370 --> 00:20:49,770
We simply build it into the cost of doing business. 

176
00:20:50,720 --> 00:20:52,540
Right , so we learn how to manage to repeatedly. 

177
00:20:52,680 --> 00:20:53,120
The rest. 

178
00:20:53,900 --> 00:21:02,308
And that reminds me some specific way starbucks is working to manage the risks of becoming error of intelligence and machine learning . 

179
00:21:02,308 --> 00:21:10,850
I'm thinking here of programs like explainable AI , which your fellow program manager , david gunning is running in a net programme . 

180
00:21:10,850 --> 00:21:33,227
He's got the goal of developing artificial intelligence and machine learning techniques that not only produce results and gather data and make decisions to help human partners managing operate the complex systems in our world , but AI systems that do this in ways that are transparent to the humans in the loop , and that with the machine behavior , remains understandable and explainable . 

181
00:21:33,227 --> 00:21:34,470
And listen theory trust . 

182
00:21:34,780 --> 00:21:38,700
So I want a kind of move away from maybe the specific programs . 

183
00:21:38,700 --> 00:21:59,760
And I ask you about durban as a part of the overall innovation ecosystem , and what i'm thinking about there , as other elements of the ecosystem are , say , academic were hurt , thinking about refunded mental source of questions and doing maybe fundamental , which getting new data . 

184
00:21:59,760 --> 00:22:19,100
The letter benedict , before i'm thinking about , archaeologists , were thinking about how to apply science and into and turn that into various small and large technologies , and sort of weird doubt fits into all of that in , as we said in the very beginning in being a driver of technology. 

185
00:22:20,260 --> 00:22:25,040
Well , of course , dark has a very distinguished history is a driver of technology . 

186
00:22:25,250 --> 00:22:31,230
And we could talk about why it's been so successful , but maybe I could related a bit . 

187
00:22:31,230 --> 00:22:38,720
My personal parents would succeed work for dark as a performer for thirty years before coming here . 

188
00:22:38,720 --> 00:22:39,800
The program manager . 

189
00:22:40,360 --> 00:22:52,086
And I have to say , you know , I thought I knew dark , but it wasn't like god in the building that I realized that is really , I think AApretty narrow slice of what dark it does . 

190
00:22:52,086 --> 00:22:58,610
Darpa sits at the intersection of government , industry , academia , another kinds of research , and it brings you all together . 

191
00:22:58,610 --> 00:23:00,630
It's , it's got this . 

192
00:23:00,630 --> 00:23:08,790
So the big picture of how innovation happens in the united states , and he's been very successful at. 

193
00:23:09,150 --> 00:23:12,090
Making it possible for everybody to do what they do best. 

194
00:23:12,280 --> 00:23:26,870
You know , the words that goes on the academy , it tends to be , if I might say a little conservative academics might not be surprised to do this , but I think academics , but tend to be pretty conservative in the problems they , they are . 

195
00:23:26,870 --> 00:23:45,501
They tackle so , so dark as role and academia has , let's take on something that's impossible , and we can appeal to get it done , you know , so for that , that sort of the message to academia , the message , the government is look what we've just changed your view . 

196
00:23:45,501 --> 00:23:49,847
What , what technology can do think about how you can use it . 

197
00:23:49,847 --> 00:23:58,110
Think about how you can use it to achieve darkness , mission of creating , avoiding strategic surprise. 

198
00:23:58,710 --> 00:24:09,060
Right and listen to you , talk about the replacement in the innovation ecosystem and moving technology for it also reminds me a little bit of what you were saying about . 

199
00:24:09,400 --> 00:24:28,640
Big mechanism is and bringing for the new kind of synthesis , new ability to take many different categories of thinking , bringing together one thing the dark seems to do is to bring communities together to cross the disciplinary boundaries in ways that might not be very easy for the performers in their own institutions . 

200
00:24:28,860 --> 00:24:35,250
But here is a program manager , you know , when we can bring together five , six different performers together and the create a community to send exist before. 

201
00:24:35,610 --> 00:24:40,040
So we thought about ultra specialization and mom and go on users who are disciplinary . 

202
00:24:40,040 --> 00:24:52,550
And and I got really interested in the word to interdict general , because it occurred to me that I heard it every day at the university , and I hear never adopted her . 

203
00:24:53,370 --> 00:25:00,670
And i've come to view the word into disciplinary as standing for a problem , not standing for solution . 

204
00:25:01,240 --> 00:25:09,280
And the reason that we don't hear that where the dark is that we just take it for granted , everything we do darkness into disciplinary . 

205
00:25:09,560 --> 00:25:11,980
All of my teams are into disciplinary . 

206
00:25:11,980 --> 00:25:13,430
I have machine reading people . 

207
00:25:13,430 --> 00:25:14,730
All I have linguist side . 

208
00:25:14,730 --> 00:25:17,610
Biologists said it's just taken for granted . 

209
00:25:17,610 --> 00:25:17,960
right . 

210
00:25:17,960 --> 00:25:19,140
I mean , anything I looked at. 

211
00:25:19,220 --> 00:25:26,950
See the pin dark by the projects , which is another way of thinking about this is a sort of problem driven way of going about orange . 

212
00:25:26,950 --> 00:25:29,470
So you have the end a part of an appointment . 

213
00:25:29,470 --> 00:25:34,910
You don't know exactly where you're going , because these are leading areas , but you have some sense . 

214
00:25:35,310 --> 00:25:41,220
Then really , you're not saying this is a biology problem with physics , problem or engineering problem . 

215
00:25:41,220 --> 00:25:47,130
It's just a problem that has to be solved , no matter what mindset or skill set you need to bring in . 

216
00:25:47,130 --> 00:25:49,320
And it turns out that may be a huge. 

217
00:25:49,350 --> 00:25:57,930
Hey yeah , when people get to the project , and they , they are trained in machine learning , and suddenly a cancer biologist is up . 

218
00:25:57,930 --> 00:25:59,580
There are speaking their eyes . 

219
00:25:59,580 --> 00:26:01,660
Get big , making the room gets quiet . 

220
00:26:02,060 --> 00:26:14,540
And after a few weeks or months there , the happiest people , because this is the way things should be done , and they get to participate in that kind of science. 

221
00:26:15,190 --> 00:26:23,680
On that note , I think we've run out of our time for the discussion , and I can be eager to follow up with you at some other times . 

222
00:26:23,680 --> 00:26:25,040
And so I just want to thank you . 

223
00:26:25,040 --> 00:26:28,710
Paul , for joining me in the studio for this really great. 

224
00:26:28,955 --> 00:26:31,015
Yeah , I can choose you very much , thanks so much. 

225
00:26:32,110 --> 00:26:34,490
And thank you listeners for joining us . 

