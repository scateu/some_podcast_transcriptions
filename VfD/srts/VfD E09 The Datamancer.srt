1
00:00:11,580 --> 00:00:17,760
Stop technology , the internet GPS in the column of your hand on tournament. 

2
00:00:19,120 --> 00:00:30,210
Technology is a driver of our times since its founding a nightmare video in the midst of the cold war , doubt the defence advanced research has been a driver of technology . 

3
00:00:30,570 --> 00:00:39,260
Welcome to Voices from DARPA , a window on the dark as core of program managers , their job to redefine what is possible . 

4
00:00:39,920 --> 00:00:42,750
My name is ivan , a motto and arm your dark bowls . 

5
00:00:43,140 --> 00:00:57,001
And today I am pleased to have with me in the studio weight shen , a program manager since twenty fourteen industry information office , weighed earned a bachelor's degree in electrical engineering in computer science from the university . 

6
00:00:57,001 --> 00:00:58,227
Called , from your birthday . 

7
00:00:58,227 --> 00:01:14,080
He then attended the university of in college park , which is just about twenty miles from where we are sitting in dark headquarters , where he received his massive screen computer science and particularly to double weight work at the m it linking laboratory in the human language technology group . 

8
00:01:14,640 --> 00:01:16,990
You can tell from the portfolio programs . 

9
00:01:16,990 --> 00:01:37,680
He runs the dark that he is fascinated by language , both of the human kind end of the computer , and you can tell also that it is driven by a very dark ASvision of computers and human beings were working together in ways that extend what either type of intelligence , look , computer intelligence or human intelligence can do on its own . 

10
00:01:37,680 --> 00:01:40,360
So thanks , wait for joining me in the studio today . 

11
00:01:40,470 --> 00:01:41,120
Thanks , I will . 

12
00:01:42,120 --> 00:01:44,020
So let's start up by talking about you . 

13
00:01:44,020 --> 00:01:49,230
Wait a little bit about your background and how you ended up at. 

14
00:01:49,310 --> 00:01:50,140
Been twenty fourteen . 

15
00:01:50,580 --> 00:02:09,424
Sure , so in a dark has a long string of research and communicating with computers and various forms and involved in the number of those programs over many years from from day's been a graduate student through through my time at lincoln . 

16
00:02:09,424 --> 00:02:18,560
And we sort of natural progression , I was working on a program as a program , and I knew the program manager really well . 

17
00:02:18,560 --> 00:02:27,280
And he said , hey , you know , you should think about coming the job , but you know , this is an opportunity to do things that you can't possibly do in the job that you're currently at. 

18
00:02:28,170 --> 00:02:47,770
Before we dive in some of the dark programs you oversee , I am hoping you can let us in on some of the overarching ideas you have about human minds and computer mines and language , perhaps in the context of these times of our times , when so much of what we do involves people interacting. 

19
00:02:47,910 --> 00:03:01,110
Computers , you know , sort of the history of this is the in our computers were good at various kinds of things that humans were good at doing mathematical calculations without errors in a repeated and very quickly . 

20
00:03:01,640 --> 00:03:06,960
But the fact this is that we didn't quite know how to honest that to solve problems we really care about . 

21
00:03:06,960 --> 00:03:20,554
So any serious problem , like decision making of any kind in a global scale policy , anything that you might think of in a requiring human expertise to do , you know that wasn't something that we would engage computers and right ? 

22
00:03:20,554 --> 00:03:33,840
It's just now that we're thinking about having computers , drive cars , and that's a very basic function that , you know , we trust large portions of our population to do and comes naturally to human beings with some practice . 

23
00:03:34,470 --> 00:03:46,750
But computers are just getting to the point where they can sense the world well enough that they can do that , and they can make decisions that are your commentary with what human being can do on these very basic tasks . 

24
00:03:47,160 --> 00:03:59,930
And a number of us want computers to be smarter because we want them to be companions that can help us do things right , and we want them to help us solve problems that are really , really hard for us and make decisions that are much more complicated . 

25
00:04:00,030 --> 00:04:01,760
And then just driving a car , for instance. 

26
00:04:01,810 --> 00:04:02,239
right ? 

27
00:04:02,239 --> 00:04:12,115
So let me just stop you on understanding of computers making decisions because in a fertilize of me , I cannot really always pass out how I make my own decisions . 

28
00:04:12,115 --> 00:04:15,540
I presume , bringing in different kinds of data , different elements . 

29
00:04:15,540 --> 00:04:21,460
And then there are some kind of algorithm going on my own head by which IImake a decision about where i'm going to go to day . 

30
00:04:21,460 --> 00:04:24,360
We're from going to buy this item , forces that item . 

31
00:04:24,730 --> 00:04:33,430
So can you talk to me a little bit about what is it takes to put that mystery into working in a working way into a computer? 

32
00:04:33,540 --> 00:04:37,590
The mystery of decision , like it isn't necessarily the case that we want to make him a decision making . 

33
00:04:37,590 --> 00:04:38,131
It anyway . 

34
00:04:38,131 --> 00:04:41,148
Human beings are always great making decisions . 

35
00:04:41,148 --> 00:04:53,928
You know , you know about the study of children , and whether or not they'll take a sneakers bar now versus waiting for thirty minutes and then getting five sneakers bars , and in a day , which tuesday is the satisfaction now . 

36
00:04:53,928 --> 00:04:57,156
And so we have authorities in our own decision making . 

37
00:04:57,156 --> 00:04:59,980
It makes us make decisions that are so optimistic . 

38
00:05:00,030 --> 00:05:02,520
Our own outcomes , right ? 

39
00:05:02,790 --> 00:05:09,890
And this happens at a huge scale that happens when we make political decisions that happen when we make economic decisions . 

40
00:05:09,890 --> 00:05:24,280
And someone is so full , and what you hopefully is the ability to make better decisions , given an outcome goal that you are based on better data and better understanding of of the processes under the heart . 

41
00:05:24,630 --> 00:05:26,020
And that's why things computers can help. 

42
00:05:26,220 --> 00:05:31,630
Well , I, Ifor one would personally like to be in a position where , where I can make better decisions in my own personal life . 

43
00:05:31,630 --> 00:05:36,470
If we can do that also on national and geopolitical scales , all the better . 

44
00:05:36,470 --> 00:05:40,090
So all the power to you wait , I see in that political ambition . 

45
00:05:40,350 --> 00:05:45,310
So what I like to do now is just talk to you live with a report photo of programme . 

46
00:05:45,310 --> 00:05:52,430
What would you start with the moments program and tells a little bit about what that is about , and what its statistics now has. 

47
00:05:52,530 --> 00:05:53,750
News a few times , yes . 

48
00:05:53,750 --> 00:05:57,190
So the premise behind the magic programmers is really very simple . 

49
00:05:57,570 --> 00:06:05,490
It's the first data after on the internet that we can't quite get a hold of or make sense of for investigator purposes . 

50
00:06:05,960 --> 00:06:12,950
So our program concept was , how do we help people who are doing investigations online ? 

51
00:06:12,950 --> 00:06:20,170
And we started with human trafficking , because human trafficking is something that were the point of sale was almost always online , at least for sex trafficking . 

52
00:06:21,110 --> 00:06:35,430
We started with that particular domain because we notice that the data online , you know , people buy and sell human beings online and visit a grotesque and overt at the same time . 

53
00:06:36,170 --> 00:06:43,430
And yet , because of the volume of the data that out there , because it's , it's hiding in a huge sea of even more data . 

54
00:06:43,790 --> 00:06:47,690
It's actually really hard for investigators to explore it. 

55
00:06:47,950 --> 00:06:54,400
Effectively , it is not just the data that seems as kind of most of us think of as the normal web , which is index by normal. 

56
00:06:54,480 --> 00:06:54,990
正确的是。

57
00:06:55,070 --> 00:07:04,030
In other places that try it and places like the deep web and the dark web and play , this is that commercial search indexes don't spend a lot of time dealing with . 

58
00:07:04,470 --> 00:07:10,240
And you know , our only real view into that data is through commercial search engine . 

59
00:07:10,400 --> 00:07:20,870
We rely on them to index and make available to search the contents that online , and it's not indexed or available through google is very likely you never gonna see . 

60
00:07:20,930 --> 00:07:30,178
And so portions of the dark web , where a lot of crime happens , portions of the deep webb were lots of ADS for sexual for prostitution exist . 

61
00:07:30,178 --> 00:07:34,210
A lot of that isn't fully index by our commercial , certains . 

62
00:07:34,370 --> 00:07:49,120
And so the promise of the program was collected right data , and then automatically organized a data in such a way that we can explore that data and most importantly , generate leads and take instances of a bad activity . 

63
00:07:49,540 --> 00:08:10,000
And so we have been working for the last almost three years now to build a system that would help investigators one chase down leads that they have , but too , also , they lead to automatically , potentially , potentially adds that are associated with trafficking so that they can prioritize their investigations . 

64
00:08:10,000 --> 00:08:15,450
And so the concept here is in a sort of back to the theme of human machine interaction . 

65
00:08:15,450 --> 00:08:26,350
The concept here is , how do you help the investigator get to the data that sailing and further particular investigation , and then how do you help them analyse data that they can possibly analyze on their own ? 

66
00:08:26,720 --> 00:08:40,196
And one of the key findings that program was the pricing behaviors of organizations that involved in trafficking different than the pricing behaviors of in normal , independent contractor prostitutes . 

67
00:08:40,196 --> 00:09:04,526
And you can detect those , and you can detect those patterns , but you can see that by reading an individual and online , you have to look collectively at a whole bunch of ADS uh author by the same person , sometimes tens of thousands of these ADS in order to understand the statistical pattern of health price , risky behaviors differently than the average in that marketplace . 

68
00:09:04,526 --> 00:09:14,010
That's the kind of question that's very , very hard for an individual to answer without analytics that help them do that analysis of religion and data and weight has. 

69
00:09:14,040 --> 00:09:21,750
This tool mexico went to against the kind of deliverable that imagine we all want , which is sort of taking down sex trafficking rings or other criminal. 

70
00:09:21,880 --> 00:09:27,230
Yeah , so in a way , since the beginning of this program , we have been engaged . 

71
00:09:27,530 --> 00:09:40,919
Law enforcement agencies across the country and really across the world , we have users in the UKand canada and australia and other places , not just the united states who every day working with these tools to arrest people . 

72
00:09:40,919 --> 00:09:49,610
This morning , we have hundreds of arrests that have been credited to the use of magic store and a number different convictions that happened out. 

73
00:09:49,760 --> 00:09:53,870
Well , great a sense ago , at least they slightly improved worlds by a mammoths . 

74
00:09:53,870 --> 00:09:54,600
That's great news . 

75
00:09:54,770 --> 00:09:58,070
Let's segregate to another one of your programs . 

76
00:09:58,070 --> 00:10:01,202
The data driven discovery of models programs . 

77
00:10:01,202 --> 00:10:03,406
So we wait , what do you try to do with this ? 

78
00:10:03,406 --> 00:10:05,210
And what is the status of this one? 

79
00:10:05,420 --> 00:10:15,440
Yes , the fundamental thought in this program is that , you know , when we want to construct an empirical model of how the world works , we use data to do that nowadays . 

80
00:10:15,440 --> 00:10:16,894
We stayed everywhere . 

81
00:10:16,894 --> 00:10:28,913
And so the bottle networks to construct models of how the world's isn't , the , you know , isn't any longer the collection of the data itself necessarily process of constructing the model . 

82
00:10:28,913 --> 00:10:39,625
That process is something that's relegated to people who are typically called data scientists from machine learning people , statisticians , people with that kind of title associated with him . 

83
00:10:39,625 --> 00:10:57,544
And though that the effort involved in constructing those models is huge rights , hundreds of thousands of millions of effort , largely because the data's massive , largely because how they model that data statistically requires lots of expertise in the areas of statistics and machine learning . 

84
00:10:57,544 --> 00:11:03,520
And so what we basically said is he , let's that out of the hands of those kinds of experts and automatic. 

85
00:11:03,520 --> 00:11:04,937
Let's motivate this reason . 

86
00:11:04,937 --> 00:11:10,430
I wanted to want to hear about this , but also I want to listen to have a little bit more of a concrete sense . 

87
00:11:10,430 --> 00:11:12,380
We've been talking in certain general terms . 

88
00:11:12,380 --> 00:11:14,002
Yeah , models and and systems . 

89
00:11:14,002 --> 00:11:21,650
Uh , what might be some of the specific phenomenon that you're talking about for this kind of model development programs. 

90
00:11:21,680 --> 00:11:33,364
Just to give you a couple of problems that or can't create , the people have been thinking , what is , you know , how would you given different kinds of general types of various forms of weight ? 

91
00:11:33,364 --> 00:11:55,830
How would you predict the yield of those varieties of weight and different environmental that requires a model , and if you could do that , you could know what a plane , any given year , depending on what you expect for the weather and the demand of rain in the particular soil location , you could make decisions that were much smarter than we make now . 

92
00:11:56,350 --> 00:12:00,714
And so one of the problems that we're working on is , how do we ? 

93
00:12:00,714 --> 00:12:18,730
How do we predict copyright based on data and I data can come from general type data can confirm environmental observations of centers are like taking pictures of crop fields , making come from , you know , your farmers , all my neck that tells you , you know , three months from now , what the weather is likely to be the season . 

94
00:12:19,660 --> 00:12:29,759
So those kinds of data can build a predictive model of what kind of crop yields you , and if you can do that , you can need of highly affect the outcomes of agriculture . 

95
00:12:29,759 --> 00:12:31,830
That's just one kind of creative example. 

96
00:12:31,860 --> 00:12:48,310
We probably can also link this to a big mission of double , which is a and enhancing national security when we , even when we're talking about food that relates to things like food securities of we can , we will keep our food supply of constant , and at levels we need that actually . 

97
00:12:48,310 --> 00:12:50,160
And it does a conductor across the night. 

98
00:12:50,210 --> 00:12:54,428
That's right , it , it helps us to predict political instability . 

99
00:12:54,428 --> 00:12:57,315
For instance , in countries that we care about . 

100
00:12:57,315 --> 00:13:00,946
So those kinds of models are are always , it is important . 

101
00:13:00,946 --> 00:13:03,028
Social , political models are important . 

102
00:13:03,028 --> 00:13:03,407
right ? 

103
00:13:03,407 --> 00:13:16,710
Can you predict whether or right is going to work , and can you predict that unrest , political and rest of a certain type is going to result ten days from now in some kind of bad event , those are the kinds of things that you like to build models. 

104
00:13:16,830 --> 00:13:23,598
What is the challenge that of getting a kind of computer with access to all kinds of data to generate those models ? 

105
00:13:23,598 --> 00:13:31,650
I don't know where the human mind comes into this , but to generate those models from the data and in perfect sort of massive information that are available. 

106
00:13:31,650 --> 00:13:34,810
So there's sort of two real problems that we have to content . 

107
00:13:35,030 --> 00:13:38,680
So first of all , we have too much data of too many different kinds . 

108
00:13:38,970 --> 00:13:48,060
And so the problem with that is that we don't know which combinations of the data are likely to explain the particular outcome that we care about , care about crop yields . 

109
00:13:48,060 --> 00:13:57,119
We need to know that temperatures are a funeral of days prior to a particular event also said , with croppy alcohol standards later , right ? 

110
00:13:57,119 --> 00:14:03,502
That's a kind of thing that , you know , require expertise and agriculture to be able to know computers . 

111
00:14:03,502 --> 00:14:07,661
Don't know that there is where sympathizes comes battery , battery . 

112
00:14:07,661 --> 00:14:10,726
So what computers can do is calculate statistics . 

113
00:14:10,726 --> 00:14:19,732
We can estimate how well correlated a particular value available is where particular kind of data is to a particular outcome we care about . 

114
00:14:19,732 --> 00:14:31,246
And so computers can discover patterns that human beings have a hard time figuring out on their , but human beings are also very good at figuring out that some of these patterns aren't real . 

115
00:14:31,246 --> 00:14:34,494
Some of these parents are what we call chance correlation . 

116
00:14:34,494 --> 00:14:43,979
And so what we would like to do in order to build a really effective model is to find variable because the matter factors that affect the outcome of crop yields . 

117
00:14:43,979 --> 00:14:54,730
For instance , we like to be able to do that automatically , but because some subject matter expertise is involved , we need the help of human being to help curry this process . 

118
00:14:55,090 --> 00:14:59,930
And so in the district program , the goal is to make it at the day. 

119
00:15:00,030 --> 00:15:03,480
Leader driven discovery , he has some models program , the deterioration program. 

120
00:15:03,530 --> 00:15:17,270
The goal that program is to fundamentally change the category of labor from statistician , the machine learning expert that constructs the model to machine plus subject matter expert , to construct the model . 

121
00:15:17,740 --> 00:15:21,560
And so the idea is that we have a machine that automatically test . 

122
00:15:21,560 --> 00:15:22,700
So how about your models ? 

123
00:15:22,890 --> 00:15:29,794
And it does so an efficient way , and , you know , in general , for most of these problems are infinitely many models that could be used . 

124
00:15:29,794 --> 00:15:50,700
And so the question is how you find the right , one machines are very good at doing that testing and automatic fashion , and if they can work with human beings to constrain the kinds of models that they explore , they can potentially find models very , very quickly , very effectively without the need for export machine learning personnel , or did the scientists. 

125
00:15:51,420 --> 00:15:58,360
Well , sleeping , so let's move to the third program , the quantitative crisis response programs . 

126
00:15:58,360 --> 00:16:00,640
Again , a little bit of description program . 

127
00:16:00,640 --> 00:16:02,060
And what kind of program is making it? 

128
00:16:02,470 --> 00:16:17,480
So the golden QCRquantitated crisis response program was to figure out how information affects people and to build a way to measure that that is the fundamental goal in our program . 

129
00:16:17,850 --> 00:16:30,488
So you know , with social media and fake news and propaganda being distributed by organizations like right , argo was to figure out , how does that turn into a radicalization outcome ? 

130
00:16:30,488 --> 00:16:36,532
How was it that videos online , convert people into fighters on the battlefield ? 

131
00:16:36,532 --> 00:16:53,920
So what we've been doing that crime is building models again of how human beings respond to propaganda , and how effective the spirit of propaganda are by looking at measurements of how people react in places like social media . 

132
00:16:54,670 --> 00:17:08,108
And so the fundamental goal there is to build a recall measure of effectiveness , which is really way to measure concrete outcomes associated with various pieces of information that I put out their very speech of propaganda that put out there . 

133
00:17:08,108 --> 00:17:19,080
So , you know , in the last couple of years , what we see is that our organizations like ice is , can reach millions of people through social media , and they effectively recruit a small number of them . 

134
00:17:19,080 --> 00:17:24,100
But still , you know , when you can recruit millions more percentages , add up to a lot of fighters . 

135
00:17:24,100 --> 00:17:33,855
And and so the question is , how did they do that on the one hand , and how effective are we stopping them , or at , you know , are they actually achieving that outcome ? 

136
00:17:33,855 --> 00:17:56,795
He said , and so what we've been building is essentially a gauge is essentially a meter that says , oh , today , that messages that you put out has a air degree of potential for radicalizing people than the one that you put , and why that matters is that it allows us people who are interested in covering recognition to estimate how well we are doing it , preventing that radicalization from happening . 

137
00:17:56,795 --> 00:17:59,980
Recently , building a meter that allows us to know. 

138
00:18:00,000 --> 00:18:06,638
We're doing well or poorly , so they're the one thing that seems to apply . 

139
00:18:06,638 --> 00:18:11,380
Maybe all of these programs is that idea of economic . 

140
00:18:11,410 --> 00:18:27,258
How can human beings and their machines and their computers and all of their online resources , how can all of these things work together , economic and a symbolic way so that we derive knowledge and wisdom for from all of this interaction . 

141
00:18:27,258 --> 00:18:36,598
It reminds me of real account of the early days , year of garbage , and particularly thinking of one of our more famous and early program managers . 

142
00:18:36,598 --> 00:18:38,466
Joseph carl , rob , net legislator . 

143
00:18:38,466 --> 00:18:44,924
Most people know jessica legal letter and who was even back in the sixties when it was a program manager . 

144
00:18:44,924 --> 00:18:50,449
Here was articulating this idea of a symbolism between human beings and machines . 

145
00:18:50,449 --> 00:18:51,650
I'm just wanting to . 

146
00:18:51,680 --> 00:18:55,740
Has it failed to be a program manager , and it was working in this long , long. 

147
00:18:56,160 --> 00:19:06,240
A traditional durban , well , it first was humbling when you look at the history of program managers are darker , and the various things that dark is produced over its lifespan . 

148
00:19:06,240 --> 00:19:19,320
We are all humbled by the great outcomes those folks had in the programs that they were produced out of here , you know , most of us try to do our job , but all of us are thinking bigger than the things that we could donate our own . 

149
00:19:19,600 --> 00:19:31,320
And that's the thing , the dark benjamins and all of us , right , is the opportunity lawyer in his research , that's much larger than anyone's research organization can do. 

150
00:19:31,320 --> 00:19:39,410
Before we , we close out of our conversation , and I think is relatively with what you just said it might like you to come to stand back from any particular program . 

151
00:19:39,410 --> 00:20:05,765
And it may be sure with us , some of what you observe since twenty fourteen , as a program manager , about doctors role in the overall innovation ecosystem , and as an eco system and includes many players and stakeholders , scientists and engineers in an academic industry and government loves those among the stakeholders , entrepreneurs , inventor , capitalists , the science policy community . 

152
00:20:05,765 --> 00:20:11,530
The ecosystem involves players in the commercial and environmental , defence and other sectors . 

153
00:20:12,420 --> 00:20:16,670
So where do you think the arab fits into this overall , you know. 

154
00:20:16,750 --> 00:20:18,060
Innovation , dynamic . 

155
00:20:18,320 --> 00:20:19,910
Well , it is interesting , right ? 

156
00:20:19,910 --> 00:20:40,030
So there are a lot of , obviously , even within the USgovernment , there are a number of organizations from basic research , the fund applied research , the fund , various projects along that continuum , and I think darker , has a very unique role that spends the gap between basic research and polite science . 

157
00:20:40,650 --> 00:20:57,690
Often times we are looking for the applications that seem impossible , whether it's in a launching drawings of a flying aircraft , or , you know , detecting a human trafficking online , all of these things are things that we are emerging . 

158
00:20:57,690 --> 00:20:59,580
Capability three or four years ago . 

159
00:21:00,030 --> 00:21:16,190
Now , he proved to be possible , and so darkness in a very unique place that allows us to push the technology beyond the theoretical stages to the point where we can actually see where the , where the applications are going to be . 

160
00:21:16,190 --> 00:21:25,600
So we're not builders of the ultimate technology right at the end of the day , we found the research , and that research eventually becomes part of another form . 

161
00:21:25,780 --> 00:21:45,390
But if you look at the in a progression of robotics and self driving cars , all of those AI technologies were created out of dark programs early on and have , subsequently , you know , a ten year horizon after that , you know , become prototypes of industry has been able to take it up . 

162
00:21:45,390 --> 00:21:47,250
So we serve a very crucial role . 

163
00:21:47,700 --> 00:21:53,430
And the reason that we can serve that role is to fall on the program managers themselves . 

164
00:21:53,430 --> 00:22:02,580
These folks come with the ideas , and they can see the horizon they been working in the research they understand , and the basic research is happening in that field. 

165
00:22:02,625 --> 00:22:06,207
So you have the same position , you , you have boots on the ground . 

166
00:22:06,207 --> 00:22:10,000
Let's write a particular of technology strategy coming from. 

167
00:22:10,030 --> 00:22:11,190
That's right , and we don't stay long . 

168
00:22:11,280 --> 00:22:15,260
And part of the reason we can stay long is that we are going to lose view of the horizon . 

169
00:22:15,260 --> 00:22:18,560
If we do right , we need to be back in the gulf of that research . 

170
00:22:19,040 --> 00:22:21,780
And so all of us are here for the temporary period time . 

171
00:22:21,780 --> 00:22:26,090
And I think that was a brilliant design in terms of how we structure darker . 

172
00:22:26,620 --> 00:22:40,300
The other thing I would say is that the funding amount and the catalysts that dark or provides in terms of scope of programs is at the right levels to be able to do the kinds of projects that we can do is g demonstrators , and there's early prototype . 

173
00:22:40,630 --> 00:22:52,000
Those are things that are really , really hard to do without a high level of investment , because they often involve multiple disciplines from multiple fields and or the edge of something . 

174
00:22:52,330 --> 00:23:03,300
But that something requires them to all work together to build it and to build the early stage prototypes of it fail multiple times , potentially in the process of doing that. 

175
00:23:03,370 --> 00:23:07,110
Yeah , you bring up with the failure in the tolerance for failure here to me . 

176
00:23:07,110 --> 00:23:24,640
That's one of the most interesting things that are up , because they're not all that many other stakeholders in the innovation ecosystem that can actually afford to do that if your company , you might be able to feel a little bit , but you've got to build a deliver a technology you've got to be able to deliver it eventually . 

177
00:23:24,690 --> 00:23:25,480
Words here . 

178
00:23:25,480 --> 00:23:40,970
I guess program managers like you can probe technology possibilities and determine is that something that is has good likelihood of moving for another word you can take the down for others who then might take it to write exactly. 

179
00:23:41,130 --> 00:23:45,240
So I worked and started borrowed prior to going linking , and one of the things I can tell you . 

180
00:23:45,240 --> 00:23:55,595
There is that , you know , there's actually relatively little tolerance , the kinds of risks that you can tolerate are not the same as the kinds of risks that we can tolerate . 

181
00:23:55,595 --> 00:24:03,030
Here , a double , the concrete idea that you haven't started better , be better , be actionable is a product in three to four years . 

182
00:24:03,030 --> 00:24:16,138
Otherwise , in others , there is no , no sense of funding that there is also a business related risk , execution related risk , but technology risk can be the biggest portion of that a dark , but we can take technology risks , science risks . 

183
00:24:16,138 --> 00:24:20,850
And I think that's actually the fundamental thing that it allows us to do what we do. 

184
00:24:20,850 --> 00:24:26,571
So we'd like other program managers , and you are a technology developer . 

185
00:24:26,571 --> 00:24:31,726
Your exercise isn't your science , engineering and kind of technology , imagination to innovation . 

186
00:24:31,726 --> 00:24:36,396
What might be possible and really determining whether , in fact , it is , or might not be . 

187
00:24:36,396 --> 00:24:44,430
So i'm just wondering if there is a kind of fictional technology that you might have been thinking about , who knows , since you were a kid , but you wish. 

188
00:24:46,090 --> 00:24:47,670
But isn't yet real . 

189
00:24:48,180 --> 00:24:50,470
Well , so that's that's a fun one . 

190
00:24:50,700 --> 00:25:04,950
You know , being a language person , the thing that , and when they worked on a train solution for many years , you know , for years , we've been a certain driving towards the this kind of bubble , fish kind of capability . 

191
00:25:04,950 --> 00:25:05,280
Do you know ? 

192
00:25:05,280 --> 00:25:06,320
I mean that ? 

193
00:25:06,320 --> 00:25:13,530
So if you have , you never read the safari books by a double settings , he tried to sky to the galaxy and so on . 

194
00:25:14,210 --> 00:25:17,880
You know , in the various inner steller spaces , right ? 

195
00:25:17,880 --> 00:25:19,380
Everybody speaks a different language . 

196
00:25:19,760 --> 00:25:36,870
So the solution that this is the bubble fish is the fish that you put in your area , and suddenly you can hear your own language , you know , the translation , the perfect translation of , you know , all of these alien species university and the universal translator , right , is exactly from start record . 

197
00:25:37,810 --> 00:25:53,380
Well , you know , that what's really fascinating in the last couple years because of the work that's going on a deep neural networks , and the cool work is been going both and dark , offended research and and commercially funded work . 

198
00:25:53,380 --> 00:25:54,910
We're really getting closer that . 

199
00:25:54,910 --> 00:26:10,000
So there was this paper last year by google and described the use multiple languages to do translation , translation , translating from multiple languages into language and training those translators from various different . 

200
00:26:10,000 --> 00:26:20,508
And what they discovered is that the ignorant networks learn kind of a universal , synthetic representation of language is part of the process of training . 

201
00:26:20,508 --> 00:26:21,675
That's emerging . 

202
00:26:21,675 --> 00:26:34,060
It's an emergent how these new networks were built , and so what it leads to , in long run is potentially actually having a universal translator , and that is going to be a really cool. 

203
00:26:34,530 --> 00:26:38,700
And just give it a shot , if you wanna give listeners in any idea what you mean by normal. 

204
00:26:39,420 --> 00:26:43,810
Yeah , so , so we use the terminal network . 

205
00:26:43,810 --> 00:26:47,630
It actually has very little to do with neurons , as in the south . 

206
00:26:48,720 --> 00:27:00,900
But these are models of how your noodles work , sort of very prominent of models of how your neurons in your brain work that are composed together to form little small brain like unit . 

207
00:27:01,360 --> 00:27:16,550
And what the google guys did was they figured out how to build a normal network that could take information from multiple languages , and essentially regularized into a form that would allow it to translate in english . 

208
00:27:16,970 --> 00:27:33,640
And that in a media at the form , which is just in some layer in the middle of this rather large network , you know , happens to have properties that look the same across multiple languages , meetings that it's effectively like the shared synthetics of different language. 

209
00:27:34,630 --> 00:27:39,659
Fascinating , well , thanks for explaining that sounds like we might be going from the idea of fictional . 

210
00:27:39,659 --> 00:27:45,227
The universal translator is something that we might actually all of us be able to use , hopefully or at all . 

211
00:27:45,227 --> 00:27:48,640
Seven point four billion dollars on the plant might be able to talk . 

212
00:27:48,640 --> 00:27:52,495
And I tried in the thousands of languages that we speak . 

213
00:27:52,495 --> 00:27:59,980
So wait , we have covered a lot of fascinating ground here , and I just want to thank you for this engaging conversation . 

214
00:28:00,000 --> 00:28:00,930
WOW , thank you . 

215
00:28:00,930 --> 00:28:02,790
Evan is great talking to you . 

216
00:28:02,790 --> 00:28:05,812
And thanks listeners for sharing this time with us . 

217
00:28:05,812 --> 00:28:19,780
I hope you join us again for the next Voices from the other for more information about weight , his programs and the other break through technology start is working on visit , dark at that mill . 

